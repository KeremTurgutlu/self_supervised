{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Barlow Twins Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** This notebook demonstrates how to use `SimCLR` callback with a single GPU. For distributed version, `DistributedSimCLR` checkout documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First import **fastai** for training and other helpers, you can choose not to use **wandb** by setting `WANDB=False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *\n",
    "from fastai.callback.wandb import WandbCallback\n",
    "import wandb\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "WANDB = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then import **self_supervised** `augmentations` module for creating augmentations pipeline, `layers` module for creating encoder and model, and finally `simclr` for self-supervised training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from self_supervised.augmentations import *\n",
    "from self_supervised.layers import *\n",
    "# from self_supervised.vision.simclr import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will take a look at [ImageWang](https://github.com/fastai/imagenette#image%E7%BD%91) benchmark, how to train a self-supervised model using MoCo algorithm and then how to use this pretrained model for finetuning on the given downstream task. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dls(size, bs, workers=None):\n",
    "    path = URLs.IMAGEWANG_160 if size <= 160 else URLs.IMAGEWANG\n",
    "    source = untar_data(path)\n",
    "    \n",
    "    files = get_image_files(source)\n",
    "    tfms = [[PILImage.create, ToTensor, RandomResizedCrop(size, min_scale=1.)], \n",
    "            [parent_label, Categorize()]]\n",
    "    \n",
    "    dsets = Datasets(files, tfms=tfms, splits=RandomSplitter(valid_pct=0.1)(files))\n",
    "    \n",
    "    batch_tfms = [IntToFloatTensor]\n",
    "    dls = dsets.dataloaders(bs=bs, num_workers=workers, after_batch=batch_tfms)\n",
    "    return dls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ImageWang has several benchmarks for different image sizes, in this tutorial we will go for `size=224` and also demonstrate how effectively you can utilize GPU memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define batch size, resize resolution before batching and size for random cropping during self-supervised training. It's always good to use a batch size as high as it can fit the GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs, resize, size = 256, 256, 224"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select architecture to train on, remember all **timm** and **fastai** models are available! We need to set `pretrained=False` here because using imagenet weights for ImageWang data would be cheating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arch = \"xresnet34\"\n",
    "# encoder = create_encoder(arch, pretrained=False, n_in=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if WANDB:\n",
    "#     xtra_config = {\"Arch\":arch, \"Resize\":resize, \"Size\":size, \"Algorithm\":\"Barlow-Twins\"}\n",
    "#     wandb.init(project=\"self-supervised-imagewang\", config=xtra_config);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the Dataloaders using the function above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dls = get_dls(resize, bs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create SimCLR model. You can change values of `hidden_size`, `projection_size`, and `n_layers`. For this problem, defaults work just fine so we don't do any changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class BarlowTwinsModel(Module):\n",
    "    \"An encoder followed by a projector\" \n",
    "    def __init__(self,encoder,projector): self.encoder,self.projector = encoder,projector\n",
    "    def forward(self,x): return self.projector(self.encoder(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def create_barlow_twins_model(encoder, hidden_size=256, projection_size=128):\n",
    "    \"Create SimCLR model\"\n",
    "    n_in  = in_channels(encoder)\n",
    "    with torch.no_grad(): representation = encoder(torch.randn((2,n_in,128,128)))\n",
    "    projector = create_mlp_module(representation.size(1), hidden_size, projection_size, bn=True, nlayers=3) \n",
    "    apply_init(projector)\n",
    "    return BarlowTwinsModel(encoder, projector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = create_barlow_twins_model(encoder, hidden_size=768, projection_size=768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.projector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is perhaps the most critical step for achieving good results on a custom problem - data augmentation. For this, we will use utility function from `self_supervised.vision.simclr.get_simclr_aug_pipelines` but you can also use your own list of Pipeline augmentations. `self_supervised.vision.simclr.get_moco_aug_pipelines`should be enough for most of the cases since under the hood it uses `self_supervised.augmentations.get_multi_aug_pipelines` and `self_supervised.augmentations.get_batch_augs`. You can do shift+tab and see all the arguments that can be passed to `get_simclr_aug_pipelines`. You can simply pass anything that you could pass to `get_batch_augs` including custom `xtra_tfms`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`get_simclr_aug_pipelines` excepts size for random resized cropping of the 2 views of a given image and the rest of the arguments are coming from `get_batch_augs()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aug_pipelines = get_multi_aug_pipelines(n=2, size=size, rotate=True, \n",
    "#                                         rotate_deg=10, jitter=True, bw=True, blur=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aug_pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will feed the augmentation pipelines and leave temperature parameter as default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class BarlowTwins(Callback):\n",
    "    order,run_valid = 9,True\n",
    "    def __init__(self, aug_pipelines, lmb=5e-3, print_augs=False):\n",
    "        assert_aug_pipelines(aug_pipelines)\n",
    "        self.aug1, self.aug2 = aug_pipelines\n",
    "        if print_augs: print(self.aug1), print(self.aug2)\n",
    "        store_attr('lmb')\n",
    "        \n",
    "        \n",
    "    def before_fit(self): \n",
    "        self.learn.loss_func = self.lf\n",
    "        nf = self.learn.model.projector[-1].out_features\n",
    "        self.I = torch.eye(nf).to(self.dls.device)\n",
    "                    \n",
    "            \n",
    "    def before_batch(self):\n",
    "        xi,xj = self.aug1(self.x), self.aug2(self.x)\n",
    "        self.learn.xb = (torch.cat([xi, xj]),)\n",
    "        \n",
    "    \n",
    "    def lf(self, pred, *yb):\n",
    "        bs,nf = pred.size(0)//2,pred.size(1)\n",
    "        z1, z2 = pred[:bs],pred[bs:]\n",
    "        \n",
    "        z1norm = (z1 - z1.mean(0)) / z1.std(0, unbiased=False)\n",
    "        z2norm = (z2 - z2.mean(0)) / z2.std(0, unbiased=False)\n",
    "        \n",
    "        C = (z1norm.T @ z2norm) / bs \n",
    "        cdiff = (C - self.I)**2\n",
    "        loss = (cdiff*self.I + cdiff*(1-self.I)*self.lmb).sum() \n",
    "        return loss\n",
    "\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def show(self, n=1):\n",
    "        bs = self.learn.x.size(0)//2\n",
    "        x1,x2  = self.learn.x[:bs], self.learn.x[bs:] \n",
    "        idxs = np.random.choice(range(bs),n,False)\n",
    "        x1 = self.aug1.decode(x1[idxs].to('cpu').clone()).clamp(0,1)\n",
    "        x2 = self.aug2.decode(x2[idxs].to('cpu').clone()).clamp(0,1)\n",
    "        images = []\n",
    "        for i in range(n): images += [x1[i],x2[i]] \n",
    "        return show_batch(x1[0], None, images, max_n=len(images), nrows=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cbs=[BarlowTwins(aug_pipelines, lmb=0.1)]\n",
    "# if WANDB: cbs += [WandbCallback(log_preds=False,log_model=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn = Learner(dls, model, cbs=cbs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting training let's check whether our augmentations makes sense or not. Since this step consumes GPU memory, once you are done with inspection, restart the notebook and skip this step. We can see that 2 views of the same image side by side and indeed augmentations look pretty good. Now, it's time restart the notebook and skip this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b = dls.one_batch()\n",
    "# learn._split(b)\n",
    "# learn('before_batch')\n",
    "# learn.sim_clr.show(n=5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use mixed precision with `to_fp16()` for more GPU memory, larger batch size and faster training . We could also use gradient checkpointing wrapper models from `self_supervised.layers` to save even more memory, e.g. `CheckpointSequential()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.to_fp16();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.lr_find()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning good representations via contrastive learning usually takes a lot of epochs. So here number epochs are set to 100. This might change depending on your data distribution and dataset size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr,wd,epochs=1e-2,1e-2,100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.unfreeze()\n",
    "# learn.fit_flat_cos(epochs, lr, wd=wd, pct_start=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if WANDB: wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search Best Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to query for notebook name, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkeremturgutlu\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.22 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.20<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">soft-river-25</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/keremturgutlu/self-supervised-imagewang\" target=\"_blank\">https://wandb.ai/keremturgutlu/self-supervised-imagewang</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/keremturgutlu/self-supervised-imagewang/runs/1a6lt7bk\" target=\"_blank\">https://wandb.ai/keremturgutlu/self-supervised-imagewang/runs/1a6lt7bk</a><br/>\n",
       "                Run data is saved locally in <code>/home/code-base/extra_space/self_supervised/examples/wandb/run-20210316_073455-1a6lt7bk</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1786.907349</td>\n",
       "      <td>1100.402954</td>\n",
       "      <td>01:27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 198520<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aec30fb6d24d4e3e8115b06c0f5c23b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.02MB of 0.02MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/code-base/extra_space/self_supervised/examples/wandb/run-20210316_073455-1a6lt7bk/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/code-base/extra_space/self_supervised/examples/wandb/run-20210316_073455-1a6lt7bk/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>1</td></tr><tr><td>train_loss</td><td>1786.90735</td></tr><tr><td>raw_loss</td><td>1284.19092</td></tr><tr><td>wd_0</td><td>0.01</td></tr><tr><td>sqr_mom_0</td><td>0.99</td></tr><tr><td>lr_0</td><td>1e-05</td></tr><tr><td>mom_0</td><td>0.9</td></tr><tr><td>eps_0</td><td>1e-05</td></tr><tr><td>_runtime</td><td>99</td></tr><tr><td>_timestamp</td><td>1615880194</td></tr><tr><td>_step</td><td>91</td></tr><tr><td>valid_loss</td><td>1100.40295</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train_loss</td><td>▃▄▂▂▂▅▆▇▆▇▆▅█▇▆▇▆▆▇▇▆▅▅▅▄▄▄▄▃▃▂▂▂▁▁▁▂▂▂▂</td></tr><tr><td>raw_loss</td><td>▂▃▂▂▂▇▂▅▂▁▁▂█▂▁▄▁▁▆▁▂▂▁▁▁▁▂▆▁▂▁▁▁▁▁▃▄▂▂▂</td></tr><tr><td>wd_0</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>sqr_mom_0</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr_0</td><td>███████████████▇▇▇▇▇▆▆▆▅▅▄▄▄▃▃▃▂▂▂▂▁▁▁▁▁</td></tr><tr><td>mom_0</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eps_0</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>_runtime</td><td>▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇█</td></tr><tr><td>_timestamp</td><td>▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇█</td></tr><tr><td>_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>valid_loss</td><td>▁</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 4 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">soft-river-25</strong>: <a href=\"https://wandb.ai/keremturgutlu/self-supervised-imagewang/runs/1a6lt7bk\" target=\"_blank\">https://wandb.ai/keremturgutlu/self-supervised-imagewang/runs/1a6lt7bk</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.22 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.20<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">true-galaxy-26</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/keremturgutlu/self-supervised-imagewang\" target=\"_blank\">https://wandb.ai/keremturgutlu/self-supervised-imagewang</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/keremturgutlu/self-supervised-imagewang/runs/b5a4rlei\" target=\"_blank\">https://wandb.ai/keremturgutlu/self-supervised-imagewang/runs/b5a4rlei</a><br/>\n",
       "                Run data is saved locally in <code>/home/code-base/extra_space/self_supervised/examples/wandb/run-20210316_073639-b5a4rlei</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>407.933136</td>\n",
       "      <td>392.641327</td>\n",
       "      <td>01:27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 198631<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25cb272f75154d3b96241114138adaff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/code-base/extra_space/self_supervised/examples/wandb/run-20210316_073639-b5a4rlei/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/code-base/extra_space/self_supervised/examples/wandb/run-20210316_073639-b5a4rlei/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>1</td></tr><tr><td>train_loss</td><td>407.93314</td></tr><tr><td>raw_loss</td><td>392.93689</td></tr><tr><td>wd_0</td><td>0.01</td></tr><tr><td>sqr_mom_0</td><td>0.99</td></tr><tr><td>lr_0</td><td>1e-05</td></tr><tr><td>mom_0</td><td>0.9</td></tr><tr><td>eps_0</td><td>1e-05</td></tr><tr><td>_runtime</td><td>94</td></tr><tr><td>_timestamp</td><td>1615880293</td></tr><tr><td>_step</td><td>91</td></tr><tr><td>valid_loss</td><td>392.64133</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train_loss</td><td>▁▂▃▄▅▆▇████████▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▄▃▃▃</td></tr><tr><td>raw_loss</td><td>▁▂▃█▄█▄▄▃▅▅▃▄▃▃▄▃▅▃▂▃▃▂▃▂▂▂▂▂▂▁▃▂▁▂▁▁▁▁▂</td></tr><tr><td>wd_0</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>sqr_mom_0</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr_0</td><td>███████████████▇▇▇▇▇▆▆▆▅▅▄▄▄▃▃▃▂▂▂▂▁▁▁▁▁</td></tr><tr><td>mom_0</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eps_0</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>_runtime</td><td>▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇█</td></tr><tr><td>_timestamp</td><td>▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇█</td></tr><tr><td>_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>valid_loss</td><td>▁</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">true-galaxy-26</strong>: <a href=\"https://wandb.ai/keremturgutlu/self-supervised-imagewang/runs/b5a4rlei\" target=\"_blank\">https://wandb.ai/keremturgutlu/self-supervised-imagewang/runs/b5a4rlei</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.22 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.20<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">cool-snowflake-27</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/keremturgutlu/self-supervised-imagewang\" target=\"_blank\">https://wandb.ai/keremturgutlu/self-supervised-imagewang</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/keremturgutlu/self-supervised-imagewang/runs/2frm71vx\" target=\"_blank\">https://wandb.ai/keremturgutlu/self-supervised-imagewang/runs/2frm71vx</a><br/>\n",
       "                Run data is saved locally in <code>/home/code-base/extra_space/self_supervised/examples/wandb/run-20210316_073818-2frm71vx</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/1 00:00<00:00]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='56' class='' max='92' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      60.87% [56/92 00:53<00:34 183.9926]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bs, resize, size = 256, 256, 224\n",
    "lr,wd,epochs=1e-2,1e-2,1\n",
    "WANDB = True\n",
    "arch = \"xresnet34\"\n",
    "\n",
    "\n",
    "for lmb in (5e-1,5e-2,5e-3,5e-4,5e-5):\n",
    "    if WANDB:\n",
    "        xtra_config = {\"Arch\":arch, \"Resize\":resize, \"Size\":size, \"Algorithm\":\"Barlow-Twins\"}\n",
    "        wandb.init(project=\"self-supervised-imagewang\", config=xtra_config)\n",
    "    \n",
    "    dls = get_dls(resize, bs)\n",
    "    encoder = create_encoder(arch, pretrained=False, n_in=3)\n",
    "    model = create_barlow_twins_model(encoder, hidden_size=512, projection_size=512)\n",
    "    aug_pipelines = get_multi_aug_pipelines(n=2, size=size, rotate=True, \n",
    "                                            rotate_deg=10, jitter=True, bw=True, blur=False)\n",
    "    cbs=[BarlowTwins(aug_pipelines, lmb=lmb)]\n",
    "    if WANDB: cbs += [WandbCallback(log_preds=False,log_model=False)]\n",
    "    \n",
    "    learn = Learner(dls, model, cbs=cbs)\n",
    "    learn.to_fp16()\n",
    "    learn.fit_flat_cos(epochs, lr, wd=wd, pct_start=0.25)\n",
    "\n",
    "    save_name = f'btwins_iwang_sz{size}_epc{epochs}_lmb{lmb}'\n",
    "    learn.save(save_name)\n",
    "    torch.save(learn.model.encoder.state_dict(), learn.path/learn.model_dir/f'{save_name}_encoder.pth')\n",
    "    \n",
    "    if WANDB: wandb.finish()\n",
    "    \n",
    "    \n",
    "    del dls, learn\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downstream Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optdict = dict(sqrmom=0.99,mom=0.95,beta=0.,eps=1e-4)\n",
    "opt_func = partial(ranger, **optdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs, size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dls(size, bs, workers=None):\n",
    "    path = URLs.IMAGEWANG_160 if size <= 160 else URLs.IMAGEWANG\n",
    "    source = untar_data(path)\n",
    "    files = get_image_files(source, folders=['train', 'val'])\n",
    "    splits = GrandparentSplitter(valid_name='val')(files)\n",
    "    \n",
    "    item_aug = [RandomResizedCrop(size, min_scale=0.35), FlipItem(0.5)]\n",
    "    tfms = [[PILImage.create, ToTensor, *item_aug], \n",
    "            [parent_label, Categorize()]]\n",
    "    \n",
    "    dsets = Datasets(files, tfms=tfms, splits=splits)\n",
    "    \n",
    "    batch_tfms = [IntToFloatTensor, Normalize.from_stats(*imagenet_stats)]\n",
    "    dls = dsets.dataloaders(bs=bs, num_workers=workers, after_batch=batch_tfms)\n",
    "    return dls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_func(m): return L(m[0], m[1]).map(params)\n",
    "\n",
    "def create_learner(size=size, arch='xresnet34', encoder_path=\"models/swav_iwang_sz128_epc100_encoder.pth\"):\n",
    "    \n",
    "    dls = get_dls(size, bs=bs//2)\n",
    "    pretrained_encoder = torch.load(encoder_path)\n",
    "    encoder = create_encoder(arch, pretrained=False, n_in=3)\n",
    "    encoder.load_state_dict(pretrained_encoder)\n",
    "    nf = encoder(torch.randn(2,3,224,224)).size(-1)\n",
    "    classifier = create_cls_module(nf, dls.c)\n",
    "    model = nn.Sequential(encoder, classifier)\n",
    "    learn = Learner(dls, model, opt_func=opt_func, splitter=split_func,\n",
    "                metrics=[accuracy,top_k_accuracy], loss_func=LabelSmoothingCrossEntropy())\n",
    "    return learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune(size, epochs, arch, encoder_path, lr=1e-2, wd=1e-2):\n",
    "    learn = create_learner(size, arch, encoder_path)\n",
    "    learn.unfreeze()\n",
    "    learn.fit_flat_cos(epochs, lr, wd=wd)\n",
    "    final_acc = learn.recorder.values[-1][-2]\n",
    "    return final_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = []\n",
    "runs = 5\n",
    "for i in range(runs): acc += [finetune(size, epochs=5, arch='xresnet34', encoder_path=f'models/simclr_iwang_sz{size}_epc100_encoder.pth')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = []\n",
    "runs = 3\n",
    "for i in range(runs): acc += [finetune(size, epochs=20, arch='xresnet34', encoder_path=f'models/simclr_iwang_sz{size}_epc100_encoder.pth')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 80 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = []\n",
    "runs = 1\n",
    "for i in range(runs): acc += [finetune(size, epochs=80, arch='xresnet34',encoder_path=f'models/simclr_iwang_sz{size}_epc100_encoder.pth')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 200 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = []\n",
    "runs = 1\n",
    "for i in range(runs): acc += [finetune(size, epochs=200, arch='xresnet34', encoder_path=f'models/simclr_iwang_sz{size}_epc100_encoder.pth')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
