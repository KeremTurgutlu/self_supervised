{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *\n",
    "from self_supervised.augmentations import *\n",
    "from self_supervised.layers import *\n",
    "from self_supervised.swav import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we will take a look at how to extract emebddings using the encoders trained with any of the self-supervised learning algorithms in this repo. Here, we will use SWAV algorithm as an example. Below code shows how to create a Learner for training and this might be something you have already done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqrmom=0.99\n",
    "mom=0.95\n",
    "beta=0.\n",
    "eps=1e-4\n",
    "opt_func = partial(ranger, mom=mom, sqr_mom=sqr_mom, eps=eps, beta=beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dls(size, bs, workers=None):\n",
    "    path = URLs.IMAGEWANG_160 if size <= 160 else URLs.IMAGEWANG\n",
    "    source = untar_data(path)\n",
    "    \n",
    "    files = get_image_files(source)\n",
    "    tfms = [[PILImage.create, ToTensor, RandomResizedCrop(size, min_scale=1.)], \n",
    "            [parent_label, Categorize()]]\n",
    "    \n",
    "    dsets = Datasets(files, tfms=tfms, splits=RandomSplitter(valid_pct=0.1)(files))\n",
    "    \n",
    "    batch_tfms = [IntToFloatTensor]\n",
    "    dls = dsets.dataloaders(bs=bs, num_workers=workers, after_batch=batch_tfms)\n",
    "    return dls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs=96\n",
    "resize, size = 256, 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "arch = \"xresnet34\"\n",
    "encoder = create_encoder(arch, pretrained=False, n_in=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = get_dls(resize, bs)\n",
    "model = create_swav_model(encoder, n_in=3)\n",
    "learn = Learner(dls, model, SWAVLoss(),\n",
    "                cbs=[SWAV(aug_func=get_batch_augs,\n",
    "                          crop_sizes=[size,int(3/4*size)], #4/3 - large to small crop size ratio is an important hyperparam for optimization! If we kept small crop 96 and just increased 128 to 192 training has hard time\n",
    "                          num_crops=[2,6],\n",
    "                          min_scales=[0.25,0.2],\n",
    "                          max_scales=[1.0,0.35],                \n",
    "                          rotate=True,\n",
    "                          rotate_deg=10,\n",
    "                          jitter=True,\n",
    "                          bw=True,\n",
    "                          blur=False\n",
    "                          ),\n",
    "                     TerminateOnNaNCallback(),\n",
    "                     ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I already had a model trained so for demonstration purposes I will just load it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<fastai.learner.Learner at 0x7f074d24dd90>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs=100\n",
    "load_name = f'swav_iwang_sz{size}_epc{epochs}'\n",
    "learn.load(load_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Your Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once a model is trained you can simply save either the `model weights`, `encoder weights` or directly the `Learner` itself. We will use fastai and pytorch for this. You can skip this part if you already have either one of them saved already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_name = f'swav_iwang_sz{size}_epc{epochs}' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save(save_name) # saves whole model weights with_opt=True by default\n",
    "torch.save(learn.model.encoder.state_dict(), learn.path/learn.model_dir/f'{save_name}_encoder.pth') # saves only the encoder state dict\n",
    "learn.export(learn.path/learn.model_dir/f'{save_name}_export.pkl') # saves whole Learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can simply see all of them saved successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#3) [Path('models/swav_iwang_sz224_epc100_encoder.pth'),Path('models/swav_iwang_sz224_epc100.pth'),Path('models/swav_iwang_sz224_epc100_export.pkl')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(learn.path/learn.model_dir).ls().filter(lambda o: save_name in o.stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting with Learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your input items are compatible with the Dataloaders/dls you used during training then using the saved `Learner` object is the most straightforward solution for embedding extraction. For example the dls we used for training uses the following tfms for getting the input ready, so it expects a filename.\n",
    "\n",
    "```[PILImage.create, ToTensor, RandomResizedCrop(size, min_scale=1.)]```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's take a look at how to embed everything we have in an efficient manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = URLs.IMAGEWANG_160 if size <= 160 else URLs.IMAGEWANG\n",
    "source = untar_data(path)\n",
    "embedding_files = get_image_files(source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's create a dataframe to later use during visualization with some neat functional programming using L object from fastcore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"filename\":embedding_files})\n",
    "\n",
    "df['split'] = (embedding_files).map(lambda o: o.parent.parent.name)\n",
    "df['split'] = df['split'].replace(to_replace='imagewang', value='unsup')\n",
    "df['label'] = (embedding_files).map(lambda o: o.parent.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>split</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/root/.fastai/data/imagewang/train/n02096294/n02096294_2289.JPEG</td>\n",
       "      <td>train</td>\n",
       "      <td>n02096294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/root/.fastai/data/imagewang/train/n02096294/ILSVRC2012_val_00037999.JPEG</td>\n",
       "      <td>train</td>\n",
       "      <td>n02096294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/root/.fastai/data/imagewang/train/n02096294/n02096294_4119.JPEG</td>\n",
       "      <td>train</td>\n",
       "      <td>n02096294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/root/.fastai/data/imagewang/train/n02096294/n02096294_6689.JPEG</td>\n",
       "      <td>train</td>\n",
       "      <td>n02096294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/root/.fastai/data/imagewang/train/n02096294/n02096294_4839.JPEG</td>\n",
       "      <td>train</td>\n",
       "      <td>n02096294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26343</th>\n",
       "      <td>/root/.fastai/data/imagewang/unsup/n02105641_10903.JPEG</td>\n",
       "      <td>unsup</td>\n",
       "      <td>unsup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26344</th>\n",
       "      <td>/root/.fastai/data/imagewang/unsup/n02093754_898.JPEG</td>\n",
       "      <td>unsup</td>\n",
       "      <td>unsup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26345</th>\n",
       "      <td>/root/.fastai/data/imagewang/unsup/n02111889_17867.JPEG</td>\n",
       "      <td>unsup</td>\n",
       "      <td>unsup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26346</th>\n",
       "      <td>/root/.fastai/data/imagewang/unsup/n02111889_5998.JPEG</td>\n",
       "      <td>unsup</td>\n",
       "      <td>unsup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26347</th>\n",
       "      <td>/root/.fastai/data/imagewang/unsup/n02093754_5227.JPEG</td>\n",
       "      <td>unsup</td>\n",
       "      <td>unsup</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26348 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                        filename  \\\n",
       "0               /root/.fastai/data/imagewang/train/n02096294/n02096294_2289.JPEG   \n",
       "1      /root/.fastai/data/imagewang/train/n02096294/ILSVRC2012_val_00037999.JPEG   \n",
       "2               /root/.fastai/data/imagewang/train/n02096294/n02096294_4119.JPEG   \n",
       "3               /root/.fastai/data/imagewang/train/n02096294/n02096294_6689.JPEG   \n",
       "4               /root/.fastai/data/imagewang/train/n02096294/n02096294_4839.JPEG   \n",
       "...                                                                          ...   \n",
       "26343                    /root/.fastai/data/imagewang/unsup/n02105641_10903.JPEG   \n",
       "26344                      /root/.fastai/data/imagewang/unsup/n02093754_898.JPEG   \n",
       "26345                    /root/.fastai/data/imagewang/unsup/n02111889_17867.JPEG   \n",
       "26346                     /root/.fastai/data/imagewang/unsup/n02111889_5998.JPEG   \n",
       "26347                     /root/.fastai/data/imagewang/unsup/n02093754_5227.JPEG   \n",
       "\n",
       "       split      label  \n",
       "0      train  n02096294  \n",
       "1      train  n02096294  \n",
       "2      train  n02096294  \n",
       "3      train  n02096294  \n",
       "4      train  n02096294  \n",
       "...      ...        ...  \n",
       "26343  unsup      unsup  \n",
       "26344  unsup      unsup  \n",
       "26345  unsup      unsup  \n",
       "26346  unsup      unsup  \n",
       "26347  unsup      unsup  \n",
       "\n",
       "[26348 rows x 3 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can load the learner directly and create a test_dl for embedding extraction, also don't forget to disable cpu loading if you have a GPU!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = load_learner(\"./models/swav_iwang_sz224_epc100_export.pkl\", cpu=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline: RandomResizedCrop -> RandomHorizontalFlip -> ColorJitter -> RandomGrayscale -> Rotate -- {'size': None, 'mode': 'bilinear', 'pad_mode': 'reflection', 'mode_mask': 'nearest', 'align_corners': True, 'p': 1.0} -> Normalize -- {'mean': tensor([[[[0.4850]],\n",
       "\n",
       "         [[0.4560]],\n",
       "\n",
       "         [[0.4060]]]], device='cuda:0'), 'std': tensor([[[[0.2290]],\n",
       "\n",
       "         [[0.2240]],\n",
       "\n",
       "         [[0.2250]]]], device='cuda:0'), 'axes': (0, 2, 3)}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.swav.augs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point you can either add a `Hook` to `learn.model.encoder` or overwrite `learn.model=learn.model.encoder`. I will overwrite it since I won't need the remaining part of the model in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (8): AdaptiveConcatPool2d(\n",
       "    (ap): AdaptiveAvgPool2d(output_size=1)\n",
       "    (mp): AdaptiveMaxPool2d(output_size=1)\n",
       "  )\n",
       "  (9): Flatten(full=False)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.model = learn.model.encoder; learn.model[-2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to disable SWAV callback and add back `Normalization` if any used during training.\n",
    "\n",
    "**Important** All of the augmentations including `Normalization` happen inside self-supervised learning callbacks. By default `imagenet_stats` is used, but it can be different in your case. In this learner example, images were first resized to 256 for batching and further 224 crops were used during SWAV training. You can try both resolution during embedding extraction to see which one is better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#4) [TrainEvalCallback,Recorder,ProgressCallback,TerminateOnNaNCallback]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.cbs = L([cb for cb in learn.cbs if cb.name != 'swav']); learn.cbs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to use correct tfms Pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dl = learn.dls.test_dl(embedding_files, after_batch=[IntToFloatTensor(), Normalize.from_stats(*imagenet_stats)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that images will be resized to 256x256, if you want to change this or any data tfms in general you can skip to **Extracting with Custom Dataset** section below. `tfms` is used for creating data from the source, e.g. a filename, then `after_item` is used to apply transforms to each created data sample individually, and finally `after_batch` is used to apply transforms to collated batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Pipeline: PILBase.create -> RandomResizedCrop -- {'size': (256, 256), 'min_scale': 1.0, 'ratio': (0.75, 1.3333333333333333), 'resamples': (2, 0), 'val_xtra': 0.14, 'p': 1.0} -> ToTensor,\n",
       " Pipeline: ,\n",
       " Pipeline: IntToFloatTensor -- {'div': 255.0, 'div_mask': 1} -> Normalize -- {'mean': tensor([[[[0.4850]],\n",
       " \n",
       "          [[0.4560]],\n",
       " \n",
       "          [[0.4060]]]], device='cuda:0'), 'std': tensor([[[[0.2290]],\n",
       " \n",
       "          [[0.2240]],\n",
       " \n",
       "          [[0.2250]]]], device='cuda:0'), 'axes': (0, 2, 3)})"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_dl.tfms, emb_dl.after_item, emb_dl.after_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to set activation function for predictions as no operation because it's set for classification output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embeddings, _ = learn.get_preds(dl=emb_dl, act=noop) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the embeddings and finally save."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorBase([[ 0.9732,  5.4093,  1.0725,  ...,  0.2657,  0.2560,  1.6736],\n",
       "         [ 2.0293,  3.4570,  0.4585,  ...,  0.1125,  0.5919,  1.3086],\n",
       "         [ 4.1430,  5.6190, 18.5848,  ...,  0.5565,  0.6212,  1.8319],\n",
       "         ...,\n",
       "         [ 2.3026,  1.6281,  3.4819,  ...,  0.7759,  0.5221,  0.3157],\n",
       "         [ 5.6842,  2.0450,  0.6188,  ...,  0.2059,  0.1493,  0.4630],\n",
       "         [ 3.0677,  4.3904,  2.5808,  ...,  0.3809,  0.4606,  0.9369]]),\n",
       " torch.Size([26348, 1024]),\n",
       " TensorBase([102.1530, 123.4563, 147.9856,  ..., 202.9648, 139.4411, 107.1795]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings,  embeddings.shape, embeddings.norm(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dir = Path(\"./embeddings\")\n",
    "if not emb_dir.exists(): emb_dir.mkdir()\n",
    "torch.save(embeddings, emb_dir/'swav_iwang_embeddings.pth')\n",
    "df.to_csv(emb_dir/'iwang.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting with Custom Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes you might have a inputs which are not compatible with training dls. In those cases, you can create your own custom dls but making sure to use the same item tfms you used during training to get meaningful representations, such as `RandomResizedCrop`, image resolution and `Normalize`. I will add a dummy function to my tfms list which will mimic your own custom scenario for reading and generating inputs. Also let's assume this dataset is unlabaled, so for `y_tfms` we will pass an empty list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_func(x):\n",
    "    # your own transform for reading\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not do one pass in your dataloader, there is something wrong in it\n",
      "Could not do one pass in your dataloader, there is something wrong in it\n"
     ]
    }
   ],
   "source": [
    "tfms = [[dummy_func, PILImage.create, ToTensor, RandomResizedCrop(size=256, min_scale=1.)], []]\n",
    "dsets = Datasets(embedding_files, tfms=tfms, splits=RandomSplitter(valid_pct=0.1)(embedding_files))\n",
    "batch_tfms = [IntToFloatTensor, Normalize.from_stats(*imagenet_stats)]\n",
    "dls = dsets.dataloaders(bs=bs, num_workers=4, after_batch=batch_tfms)\n",
    "emb_dl = dls.test_dl(embedding_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Pipeline: dummy_func -> PILBase.create -> RandomResizedCrop -- {'size': (256, 256), 'min_scale': 1.0, 'ratio': (0.75, 1.3333333333333333), 'resamples': (2, 0), 'val_xtra': 0.14, 'p': 1.0} -> ToTensor,\n",
       " Pipeline: ,\n",
       " Pipeline: IntToFloatTensor -- {'div': 255.0, 'div_mask': 1} -> Normalize -- {'mean': tensor([[[[0.4850]],\n",
       " \n",
       "          [[0.4560]],\n",
       " \n",
       "          [[0.4060]]]], device='cuda:0'), 'std': tensor([[[[0.2290]],\n",
       " \n",
       "          [[0.2240]],\n",
       " \n",
       "          [[0.2250]]]], device='cuda:0'), 'axes': (0, 2, 3)})"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_dl.tfms, emb_dl.after_item, emb_dl.after_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the same embeddings as learner but using a custom data Pipeline, so that you can modify it for your own needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embeddings, _ = learn.get_preds(dl=emb_dl, act=noop) # we need to set activation as no operation because it's set for classification output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorBase([[ 0.9732,  5.4093,  1.0725,  ...,  0.2657,  0.2560,  1.6736],\n",
       "         [ 2.0293,  3.4570,  0.4585,  ...,  0.1125,  0.5919,  1.3086],\n",
       "         [ 4.1430,  5.6190, 18.5848,  ...,  0.5565,  0.6212,  1.8319],\n",
       "         ...,\n",
       "         [ 2.3026,  1.6281,  3.4819,  ...,  0.7759,  0.5221,  0.3157],\n",
       "         [ 5.6842,  2.0450,  0.6188,  ...,  0.2059,  0.1493,  0.4630],\n",
       "         [ 3.0677,  4.3904,  2.5808,  ...,  0.3809,  0.4606,  0.9369]]),\n",
       " torch.Size([26348, 1024]),\n",
       " TensorBase([102.1530, 123.4563, 147.9856,  ..., 202.9648, 139.4411, 107.1795]))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings,  embeddings.shape, embeddings.norm(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dir = Path(\"./embeddings\")\n",
    "if not emb_dir.exists(): emb_dir.mkdir()\n",
    "torch.save(embeddings, emb_dir/'swav_iwang_embeddings.pth')\n",
    "df.to_csv(emb_dir/'iwang.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting with PyTorch Boilerplate\n",
    "\n",
    "If you've only saved model/encoder weights and not learner, then in that case you need to follow an approach like below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "arch = \"xresnet34\"\n",
    "encoder = create_encoder(arch, pretrained=False, n_in=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.load_state_dict(torch.load( learn.path/learn.model_dir/f'{save_name}_encoder.pth'))\n",
    "encoder.eval().cuda();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='275' class='' max='275' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [275/275 01:04<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embeddings = []\n",
    "for xb in progress_bar(emb_dl):\n",
    "    xb = xb[0]\n",
    "    embeddings += [to_detach(encoder(xb))]\n",
    "embeddings = torch.cat(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorBase([[ 0.9732,  5.4093,  1.0725,  ...,  0.2657,  0.2560,  1.6736],\n",
       "         [ 2.0293,  3.4570,  0.4585,  ...,  0.1125,  0.5919,  1.3086],\n",
       "         [ 4.1430,  5.6190, 18.5848,  ...,  0.5565,  0.6212,  1.8319],\n",
       "         ...,\n",
       "         [ 2.3026,  1.6281,  3.4819,  ...,  0.7759,  0.5221,  0.3157],\n",
       "         [ 5.6842,  2.0450,  0.6188,  ...,  0.2059,  0.1493,  0.4630],\n",
       "         [ 3.0677,  4.3904,  2.5808,  ...,  0.3809,  0.4606,  0.9369]]),\n",
       " torch.Size([26348, 1024]),\n",
       " TensorBase([102.1530, 123.4563, 147.9856,  ..., 202.9648, 139.4411, 107.1795]))"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings,  embeddings.shape, embeddings.norm(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way of qualitative analysis you can do is to visualize learned representations/embeddings. For this purpose [Nvidia Rapids cuML](https://docs.rapids.ai/api/cuml/stable/) library can be used, it's very fast due to GPU support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
