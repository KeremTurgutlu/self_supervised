{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *\n",
    "from self_supervised.augmentations import *\n",
    "from self_supervised.layers import *\n",
    "from self_supervised.swav import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we will take a look at how to extract emebddings using the encoders trained with any of the self-supervised learning algorithms in this repo. Here, we will use SWAV algorithm as an example. Below code shows how to create a Learner for training and this might be something you have already done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqrmom=0.99\n",
    "mom=0.95\n",
    "beta=0.\n",
    "eps=1e-4\n",
    "opt_func = partial(ranger, mom=mom, sqr_mom=sqrmom, eps=eps, beta=beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dls(size, bs, workers=None):\n",
    "    path = URLs.IMAGEWANG_160 if size <= 160 else URLs.IMAGEWANG\n",
    "    source = untar_data(path)\n",
    "    \n",
    "    files = get_image_files(source)\n",
    "    tfms = [[PILImage.create, ToTensor, RandomResizedCrop(size, min_scale=1.)], \n",
    "            [parent_label, Categorize()]]\n",
    "    \n",
    "    dsets = Datasets(files, tfms=tfms, splits=RandomSplitter(valid_pct=0.1)(files))\n",
    "    \n",
    "    batch_tfms = [IntToFloatTensor]\n",
    "    dls = dsets.dataloaders(bs=bs, num_workers=workers, after_batch=batch_tfms)\n",
    "    return dls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs=96\n",
    "resize, size = 256, 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "arch = \"xresnet34\"\n",
    "encoder = create_encoder(arch, pretrained=False, n_in=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline: RandomResizedCrop -> RandomHorizontalFlip -> ColorJitter -> RandomGrayscale -> Rotate -- {'size': None, 'mode': 'bilinear', 'pad_mode': 'reflection', 'mode_mask': 'nearest', 'align_corners': True, 'p': 1.0} -> Normalize -- {'mean': tensor([[[[0.4850]],\n",
      "\n",
      "         [[0.4560]],\n",
      "\n",
      "         [[0.4060]]]], device='cuda:0'), 'std': tensor([[[[0.2290]],\n",
      "\n",
      "         [[0.2240]],\n",
      "\n",
      "         [[0.2250]]]], device='cuda:0'), 'axes': (0, 2, 3)}\n",
      "Pipeline: RandomResizedCrop -> RandomHorizontalFlip -> ColorJitter -> RandomGrayscale -> Rotate -- {'size': None, 'mode': 'bilinear', 'pad_mode': 'reflection', 'mode_mask': 'nearest', 'align_corners': True, 'p': 1.0} -> Normalize -- {'mean': tensor([[[[0.4850]],\n",
      "\n",
      "         [[0.4560]],\n",
      "\n",
      "         [[0.4060]]]], device='cuda:0'), 'std': tensor([[[[0.2290]],\n",
      "\n",
      "         [[0.2240]],\n",
      "\n",
      "         [[0.2250]]]], device='cuda:0'), 'axes': (0, 2, 3)}\n",
      "Pipeline: RandomResizedCrop -> RandomHorizontalFlip -> ColorJitter -> RandomGrayscale -> Rotate -- {'size': None, 'mode': 'bilinear', 'pad_mode': 'reflection', 'mode_mask': 'nearest', 'align_corners': True, 'p': 1.0} -> Normalize -- {'mean': tensor([[[[0.4850]],\n",
      "\n",
      "         [[0.4560]],\n",
      "\n",
      "         [[0.4060]]]], device='cuda:0'), 'std': tensor([[[[0.2290]],\n",
      "\n",
      "         [[0.2240]],\n",
      "\n",
      "         [[0.2250]]]], device='cuda:0'), 'axes': (0, 2, 3)}\n",
      "Pipeline: RandomResizedCrop -> RandomHorizontalFlip -> ColorJitter -> RandomGrayscale -> Rotate -- {'size': None, 'mode': 'bilinear', 'pad_mode': 'reflection', 'mode_mask': 'nearest', 'align_corners': True, 'p': 1.0} -> Normalize -- {'mean': tensor([[[[0.4850]],\n",
      "\n",
      "         [[0.4560]],\n",
      "\n",
      "         [[0.4060]]]], device='cuda:0'), 'std': tensor([[[[0.2290]],\n",
      "\n",
      "         [[0.2240]],\n",
      "\n",
      "         [[0.2250]]]], device='cuda:0'), 'axes': (0, 2, 3)}\n",
      "Pipeline: RandomResizedCrop -> RandomHorizontalFlip -> ColorJitter -> RandomGrayscale -> Rotate -- {'size': None, 'mode': 'bilinear', 'pad_mode': 'reflection', 'mode_mask': 'nearest', 'align_corners': True, 'p': 1.0} -> Normalize -- {'mean': tensor([[[[0.4850]],\n",
      "\n",
      "         [[0.4560]],\n",
      "\n",
      "         [[0.4060]]]], device='cuda:0'), 'std': tensor([[[[0.2290]],\n",
      "\n",
      "         [[0.2240]],\n",
      "\n",
      "         [[0.2250]]]], device='cuda:0'), 'axes': (0, 2, 3)}\n",
      "Pipeline: RandomResizedCrop -> RandomHorizontalFlip -> ColorJitter -> RandomGrayscale -> Rotate -- {'size': None, 'mode': 'bilinear', 'pad_mode': 'reflection', 'mode_mask': 'nearest', 'align_corners': True, 'p': 1.0} -> Normalize -- {'mean': tensor([[[[0.4850]],\n",
      "\n",
      "         [[0.4560]],\n",
      "\n",
      "         [[0.4060]]]], device='cuda:0'), 'std': tensor([[[[0.2290]],\n",
      "\n",
      "         [[0.2240]],\n",
      "\n",
      "         [[0.2250]]]], device='cuda:0'), 'axes': (0, 2, 3)}\n",
      "Pipeline: RandomResizedCrop -> RandomHorizontalFlip -> ColorJitter -> RandomGrayscale -> Rotate -- {'size': None, 'mode': 'bilinear', 'pad_mode': 'reflection', 'mode_mask': 'nearest', 'align_corners': True, 'p': 1.0} -> Normalize -- {'mean': tensor([[[[0.4850]],\n",
      "\n",
      "         [[0.4560]],\n",
      "\n",
      "         [[0.4060]]]], device='cuda:0'), 'std': tensor([[[[0.2290]],\n",
      "\n",
      "         [[0.2240]],\n",
      "\n",
      "         [[0.2250]]]], device='cuda:0'), 'axes': (0, 2, 3)}\n",
      "Pipeline: RandomResizedCrop -> RandomHorizontalFlip -> ColorJitter -> RandomGrayscale -> Rotate -- {'size': None, 'mode': 'bilinear', 'pad_mode': 'reflection', 'mode_mask': 'nearest', 'align_corners': True, 'p': 1.0} -> Normalize -- {'mean': tensor([[[[0.4850]],\n",
      "\n",
      "         [[0.4560]],\n",
      "\n",
      "         [[0.4060]]]], device='cuda:0'), 'std': tensor([[[[0.2290]],\n",
      "\n",
      "         [[0.2240]],\n",
      "\n",
      "         [[0.2250]]]], device='cuda:0'), 'axes': (0, 2, 3)}\n"
     ]
    }
   ],
   "source": [
    "dls = get_dls(resize, bs)\n",
    "model = create_swav_model(encoder, n_in=3)\n",
    "learn = Learner(dls, model, SWAVLoss(),\n",
    "                cbs=[SWAV(aug_func=get_batch_augs,\n",
    "                          crop_sizes=[size,int(3/4*size)], #4/3 - large to small crop size ratio is an important hyperparam for optimization! If we kept small crop 96 and just increased 128 to 192 training has hard time\n",
    "                          num_crops=[2,6],\n",
    "                          min_scales=[0.25,0.2],\n",
    "                          max_scales=[1.0,0.35],                \n",
    "                          rotate=True,\n",
    "                          rotate_deg=10,\n",
    "                          jitter=True,\n",
    "                          bw=True,\n",
    "                          blur=False\n",
    "                          ),\n",
    "                     TerminateOnNaNCallback(),\n",
    "                     ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I already had a model trained so for demonstration purposes I will just load it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<fastai.learner.Learner at 0x7f1fa53d36d0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs=100\n",
    "load_name = f'swav_iwang_sz{size}_epc{epochs}'\n",
    "learn.load(load_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Your Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once a model is trained you can simply save either the `model weights`, `encoder weights` or directly the `Learner` itself. We will use fastai and pytorch for this. You can skip this part if you already have either one of them saved already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_name = f'swav_iwang_sz{size}_epc{epochs}' \n",
    "learn.save(save_name) # saves whole model weights with_opt=True by default\n",
    "torch.save(learn.model.encoder.state_dict(), learn.path/learn.model_dir/f'{save_name}_encoder.pth') # saves only the encoder state dict\n",
    "learn.export(learn.path/learn.model_dir/f'{save_name}_export.pkl') # saves whole Learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can simply see all of them saved successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#3) [Path('models/swav_iwang_sz224_epc100_encoder.pth'),Path('models/swav_iwang_sz224_epc100.pth'),Path('models/swav_iwang_sz224_epc100_export.pkl')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(learn.path/learn.model_dir).ls().filter(lambda o: save_name in o.stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting with Learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your input items are compatible with the Dataloaders/dls you used during training then using the saved `Learner` object is the most straightforward solution for embedding extraction. For example the dls we used for training uses the following tfms for getting the input ready, so it expects a filename.\n",
    "\n",
    "```[PILImage.create, ToTensor, RandomResizedCrop(size, min_scale=1.)]```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's take a look at how to embed everything we have in an efficient manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = URLs.IMAGEWANG_160 if size <= 160 else URLs.IMAGEWANG\n",
    "source = untar_data(path)\n",
    "embedding_files = get_image_files(source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's create a dataframe to later use during visualization with some neat functional programming using L object from fastcore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"filename\":embedding_files})\n",
    "\n",
    "df['split'] = (embedding_files).map(lambda o: o.parent.parent.name)\n",
    "df['split'] = df['split'].replace(to_replace='imagewang', value='unsup')\n",
    "df['label'] = (embedding_files).map(lambda o: o.parent.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>split</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/root/.fastai/data/imagewang/train/n02096294/n02096294_2289.JPEG</td>\n",
       "      <td>train</td>\n",
       "      <td>n02096294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/root/.fastai/data/imagewang/train/n02096294/ILSVRC2012_val_00037999.JPEG</td>\n",
       "      <td>train</td>\n",
       "      <td>n02096294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/root/.fastai/data/imagewang/train/n02096294/n02096294_4119.JPEG</td>\n",
       "      <td>train</td>\n",
       "      <td>n02096294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/root/.fastai/data/imagewang/train/n02096294/n02096294_6689.JPEG</td>\n",
       "      <td>train</td>\n",
       "      <td>n02096294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/root/.fastai/data/imagewang/train/n02096294/n02096294_4839.JPEG</td>\n",
       "      <td>train</td>\n",
       "      <td>n02096294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26343</th>\n",
       "      <td>/root/.fastai/data/imagewang/unsup/n02105641_10903.JPEG</td>\n",
       "      <td>unsup</td>\n",
       "      <td>unsup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26344</th>\n",
       "      <td>/root/.fastai/data/imagewang/unsup/n02093754_898.JPEG</td>\n",
       "      <td>unsup</td>\n",
       "      <td>unsup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26345</th>\n",
       "      <td>/root/.fastai/data/imagewang/unsup/n02111889_17867.JPEG</td>\n",
       "      <td>unsup</td>\n",
       "      <td>unsup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26346</th>\n",
       "      <td>/root/.fastai/data/imagewang/unsup/n02111889_5998.JPEG</td>\n",
       "      <td>unsup</td>\n",
       "      <td>unsup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26347</th>\n",
       "      <td>/root/.fastai/data/imagewang/unsup/n02093754_5227.JPEG</td>\n",
       "      <td>unsup</td>\n",
       "      <td>unsup</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26348 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                        filename  \\\n",
       "0               /root/.fastai/data/imagewang/train/n02096294/n02096294_2289.JPEG   \n",
       "1      /root/.fastai/data/imagewang/train/n02096294/ILSVRC2012_val_00037999.JPEG   \n",
       "2               /root/.fastai/data/imagewang/train/n02096294/n02096294_4119.JPEG   \n",
       "3               /root/.fastai/data/imagewang/train/n02096294/n02096294_6689.JPEG   \n",
       "4               /root/.fastai/data/imagewang/train/n02096294/n02096294_4839.JPEG   \n",
       "...                                                                          ...   \n",
       "26343                    /root/.fastai/data/imagewang/unsup/n02105641_10903.JPEG   \n",
       "26344                      /root/.fastai/data/imagewang/unsup/n02093754_898.JPEG   \n",
       "26345                    /root/.fastai/data/imagewang/unsup/n02111889_17867.JPEG   \n",
       "26346                     /root/.fastai/data/imagewang/unsup/n02111889_5998.JPEG   \n",
       "26347                     /root/.fastai/data/imagewang/unsup/n02093754_5227.JPEG   \n",
       "\n",
       "       split      label  \n",
       "0      train  n02096294  \n",
       "1      train  n02096294  \n",
       "2      train  n02096294  \n",
       "3      train  n02096294  \n",
       "4      train  n02096294  \n",
       "...      ...        ...  \n",
       "26343  unsup      unsup  \n",
       "26344  unsup      unsup  \n",
       "26345  unsup      unsup  \n",
       "26346  unsup      unsup  \n",
       "26347  unsup      unsup  \n",
       "\n",
       "[26348 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can load the learner directly and create a test_dl for embedding extraction, also don't forget to disable cpu loading if you have a GPU!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = load_learner(\"./models/swav_iwang_sz224_epc100_export.pkl\", cpu=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline: RandomResizedCrop -> RandomHorizontalFlip -> ColorJitter -> RandomGrayscale -> Rotate -- {'size': None, 'mode': 'bilinear', 'pad_mode': 'reflection', 'mode_mask': 'nearest', 'align_corners': True, 'p': 1.0} -> Normalize -- {'mean': tensor([[[[0.4850]],\n",
       "\n",
       "         [[0.4560]],\n",
       "\n",
       "         [[0.4060]]]], device='cuda:0'), 'std': tensor([[[[0.2290]],\n",
       "\n",
       "         [[0.2240]],\n",
       "\n",
       "         [[0.2250]]]], device='cuda:0'), 'axes': (0, 2, 3)}"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.swav.augs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point you can either add a `Hook` to `learn.model.encoder` or overwrite `learn.model=learn.model.encoder`. I will overwrite it since I won't need the remaining part of the model in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (8): AdaptiveConcatPool2d(\n",
       "    (ap): AdaptiveAvgPool2d(output_size=1)\n",
       "    (mp): AdaptiveMaxPool2d(output_size=1)\n",
       "  )\n",
       "  (9): Flatten(full=False)\n",
       ")"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.model = learn.model.encoder; learn.model[-2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to disable SWAV callback and add back `Normalization` if any used during training.\n",
    "\n",
    "**Important** All of the augmentations including `Normalization` happen inside self-supervised learning callbacks. By default `imagenet_stats` are used but make sure to check "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#4) [TrainEvalCallback,Recorder,ProgressCallback,TerminateOnNaNCallback]"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.cbs = L([cb for cb in learn.cbs if cb.name != 'swav']); learn.cbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline: IntToFloatTensor -- {'div': 255.0, 'div_mask': 1}"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.dls.after_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline: IntToFloatTensor -- {'div': 255.0, 'div_mask': 1}"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_dl1 = learn.dls.test_dl(embedding_files); emb_dl1.after_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorImage([[[0.0627, 0.0627, 0.0627,  ..., 0.0275, 0.0275, 0.0275],\n",
       "         [0.0627, 0.0627, 0.0627,  ..., 0.0275, 0.0275, 0.0275],\n",
       "         [0.0627, 0.0627, 0.0627,  ..., 0.0275, 0.0275, 0.0275],\n",
       "         ...,\n",
       "         [0.1686, 0.1686, 0.1686,  ..., 0.1373, 0.1373, 0.1373],\n",
       "         [0.1725, 0.1686, 0.1686,  ..., 0.1373, 0.1373, 0.1373],\n",
       "         [0.1725, 0.1686, 0.1686,  ..., 0.1373, 0.1373, 0.1373]],\n",
       "\n",
       "        [[0.0275, 0.0275, 0.0275,  ..., 0.0275, 0.0275, 0.0275],\n",
       "         [0.0275, 0.0275, 0.0275,  ..., 0.0275, 0.0275, 0.0275],\n",
       "         [0.0275, 0.0275, 0.0275,  ..., 0.0275, 0.0275, 0.0275],\n",
       "         ...,\n",
       "         [0.0588, 0.0549, 0.0549,  ..., 0.0510, 0.0510, 0.0510],\n",
       "         [0.0588, 0.0549, 0.0549,  ..., 0.0510, 0.0510, 0.0510],\n",
       "         [0.0588, 0.0549, 0.0549,  ..., 0.0510, 0.0510, 0.0510]],\n",
       "\n",
       "        [[0.0392, 0.0392, 0.0392,  ..., 0.0275, 0.0275, 0.0275],\n",
       "         [0.0392, 0.0392, 0.0392,  ..., 0.0275, 0.0275, 0.0275],\n",
       "         [0.0392, 0.0392, 0.0392,  ..., 0.0275, 0.0275, 0.0275],\n",
       "         ...,\n",
       "         [0.0706, 0.0706, 0.0706,  ..., 0.0588, 0.0588, 0.0588],\n",
       "         [0.0706, 0.0706, 0.0706,  ..., 0.0588, 0.0588, 0.0588],\n",
       "         [0.0706, 0.0706, 0.0706,  ..., 0.0588, 0.0588, 0.0588]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb = emb_dl1.one_batch(); xb[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline: IntToFloatTensor -- {'div': 255.0, 'div_mask': 1}"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_dl2 = learn.dls.test_dl(embedding_files); emb_dl2.after_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline: IntToFloatTensor -- {'div': 255.0, 'div_mask': 1} -> Normalize -- {'mean': tensor([[[[0.4850]],\n",
       "\n",
       "         [[0.4560]],\n",
       "\n",
       "         [[0.4060]]]], device='cuda:0'), 'std': tensor([[[[0.2290]],\n",
       "\n",
       "         [[0.2240]],\n",
       "\n",
       "         [[0.2250]]]], device='cuda:0'), 'axes': (0, 2, 3)}"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_dl2.after_batch.add(Normalize.from_stats(*imagenet_stats)); emb_dl2.after_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorImage([[[-1.8439, -1.8439, -1.8439,  ..., -1.9980, -1.9980, -1.9980],\n",
       "         [-1.8439, -1.8439, -1.8439,  ..., -1.9980, -1.9980, -1.9980],\n",
       "         [-1.8439, -1.8439, -1.8439,  ..., -1.9980, -1.9980, -1.9980],\n",
       "         ...,\n",
       "         [-1.3815, -1.3815, -1.3815,  ..., -1.5185, -1.5185, -1.5185],\n",
       "         [-1.3644, -1.3815, -1.3815,  ..., -1.5185, -1.5185, -1.5185],\n",
       "         [-1.3644, -1.3815, -1.3815,  ..., -1.5185, -1.5185, -1.5185]],\n",
       "\n",
       "        [[-1.9132, -1.9132, -1.9132,  ..., -1.9132, -1.9132, -1.9132],\n",
       "         [-1.9132, -1.9132, -1.9132,  ..., -1.9132, -1.9132, -1.9132],\n",
       "         [-1.9132, -1.9132, -1.9132,  ..., -1.9132, -1.9132, -1.9132],\n",
       "         ...,\n",
       "         [-1.7731, -1.7906, -1.7906,  ..., -1.8081, -1.8081, -1.8081],\n",
       "         [-1.7731, -1.7906, -1.7906,  ..., -1.8081, -1.8081, -1.8081],\n",
       "         [-1.7731, -1.7906, -1.7906,  ..., -1.8081, -1.8081, -1.8081]],\n",
       "\n",
       "        [[-1.6302, -1.6302, -1.6302,  ..., -1.6824, -1.6824, -1.6824],\n",
       "         [-1.6302, -1.6302, -1.6302,  ..., -1.6824, -1.6824, -1.6824],\n",
       "         [-1.6302, -1.6302, -1.6302,  ..., -1.6824, -1.6824, -1.6824],\n",
       "         ...,\n",
       "         [-1.4907, -1.4907, -1.4907,  ..., -1.5430, -1.5430, -1.5430],\n",
       "         [-1.4907, -1.4907, -1.4907,  ..., -1.5430, -1.5430, -1.5430],\n",
       "         [-1.4907, -1.4907, -1.4907,  ..., -1.5430, -1.5430, -1.5430]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb = emb_dl2.one_batch(); xb[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline: IntToFloatTensor -- {'div': 255.0, 'div_mask': 1} -> Normalize -- {'mean': tensor([[[[0.4850]],\n",
       "\n",
       "         [[0.4560]],\n",
       "\n",
       "         [[0.4060]]]], device='cuda:0'), 'std': tensor([[[[0.2290]],\n",
       "\n",
       "         [[0.2240]],\n",
       "\n",
       "         [[0.2250]]]], device='cuda:0'), 'axes': (0, 2, 3)}"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_dl3 = learn.dls.test_dl(embedding_files); emb_dl3.after_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorImage([[[-1.8439, -1.8439, -1.8439,  ..., -1.9980, -1.9980, -1.9980],\n",
       "         [-1.8439, -1.8439, -1.8439,  ..., -1.9980, -1.9980, -1.9980],\n",
       "         [-1.8439, -1.8439, -1.8439,  ..., -1.9980, -1.9980, -1.9980],\n",
       "         ...,\n",
       "         [-1.3815, -1.3815, -1.3815,  ..., -1.5185, -1.5185, -1.5185],\n",
       "         [-1.3644, -1.3815, -1.3815,  ..., -1.5185, -1.5185, -1.5185],\n",
       "         [-1.3644, -1.3815, -1.3815,  ..., -1.5185, -1.5185, -1.5185]],\n",
       "\n",
       "        [[-1.9132, -1.9132, -1.9132,  ..., -1.9132, -1.9132, -1.9132],\n",
       "         [-1.9132, -1.9132, -1.9132,  ..., -1.9132, -1.9132, -1.9132],\n",
       "         [-1.9132, -1.9132, -1.9132,  ..., -1.9132, -1.9132, -1.9132],\n",
       "         ...,\n",
       "         [-1.7731, -1.7906, -1.7906,  ..., -1.8081, -1.8081, -1.8081],\n",
       "         [-1.7731, -1.7906, -1.7906,  ..., -1.8081, -1.8081, -1.8081],\n",
       "         [-1.7731, -1.7906, -1.7906,  ..., -1.8081, -1.8081, -1.8081]],\n",
       "\n",
       "        [[-1.6302, -1.6302, -1.6302,  ..., -1.6824, -1.6824, -1.6824],\n",
       "         [-1.6302, -1.6302, -1.6302,  ..., -1.6824, -1.6824, -1.6824],\n",
       "         [-1.6302, -1.6302, -1.6302,  ..., -1.6824, -1.6824, -1.6824],\n",
       "         ...,\n",
       "         [-1.4907, -1.4907, -1.4907,  ..., -1.5430, -1.5430, -1.5430],\n",
       "         [-1.4907, -1.4907, -1.4907,  ..., -1.5430, -1.5430, -1.5430],\n",
       "         [-1.4907, -1.4907, -1.4907,  ..., -1.5430, -1.5430, -1.5430]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb = emb_dl3.one_batch(); xb[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fastai.data.core.DataLoaders"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(learn.dls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fastai.data.core.TfmdDL"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(emb_dl1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0x7f4a2cde2350'"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hex(id(learn.dls.valid.after_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(139956557032656, 139956338041104, 139956568997712, 139956564855888)"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id(emb_dl1.after_batch), id(emb_dl2.after_batch), id(emb_dl3.after_batch), id(learn.dls.after_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embeddings, _ = learn.get_preds(dl=emb_dl, act=noop) # we need to set activation as no operation because it's set for classification output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the embeddings and finally save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorBase([[ 0.9732,  5.4093,  1.0725,  ...,  0.2657,  0.2560,  1.6736],\n",
       "         [ 2.0293,  3.4570,  0.4585,  ...,  0.1125,  0.5919,  1.3086],\n",
       "         [ 4.1430,  5.6190, 18.5848,  ...,  0.5565,  0.6212,  1.8319],\n",
       "         ...,\n",
       "         [ 2.3026,  1.6281,  3.4819,  ...,  0.7759,  0.5221,  0.3157],\n",
       "         [ 5.6842,  2.0450,  0.6188,  ...,  0.2059,  0.1493,  0.4630],\n",
       "         [ 3.0677,  4.3904,  2.5808,  ...,  0.3809,  0.4606,  0.9369]]),\n",
       " torch.Size([26348, 1024]),\n",
       " TensorBase([102.1530, 123.4563, 147.9856,  ..., 202.9648, 139.4411, 107.1795]))"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings,  embeddings.shape, embeddings.norm(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dir = Path(\"./embeddings\")\n",
    "if not emb_dir.exists(): emb_dir.mkdir()\n",
    "torch.save(embeddings, emb_dir/'swav_iwang_embeddings.pth')\n",
    "df.to_csv(emb_dir/'iwang.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes you might have a inputs which are not compatible with training dls. In those cases, you can create your own custom dls but making sure to use the same item tfms you used during training, such as `RandomResizedCrop`, image resolution and `Normalize`. I will add a dummy function to my tfms list which will mimic your own custom scenario for reading and generating inputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_func(x):\n",
    "    # your own transform for reading\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = get_image_files(source)\n",
    "tfms = [[dummy_func, PILImage.create, ToTensor, RandomResizedCrop(size, min_scale=1.)], \n",
    "        [parent_label, Categorize()]]\n",
    "\n",
    "dsets = Datasets(files, tfms=tfms, splits=RandomSplitter(valid_pct=0.1)(files))\n",
    "batch_tfms = [IntToFloatTensor, Normalize.from_stats(*imagenet_stats)]\n",
    "dls = dsets.dataloaders(bs=bs, num_workers=4, after_batch=batch_tfms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dl = dls.test_dl(embedding_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline: IntToFloatTensor -- {'div': 255.0, 'div_mask': 1} -> Normalize -- {'mean': tensor([[[[0.4850]],\n",
       "\n",
       "         [[0.4560]],\n",
       "\n",
       "         [[0.4060]]]], device='cuda:0'), 'std': tensor([[[[0.2290]],\n",
       "\n",
       "         [[0.2240]],\n",
       "\n",
       "         [[0.2250]]]], device='cuda:0'), 'axes': (0, 2, 3)}"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_dl.after_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embeddings, _ = learn.get_preds(dl=emb_dl, act=noop) # we need to set activation as no operation because it's set for classification output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorBase([[ 0.9235,  6.0359,  1.4319,  ...,  0.2793,  0.2447,  1.6558],\n",
       "         [ 1.0574,  1.2625,  0.1994,  ...,  0.1196,  0.6004,  1.2695],\n",
       "         [ 4.5818,  4.8277, 19.6919,  ...,  0.5332,  0.7392,  1.7014],\n",
       "         ...,\n",
       "         [ 1.8617,  1.8730,  4.1641,  ...,  0.8961,  0.5835,  0.2820],\n",
       "         [ 5.7593,  1.8655,  0.4892,  ...,  0.2413,  0.1738,  0.4212],\n",
       "         [ 2.1063,  3.4830,  2.9620,  ...,  0.3681,  0.4959,  0.9483]]),\n",
       " torch.Size([26348, 1024]),\n",
       " TensorBase([103.5237, 113.3170, 138.3017,  ..., 181.9094, 127.4829, 102.1634]))"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings,  embeddings.shape, embeddings.norm(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
