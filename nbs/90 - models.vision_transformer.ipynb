{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438380dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#skip\n",
    "! [ -e /content ] && pip install -Uqq self-supervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb1e6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp models.vision_transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d17c87",
   "metadata": {},
   "source": [
    "# ViT\n",
    "\n",
    "Variety of vision transformers with modifications which are not available by timm by default.\n",
    "\n",
    "References: \n",
    "\n",
    "- https://github.com/facebookresearch/dino\n",
    "- https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0eb697c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0438608a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import torch\n",
    "from torch import nn\n",
    "import math\n",
    "from functools import partial\n",
    "from fastcore.meta import delegates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55383d6c",
   "metadata": {},
   "source": [
    "[Why truncated normal initialization?](https://stats.stackexchange.com/questions/228670/what-is-the-benefit-of-the-truncated-normal-distribution-in-initializing-weights')\n",
    "\n",
    "\n",
    "Neurons would be dead below < -2 and above > 2 since [GeLU](https://arxiv.org/pdf/1606.08415.pdf) can be approximated with input times the sigmoid function:\n",
    "\n",
    "$$\n",
    "x\\sigma(1.702x)\n",
    "$$\n",
    "\n",
    "so truncated normal helps with that and makes sures all neurons are updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d34a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _no_grad_trunc_normal_(tensor, mean, std, a, b):\n",
    "    'https://en.wikipedia.org/wiki/Normal_distribution'\n",
    "    # Cut & paste from PyTorch official master until it's in a few official releases - RW\n",
    "    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf\n",
    "    def norm_cdf(x):\n",
    "        # Computes standard normal cumulative distribution function\n",
    "        return (1. + math.erf(x / math.sqrt(2.))) / 2.\n",
    "\n",
    "    if (mean < a - 2 * std) or (mean > b + 2 * std):\n",
    "        warnings.warn(\"mean is more than 2 std from [a, b] in nn.init.trunc_normal_. \"\n",
    "                      \"The distribution of values may be incorrect.\",\n",
    "                      stacklevel=2)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Values are generated by using a truncated uniform distribution and\n",
    "        # then using the inverse CDF for the normal distribution.\n",
    "        # Get upper and lower cdf values\n",
    "        l = norm_cdf((a - mean) / std)\n",
    "        u = norm_cdf((b - mean) / std)\n",
    "\n",
    "        # Uniformly fill tensor with values from [l, u], then translate to\n",
    "        # [2l-1, 2u-1].\n",
    "        tensor.uniform_(2 * l - 1, 2 * u - 1)\n",
    "\n",
    "        # Use inverse cdf transform for normal distribution to get truncated\n",
    "        # standard normal\n",
    "        tensor.erfinv_()\n",
    "\n",
    "        # Transform to proper mean, std\n",
    "        tensor.mul_(std * math.sqrt(2.))\n",
    "        tensor.add_(mean)\n",
    "\n",
    "        # Clamp to ensure it's in the proper range\n",
    "        tensor.clamp_(min=a, max=b)\n",
    "        return tensor\n",
    "\n",
    "\n",
    "def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):\n",
    "    # type: (Tensor, float, float, float, float) -> Tensor\n",
    "    return _no_grad_trunc_normal_(tensor, mean, std, a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c9d2ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "trunc_dist = [trunc_normal_(torch.tensor([0.]),std=1.5,a=-2,b=2).item() for o in range(5000)]\n",
    "plt.hist(trunc_dist, bins=30);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0791991",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def drop_path(x, drop_prob: float = 0., training: bool = False):\n",
    "    if drop_prob == 0. or not training:\n",
    "        return x\n",
    "    keep_prob = 1 - drop_prob\n",
    "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
    "    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
    "    random_tensor.floor_()  # binarize\n",
    "    output = x.div(keep_prob) * random_tensor\n",
    "    return output\n",
    "\n",
    "\n",
    "class DropPath(nn.Module):\n",
    "    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n",
    "    \"\"\"\n",
    "    def __init__(self, drop_prob=None):\n",
    "        super(DropPath, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        return drop_path(x, self.drop_prob, self.training)\n",
    "\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x, attn\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = Attention(\n",
    "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "\n",
    "    def forward(self, x, return_attention=False):\n",
    "        y, attn = self.attn(self.norm1(x))\n",
    "        if return_attention:\n",
    "            return attn\n",
    "        x = x + self.drop_path(y)\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        return x\n",
    "\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\" Image to Patch Embedding\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        num_patches = (img_size // patch_size) * (img_size // patch_size)\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    \"\"\" Vision Transformer \"\"\"\n",
    "    def __init__(self, img_size=[224], patch_size=16, in_chans=3, num_classes=0, embed_dim=768, depth=12,\n",
    "                 num_heads=12, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop_rate=0., attn_drop_rate=0.,\n",
    "                 drop_path_rate=0., norm_layer=nn.LayerNorm, **kwargs):\n",
    "        super().__init__()\n",
    "        self.num_features = self.embed_dim = embed_dim\n",
    "\n",
    "        self.patch_embed = PatchEmbed(\n",
    "            img_size=img_size[0], patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(\n",
    "                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)\n",
    "            for i in range(depth)])\n",
    "        self.norm = norm_layer(embed_dim)\n",
    "\n",
    "        # Classifier head\n",
    "        self.head = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
    "\n",
    "        trunc_normal_(self.pos_embed, std=.02)\n",
    "        trunc_normal_(self.cls_token, std=.02)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def interpolate_pos_encoding(self, x, w, h):\n",
    "        npatch = x.shape[1] - 1\n",
    "        N = self.pos_embed.shape[1] - 1\n",
    "        if npatch == N and w == h:\n",
    "            return self.pos_embed\n",
    "        class_pos_embed = self.pos_embed[:, 0]\n",
    "        patch_pos_embed = self.pos_embed[:, 1:]\n",
    "        dim = x.shape[-1]\n",
    "        w0 = w // self.patch_embed.patch_size\n",
    "        h0 = h // self.patch_embed.patch_size\n",
    "        # we add a small number to avoid floating point error in the interpolation\n",
    "        # see discussion at https://github.com/facebookresearch/dino/issues/8\n",
    "        w0, h0 = w0 + 0.1, h0 + 0.1\n",
    "        patch_pos_embed = nn.functional.interpolate(\n",
    "            patch_pos_embed.reshape(1, int(math.sqrt(N)), int(math.sqrt(N)), dim).permute(0, 3, 1, 2),\n",
    "            scale_factor=(w0 / math.sqrt(N), h0 / math.sqrt(N)),\n",
    "            mode='bicubic',\n",
    "        )\n",
    "        assert int(w0) == patch_pos_embed.shape[-2] and int(h0) == patch_pos_embed.shape[-1]\n",
    "        patch_pos_embed = patch_pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n",
    "        return torch.cat((class_pos_embed.unsqueeze(0), patch_pos_embed), dim=1)\n",
    "\n",
    "    def prepare_tokens(self, x):\n",
    "        B, nc, w, h = x.shape\n",
    "        x = self.patch_embed(x)  # patch linear embedding\n",
    "\n",
    "        # add the [CLS] token to the embed patch tokens\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "\n",
    "        # add positional encoding to each token\n",
    "        x = x + self.interpolate_pos_encoding(x, w, h)\n",
    "\n",
    "        return self.pos_drop(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.prepare_tokens(x)\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        x = self.norm(x)\n",
    "        return x[:, 0]\n",
    "\n",
    "    def get_last_selfattention(self, x):\n",
    "        x = self.prepare_tokens(x)\n",
    "        for i, blk in enumerate(self.blocks):\n",
    "            if i < len(self.blocks) - 1:\n",
    "                x = blk(x)\n",
    "            else:\n",
    "                # return attention of the last block\n",
    "                return blk(x, return_attention=True)\n",
    "\n",
    "    def get_intermediate_layers(self, x, n=1):\n",
    "        x = self.prepare_tokens(x)\n",
    "        # we return the output tokens from the `n` last blocks\n",
    "        output = []\n",
    "        for i, blk in enumerate(self.blocks):\n",
    "            x = blk(x)\n",
    "            if len(self.blocks) - i <= n:\n",
    "                output.append(self.norm(x))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2e4ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class MultiCropWrapper(nn.Module):\n",
    "    \"\"\"\n",
    "    Perform forward pass separately on each resolution input.\n",
    "    The inputs corresponding to a single resolution are clubbed and single\n",
    "    forward is run on the same resolution inputs. Hence we do several\n",
    "    forward passes = number of different resolutions used. We then\n",
    "    concatenate all the output features and run the head forward on these\n",
    "    concatenated features.\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder):\n",
    "        super(MultiCropWrapper, self).__init__()\n",
    "        self.encoder = encoder\n",
    "\n",
    "    def forward(self, x):\n",
    "        # convert to list\n",
    "        if not isinstance(x, list):\n",
    "            x = [x]\n",
    "        idx_crops = torch.cumsum(torch.unique_consecutive(\n",
    "            torch.tensor([inp.shape[-1] for inp in x]),\n",
    "            return_counts=True,\n",
    "        )[1], 0)\n",
    "        start_idx = 0\n",
    "        for end_idx in idx_crops:\n",
    "            _out = self.encoder(torch.cat(x[start_idx: end_idx]))\n",
    "            if start_idx == 0:\n",
    "                output = _out\n",
    "            else:\n",
    "                output = torch.cat((output, _out))\n",
    "            start_idx = end_idx\n",
    "        # Run the head forward on the concatenated features.\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70566e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 4\n",
    "x_large = [torch.randn(4,3,224,224)]*2\n",
    "x_small = [torch.randn(16,3,96,96)]*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dac4acc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([4, 3, 224, 224]),\n",
       " torch.Size([4, 3, 224, 224]),\n",
       " torch.Size([16, 3, 96, 96]),\n",
       " torch.Size([16, 3, 96, 96]),\n",
       " torch.Size([16, 3, 96, 96]),\n",
       " torch.Size([16, 3, 96, 96])]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x_large + x_small; [xi.size() for xi in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df68d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_encoder = VisionTransformer(patch_size=32, embed_dim=128, depth=4, num_heads=4, mlp_ratio=4,\n",
    "                                qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53fed25",
   "metadata": {},
   "outputs": [],
   "source": [
    "vit = MultiCropWrapper(vit_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ced4ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3063: UserWarning: Default upsampling behavior when mode=bicubic is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode))\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3103: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor changed \"\n"
     ]
    }
   ],
   "source": [
    "out = vit(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8547ed9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed28b4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "but = ['embed_dim', 'depth', 'num_heads', 'mlp_ratio', 'qkv_bias','norm_layer','eps']\n",
    "\n",
    "@delegates(VisionTransformer, but=but)\n",
    "def deit_tiny(patch_size=16, **kwargs):\n",
    "    model = VisionTransformer(\n",
    "        patch_size=patch_size, embed_dim=192, depth=12, num_heads=3, mlp_ratio=4,\n",
    "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model\n",
    "\n",
    "@delegates(VisionTransformer, but=but)\n",
    "def deit_small(patch_size=16, **kwargs):\n",
    "    model = VisionTransformer(\n",
    "        patch_size=patch_size, embed_dim=384, depth=12, num_heads=6, mlp_ratio=4,\n",
    "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model\n",
    "\n",
    "@delegates(VisionTransformer, but=but)\n",
    "def vit_base(patch_size=16, **kwargs):\n",
    "    model = VisionTransformer(\n",
    "        patch_size=patch_size, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4,\n",
    "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65b6cf7",
   "metadata": {},
   "source": [
    "## Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56f115d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 01 - augmentations.ipynb.\n",
      "Converted 02 - layers.ipynb.\n",
      "Converted 03 - distributed.ipynb.\n",
      "Converted 10 - simclr.ipynb.\n",
      "Converted 11 - moco.ipynb.\n",
      "Converted 12 - byol.ipynb.\n",
      "Converted 13 - swav.ipynb.\n",
      "Converted 14 - barlow_twins.ipynb.\n",
      "Converted 15 - dino.ipynb.\n",
      "Converted 20 - clip.ipynb.\n",
      "Converted 21 - clip-moco.ipynb.\n",
      "Converted 70 - vision.metrics.ipynb.\n",
      "Converted 90 - models.vision_transformer.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be713c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
