{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e56f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#skip\n",
    "! [ -e /content ] && pip install -Uqq self-supervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772e0226",
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp vision.dino"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a855aa0b",
   "metadata": {},
   "source": [
    "# DINO\n",
    "\n",
    "> DINO: [Emerging Properties in Self-Supervised Vision Transformers](https://arxiv.org/pdf/2104.14294.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d761fcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastai.vision.all import *\n",
    "from self_supervised.augmentations import *\n",
    "from self_supervised.layers import *\n",
    "from self_supervised.models.vision_transformer import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547a0190",
   "metadata": {},
   "source": [
    "## Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c07237",
   "metadata": {},
   "source": [
    "#### DINO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0159656b",
   "metadata": {},
   "source": [
    "![DINO Framework](images/dino.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc77c8e4",
   "metadata": {},
   "source": [
    "**Absract**: In this paper, we question if self-supervised learning provides\n",
    "new properties to Vision Transformer (ViT) [18] that\n",
    "stand out compared to convolutional networks (convnets).\n",
    "Beyond the fact that adapting self-supervised methods to this\n",
    "architecture works particularly well, we make the following\n",
    "observations: first, self-supervised ViT features contain\n",
    "explicit information about the semantic segmentation of an\n",
    "image, which does not emerge as clearly with supervised\n",
    "ViTs, nor with convnets. Second, these features are also excellent\n",
    "k-NN classifiers, reaching 78.3% top-1 on ImageNet\n",
    "with a small ViT. Our study also underlines the importance of\n",
    "momentum encoder [31], multi-crop training [10], and the\n",
    "use of small patches with ViTs. We implement our findings\n",
    "into a simple self-supervised method, called DINO, which\n",
    "we interpret as a form of self-distillation with no labels.\n",
    "We show the synergy between DINO and ViTs by achieving\n",
    "80.1% top-1 on ImageNet in linear evaluation with ViT-Base."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08578ac6",
   "metadata": {},
   "source": [
    "**Own Summary**: In this paper authors show effectiveness of the combination of DINO framework and ViT based architectures such as ViT and DEIT. There is no contrastive training nor negative pairs, rather ideas such as momentum encoder and multi-crop augmention from `BYOL` and `SWAV` respectively are adapted. They use distillation with a teacher-student configuration, and avoid representation collapse by centering and sharpening target distributions generated by the teacher. 2 large views (~50%) are used as targets and all views (2 large, 4 small) are used for predictions similar to `SWAV`. Centering values and teacher parameters are updated via ema (exponential moving average)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a30dbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class DINOHead(nn.Module):\n",
    "    '''\n",
    "    https://pytorch.org/docs/stable/generated/torch.nn.utils.weight_norm.html\n",
    "    https://pytorch.org/docs/stable/generated/torch.nn.GELU.html\n",
    "    '''\n",
    "    def __init__(self, in_dim, out_dim, use_bn=False, norm_last_layer=True, nlayers=3, hidden_dim=2048, bottleneck_dim=256):\n",
    "        super().__init__()\n",
    "        nlayers = max(nlayers, 1)\n",
    "        if nlayers == 1:\n",
    "            self.mlp = nn.Linear(in_dim, bottleneck_dim)\n",
    "        else:\n",
    "            layers = [nn.Linear(in_dim, hidden_dim)]\n",
    "            if use_bn:\n",
    "                layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "            layers.append(nn.GELU())\n",
    "            for _ in range(nlayers - 2):\n",
    "                layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "                if use_bn:\n",
    "                    layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "                layers.append(nn.GELU())\n",
    "            layers.append(nn.Linear(hidden_dim, bottleneck_dim))\n",
    "            self.mlp = nn.Sequential(*layers)\n",
    "        self.apply(self._init_weights)\n",
    "        self.last_layer = nn.utils.weight_norm(nn.Linear(bottleneck_dim, out_dim, bias=False))\n",
    "        self.last_layer.weight_g.data.fill_(1)\n",
    "        if norm_last_layer:\n",
    "            self.last_layer.weight_g.requires_grad = False\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.mlp(x)\n",
    "        x = nn.functional.normalize(x, dim=-1, p=2)\n",
    "        x = self.last_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8444bd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 4\n",
    "x_large = [torch.randn(4,3,224,224)]*2\n",
    "x_small = [torch.randn(4,3,96,96)]*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0940a1d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([4, 3, 224, 224]),\n",
       " torch.Size([4, 3, 224, 224]),\n",
       " torch.Size([4, 3, 96, 96]),\n",
       " torch.Size([4, 3, 96, 96]),\n",
       " torch.Size([4, 3, 96, 96]),\n",
       " torch.Size([4, 3, 96, 96])]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x_large + x_small; [xi.size() for xi in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597855d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_encoder = VisionTransformer(patch_size=32, embed_dim=128, depth=4, num_heads=4, mlp_ratio=4,\n",
    "                          qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6))\n",
    "teacher_encoder = VisionTransformer(patch_size=32, embed_dim=128, depth=4, num_heads=4, mlp_ratio=4,\n",
    "                          qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1214b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_encoder = MultiCropWrapper(student_encoder)\n",
    "teacher_encoder = MultiCropWrapper(teacher_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a24506b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_dim = 2**10; out_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02aa1b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_head = DINOHead(student_encoder.encoder.embed_dim, out_dim, norm_last_layer=True)\n",
    "teacher_head = DINOHead(student_encoder.encoder.embed_dim, out_dim, norm_last_layer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0811d151",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DINOHead(\n",
       "  (mlp): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=2048, bias=True)\n",
       "    (1): GELU()\n",
       "    (2): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "    (3): GELU()\n",
       "    (4): Linear(in_features=2048, out_features=256, bias=True)\n",
       "  )\n",
       "  (last_layer): Linear(in_features=256, out_features=1024, bias=False)\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6fb27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.equal(student_head.last_layer.weight, student_head.last_layer.weight_g*student_head.last_layer.weight_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0481e5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_model = nn.Sequential(student_encoder, student_head)\n",
    "teacher_model = nn.Sequential(teacher_encoder, teacher_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78bd709",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_model.load_state_dict(student_model.state_dict());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb23218",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param_t in teacher_model.parameters(): param_t.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bd994a",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = student_model(x)\n",
    "with torch.no_grad(): \n",
    "    targs = teacher_model(x_large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60eb101",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([24, 1024]), torch.Size([8, 1024]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.shape, targs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207529eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43f9f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmom, tmom = 0.999,0.999\n",
    "tps,tpt = 0.4,0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fca4324",
   "metadata": {},
   "outputs": [],
   "source": [
    "C = torch.zeros(1,out_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b6d045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# center and sharpen\n",
    "targs = F.softmax(targs - C / tpt, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5e5aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sharpen\n",
    "preds = F.log_softmax(preds / tps, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e270571e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 1024]), torch.Size([24, 1024]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targs.shape, preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3ad339",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_targs, n_preds = targs.size(0)//bs, preds.size(0)//bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d2c302",
   "metadata": {},
   "outputs": [],
   "source": [
    "targs1 = targs.view(bs, targs.size(0)//bs, -1)\n",
    "preds1 = preds.view(bs, preds.size(0)//bs, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f467216d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 2, 1024]), torch.Size([4, 6, 1024]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targs1.shape, preds1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bea005",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = (-targs1.unsqueeze(2)*preds1.unsqueeze(1)).sum(-1).mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc783400",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = torch.ones_like(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84e2a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask[0,0] = 0\n",
    "mask[1,1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74363cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.9360, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss[mask.bool()].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce13138c",
   "metadata": {},
   "outputs": [],
   "source": [
    "targs,preds = targs.chunk(n_targs), preds.chunk(n_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83641b10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 6)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(targs), len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc84608",
   "metadata": {},
   "outputs": [],
   "source": [
    "npairs = len(targs)*(len(preds)-1) \n",
    "loss = 0\n",
    "for ti in range(len(targs)):\n",
    "    for pi in range(len(preds)):\n",
    "        if ti != pi:\n",
    "            loss += (-targs[ti]*preds[pi]).sum(-1).mean() / npairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126f5a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@delegates(get_multi_aug_pipelines, but=['n', 'size', 'resize_scale'])\n",
    "def get_dino_aug_pipelines(num_crops=(2,4), crop_sizes=(224,96), min_scales=(0.4,0.05), max_scales=(1.,0.4), **kwargs): \n",
    "    aug_pipelines = []\n",
    "    for nc, size, mins, maxs in zip(num_crops, crop_sizes, min_scales, max_scales):\n",
    "        aug_pipelines += get_multi_aug_pipelines(n=nc, size=size, resize_scale=(mins,maxs), **kwargs)\n",
    "    return aug_pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db64ddaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_pipelines = get_dino_aug_pipelines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf69609",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f241358",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3998, 0.1460, 0.1550, 0.1327, 0.1665]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(x,dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753cfd1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000e+00, 1.1447e-11, 5.1287e-11, 1.0637e-12, 3.0646e-10]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(x/0.04,dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce9ee73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[9.9971e-01, 4.2010e-05, 7.6538e-05, 1.6240e-05, 1.5647e-04]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(x/0.1,dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b3b080",
   "metadata": {},
   "outputs": [],
   "source": [
    "warmup_teacher_temp\n",
    "teacher_temp\n",
    "warmup_teacher_temp_epochs\n",
    "nepochs =\n",
    "np.concatenate((\n",
    "            np.linspace(warmup_teacher_temp,\n",
    "                        teacher_temp, warmup_teacher_temp_epochs),\n",
    "            np.ones(nepochs - warmup_teacher_temp_epochs) * teacher_temp\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8228775",
   "metadata": {},
   "outputs": [],
   "source": [
    "sched_lin(0.04,0.07,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d435f0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65536"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2**16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71911f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class DINO(Callback):\n",
    "    order,run_valid = 9,True\n",
    "    def __init__(self, aug_pipelines, large_crop_ids=[0,1],\n",
    "                         cmom=0.9, tmom=0.996,\n",
    "                         tpt_warmup=0.04, tpt_warmup_pct=0.3, tpt_sched=sched_lin, tpt=0.07,\n",
    "                         tps=0.1,\n",
    "                         print_augs=False):\n",
    "        \"\"\"\n",
    "        Refer to original repo: \n",
    "        https://github.com/facebookresearch/dino/blob/0be6e112dd579203caaa1d0f066e29ca536f76dd/main_dino.py#L41\n",
    "            cmom:           Center update momentum.\n",
    "            tmom:           Teacher update momentum. Set larger, e.g. 0.9995, for small batches.\n",
    "            tpt_warmup:     Warm up starting temperature\n",
    "            tpt_warmup_pct: Percentage of training for warmup\n",
    "            tpt_sched:      Warm up scheduler, e.g. sched_lin, sched_cos, sched_exp\n",
    "            tpt:            Teacher temperature after warm up. Decrease if training loss does not decrease.\n",
    "                            Smaller temperature means more sharpening.\n",
    "            tps:            Student temperature.\n",
    "        \"\"\"\n",
    "        store_attr('K,large_crop_ids,cmom,tmom,tpt,tps')\n",
    "        self.augs = aug_pipelines\n",
    "        if print_augs: \n",
    "            for aug in self.augs: print(aug)\n",
    "        self.C = torch.zeros(1,K)\n",
    "    \n",
    "    \n",
    "    def before_fit(self):\n",
    "        \"Create teacher model as a copy of student\"\n",
    "        self.teacher_model = deepcopy(self.learn.model).to(self.dls.device)\n",
    "        for param_t in self.teacher_model.parameters(): param_t.requires_grad = False \n",
    "        self.learn.loss_func = self.lf\n",
    "        self.C = torch.zeros(1,num_features_model(self.learn.model))\n",
    "    \n",
    "    \n",
    "    def before_train(self):    self.teacher_model.train()\n",
    "    def before_validate(self): self.teacher_model.eval()\n",
    "    def before_batch(self):\n",
    "        \"Augment multi crop views\"\n",
    "        self.bs = self.x.size(0)\n",
    "        self.learn.xb = ([aug(self.x) for aug in self.augs],)\n",
    "        x_large = [self.learn.xb[0][i] for i in self.large_crop_ids]\n",
    "        self.cb = torch.cat(x_large).mean(0)\n",
    "        with torch.no_grad(): self.yb = (self.teacher_model(x_large),)\n",
    "\n",
    "            \n",
    "    def _momentum_update_teacher(self):\n",
    "        for param_q, param_t in zip(self.learn.model.parameters(), self.teacher_model.parameters()):\n",
    "            param_t.data = param_t.data * self.tmom + param_t.data * (1. - self.tmom)\n",
    "    \n",
    "    \n",
    "    def _momentum_update_center(self):\n",
    "        self.C = self.C*self.cmom + self.cb*(1-self.cmom)\n",
    "            \n",
    "            \n",
    "    def after_step(self):\n",
    "        \"Center and teacher updates\"\n",
    "        self._momentum_update_center()\n",
    "        self._momentum_update_teacher()\n",
    "\n",
    "        \n",
    "    def lf(self, pred, *yb):\n",
    "        \"Multi crop cross entropy loss: -qlog(p)\"\n",
    "        pred = F.log_softmax(pred / tps, dim=-1)\n",
    "        yb = F.softmax(yb - C / tpt, dim=-1)\n",
    "        n_targs, n_preds = yb.size(0)//bs, pred.size(0)//bs\n",
    "        yb,pred = yb.chunk(n_targs), pred.chunk(n_preds)\n",
    "        npairs = len(targs)*(len(preds)-1) \n",
    "        loss = 0\n",
    "        for ti in range(len(targs)):\n",
    "            for pi in range(len(preds)):\n",
    "                if ti != pi:\n",
    "                    loss += (-targs[ti]*preds[pi]).sum(-1).mean() / npairs\n",
    "        return loss\n",
    "        \n",
    "        \n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def show(self, n=1):\n",
    "        xbs = self.learn.xb[0]\n",
    "        idxs = np.random.choice(range(self.bs), n, False)\n",
    "        images = [aug.decode(xb.to('cpu').clone()).clamp(0, 1)[i] \n",
    "                  for i in idxs\n",
    "                  for xb, aug in zip(xbs, self.augs)]\n",
    "        return show_batch(images[0], None, images, max_n=len(images), nrows=n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128880df",
   "metadata": {},
   "source": [
    "## Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df7ae96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 01 - augmentations.ipynb.\n",
      "Converted 02 - layers.ipynb.\n",
      "Converted 03 - distributed.ipynb.\n",
      "Converted 10 - simclr.ipynb.\n",
      "Converted 11 - moco.ipynb.\n",
      "Converted 12 - byol.ipynb.\n",
      "Converted 13 - swav.ipynb.\n",
      "Converted 14 - barlow_twins.ipynb.\n",
      "Converted 15 - dino.ipynb.\n",
      "Converted 20 - clip.ipynb.\n",
      "Converted 21 - clip-moco.ipynb.\n",
      "Converted 70 - vision.metrics.ipynb.\n",
      "Converted 90 - models.vision_transformer.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
