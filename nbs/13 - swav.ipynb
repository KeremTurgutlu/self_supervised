{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp vision.swav"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SwAV\n",
    "\n",
    "> **SwAV**: [Unsupervised Learning of Visual Features by Contrasting Cluster Assignments](https://arxiv.org/pdf/2006.09882.pdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastai.vision.all import *\n",
    "from self_supervised.augmentations import *\n",
    "from self_supervised.layers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SwAV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![SwAV](images/swav.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Absract**: Unsupervised image representations have significantly reduced the gap with supervised\n",
    "pretraining, notably with the recent achievements of contrastive learning\n",
    "methods. These contrastive methods typically work online and rely on a large number\n",
    "of explicit pairwise feature comparisons, which is computationally challenging.\n",
    "In this paper, we propose an online algorithm, SwAV, that takes advantage of contrastive\n",
    "methods without requiring to compute pairwise comparisons. Specifically,\n",
    "our method simultaneously clusters the data while enforcing consistency between\n",
    "cluster assignments produced for different augmentations (or “views”) of the same\n",
    "image, instead of comparing features directly as in contrastive learning. Simply put,\n",
    "we use a “swapped” prediction mechanism where we predict the code of a view\n",
    "from the representation of another view. Our method can be trained with large and\n",
    "small batches and can scale to unlimited amounts of data. Compared to previous\n",
    "contrastive methods, our method is more memory efficient since it does not require\n",
    "a large memory bank or a special momentum network. In addition, we also propose\n",
    "a new data augmentation strategy, multi-crop, that uses a mix of views with\n",
    "different resolutions in place of two full-resolution views, without increasing the\n",
    "memory or compute requirements. We validate our findings by achieving 75:3%\n",
    "top-1 accuracy on ImageNet with ResNet-50, as well as surpassing supervised\n",
    "pretraining on all the considered transfer tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SwAVModel(Module):\n",
    "    def __init__(self,encoder,projector,prototypes): \n",
    "        self.encoder,self.projector,self.prototypes = encoder,projector,prototypes\n",
    "    \n",
    "    def forward(self, inputs): \n",
    "        \n",
    "        if not isinstance(inputs, list): inputs = [inputs]\n",
    "            \n",
    "        crop_idxs = torch.cumsum(torch.unique_consecutive(\n",
    "                                torch.tensor([inp.shape[-1] for inp in inputs]),\n",
    "                                return_counts=True)[1], 0)\n",
    "\n",
    "        start_idx = 0\n",
    "        for idx in crop_idxs:\n",
    "            _z = self.encoder(torch.cat(inputs[start_idx: idx]))\n",
    "            if not start_idx: z = _z\n",
    "            else:             z = torch.cat((z, _z))\n",
    "            start_idx = idx\n",
    "        \n",
    "        z = F.normalize(self.projector(z))\n",
    "        return z, self.prototypes(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def create_swav_model(encoder, hidden_size=256, projection_size=128, n_protos=3000):\n",
    "    \"Create SwAV model\"\n",
    "    n_in  = in_channels(encoder)\n",
    "    with torch.no_grad(): representation = encoder(torch.randn((2,n_in,128,128)))\n",
    "    projector = create_mlp_module(representation.size(1), hidden_size, projection_size, bn=True)\n",
    "    prototypes = nn.Linear(projection_size, n_protos, bias=False)\n",
    "    apply_init(projector)\n",
    "    with torch.no_grad():\n",
    "        w = prototypes.weight.data.clone()\n",
    "        prototypes.weight.copy_(F.normalize(w))\n",
    "    return SwAVModel(encoder, projector, prototypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = create_encoder(\"tf_efficientnet_b0_ns\", n_in=3, pretrained=False, pool_type=PoolingType.CatAvgMax)\n",
    "model = create_swav_model(encoder, hidden_size=2048, projection_size=128, n_protos=3000)\n",
    "multi_view_inputs = ([torch.randn(2,3,224,224) for i in range(2)] +\n",
    "                     [torch.randn(2,3,96,96) for i in range(4)])\n",
    "embedding, output = model(multi_view_inputs)\n",
    "norms = model.prototypes.weight.data.norm(dim=1)\n",
    "assert norms.shape[0] == 3000\n",
    "assert [n.item() for n in norms if test_close(n.item(), 1.)] == []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.projector[-1].out_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12, 128]), torch.Size([12, 3000]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.shape, output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SwAV Callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, `K` represents queue size, to disable queue simply pass `None`.\n",
    "\n",
    "`queue_start_pct` defines when to start using queue in terms of total training percentage, e.g if you train for 100 epochs and if `queue_start_pct` is set to 0.25 then queue will be used starting epoch 25. You should tune queue size and queue start percentage for your own data and problem. For more information you can refer to [readme from official implementation](https://github.com/facebookresearch/swav#training-gets-unstable-when-using-the-queue).\n",
    "\n",
    "`crop_sizes`: Large and small crop resolutions.\n",
    "\n",
    "`num_crops`: Number of large and small crops crops.\n",
    "\n",
    "`min_scales`: Min scale in RandomResizedCrop for large and small crops.\n",
    "\n",
    "`max_scales`: Max scale in RandomResizedCrop for large and small crops.\n",
    "\n",
    "`crop_assgn_ids`: Indexes for large crops.\n",
    "\n",
    "\n",
    "You may refer to: [official implementation](https://github.com/facebookresearch/swav)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SWAV(Callback):\n",
    "    order,run_valid = 9,True\n",
    "    def __init__(self, aug_func=get_batch_augs, print_augs=False,\n",
    "                       K=3000,\n",
    "                       queue_start_pct=0.25,\n",
    "                       crop_sizes=[224,96], \n",
    "                       num_crops=[2,6],\n",
    "                       min_scales=[0.25,0.05],\n",
    "                       max_scales=[1.,0.14],\n",
    "                       crop_assgn_ids=[0,1],\n",
    "                       eps=0.05,\n",
    "                       n_sinkh_iter=3,\n",
    "                       temp=0.1,\n",
    "                       **aug_kwargs):\n",
    "        \n",
    "        store_attr('K,queue_start_pct,num_crops,crop_assgn_ids,temp,eps,n_sinkh_iter')\n",
    "        self.augs = []\n",
    "        for nc, size, mins, maxs in zip(num_crops, crop_sizes, min_scales, max_scales):\n",
    "            self.augs += [aug_func(size, resize_scale=(mins, maxs), **aug_kwargs) for i in range(nc)]\n",
    "        if print_augs: \n",
    "            for aug in self.augs: print(aug)\n",
    "    \n",
    "    \n",
    "    def before_fit(self):\n",
    "        self.learn.loss_func = self.lf\n",
    "        \n",
    "        # init queue\n",
    "        if self.K is not None:\n",
    "            nf = self.learn.model.projector[-1].out_features\n",
    "            self.queue = torch.randn(self.K, nf).to(self.dls.device)\n",
    "            self.queue = nn.functional.normalize(self.queue, dim=1)\n",
    "            self.queue_ptr = 0\n",
    "            \n",
    "    \n",
    "    def before_batch(self):\n",
    "        \"Compute multi crop inputs\"\n",
    "        self.bs = self.x.size(0)\n",
    "        self.learn.xb = ([aug(self.x) for aug in self.augs],)\n",
    "\n",
    "\n",
    "    def after_batch(self):\n",
    "        with torch.no_grad():\n",
    "            w = self.learn.model.prototypes.weight.data.clone()\n",
    "            self.learn.model.prototypes.weight.data.copy_(F.normalize(w))\n",
    "            \n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def sinkhorn_knopp(self, Q, nmb_iters, device=default_device):\n",
    "        \"https://en.wikipedia.org/wiki/Sinkhorn%27s_theorem#Sinkhorn-Knopp_algorithm\"\n",
    "        sum_Q = torch.sum(Q)\n",
    "        Q /= sum_Q\n",
    "\n",
    "        r = (torch.ones(Q.shape[0]) / Q.shape[0]).to(device)\n",
    "        c = (torch.ones(Q.shape[1]) / Q.shape[1]).to(device)\n",
    "\n",
    "        curr_sum = torch.sum(Q, dim=1)\n",
    "\n",
    "        for it in range(nmb_iters):\n",
    "            u = curr_sum\n",
    "            Q *= (r / u).unsqueeze(1)\n",
    "            Q *= (c / torch.sum(Q, dim=0)).unsqueeze(0)\n",
    "            curr_sum = torch.sum(Q, dim=1)\n",
    "        return (Q / torch.sum(Q, dim=0, keepdim=True)).t().float()\n",
    "\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def _dequeue_and_enqueue(self, embedding):\n",
    "        assert self.K % self.bs == 0  # for simplicity\n",
    "        self.queue[self.queue_ptr:self.queue_ptr+self.bs, :] = embedding\n",
    "        self.queue_ptr = (self.queue_ptr + self.bs) % self.K  # move pointer\n",
    "        \n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _compute_codes(self, output):\n",
    "        qs = []\n",
    "        for i in self.crop_assgn_ids:            \n",
    "            # use queue\n",
    "            if self.learn.pct_train > self.queue_start_pct:\n",
    "                target_b = output[self.bs*i:self.bs*(i+1)]\n",
    "                queue_b = self.learn.model.prototypes(self.queue)\n",
    "                merged_b = torch.cat([target_b, queue_b])\n",
    "                q = torch.exp(merged_b/self.eps).t()\n",
    "                q = self.sinkhorn_knopp(q, self.n_sinkh_iter, q.device)\n",
    "                qs.append(q[:self.bs])\n",
    "            \n",
    "            # don't use queue\n",
    "            else:\n",
    "                target_b = output[self.bs*i:self.bs*(i+1)]\n",
    "                q = torch.exp(target_b/self.eps).t()\n",
    "                q = self.sinkhorn_knopp(q, self.n_sinkh_iter, q.device)\n",
    "                qs.append(q)\n",
    "        return qs\n",
    "        \n",
    "                \n",
    "    def after_pred(self):\n",
    "        \"Compute ps and qs\"\n",
    "        \n",
    "        embedding, output = self.pred\n",
    "        \n",
    "        # Update - no need to store all assignment crops, e.g. just 0 from [0,1]\n",
    "        # Update queue only during training\n",
    "        if (self.K is not None) and (self.learn.training):  self._dequeue_and_enqueue(embedding[:self.bs])\n",
    "            \n",
    "        # Compute codes\n",
    "        qs = self._compute_codes(output)\n",
    "        \n",
    "        # Compute predictions\n",
    "        log_ps = []\n",
    "        for v in np.arange(np.sum(self.num_crops)):\n",
    "            log_p = F.log_softmax(output[self.bs*v:self.bs*(v+1)] / self.temp, dim=1)\n",
    "            log_ps.append(log_p)\n",
    "        \n",
    "        log_ps, qs = torch.stack(log_ps), torch.stack(qs)\n",
    "        self.learn.pred, self.learn.yb = log_ps, (qs,)\n",
    "    \n",
    "        \n",
    "    def lf(self, pred, *yb):\n",
    "        log_ps, qs, loss = pred, yb[0], 0\n",
    "        t = (qs.unsqueeze(1)*log_ps.unsqueeze(0)).sum(-1).mean(-1)\n",
    "        for i, ti in enumerate(t): loss -= (ti.sum() - ti[i])/(len(ti)-1)/len(t)\n",
    "        return loss\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def show_one(self):\n",
    "        xb = self.learn.xb[0]\n",
    "        i = np.random.choice(self.bs)\n",
    "        images = [aug.decode(b.to('cpu').clone()).clamp(0.1)[i] \n",
    "                      for b, aug in zip(xb, self.augs)]\n",
    "        return show_batch(xb[0], None, images, max_n=len(images), ncols=len(images), nrows=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`crop_sizes` defines the size to be used for original crops and low resolution crops respectively. `num_crops` define `N`: number of original views and `V`: number of low resolution views respectively. `min_scales` and `max_scales` are used for original and low resolution views during random resized crop. `eps` is used during Sinkhorn-Knopp algorithm for calculating the codes and `n_sinkh_iter` is the number of iterations during it's calculation. `temp` is the temperature parameter in cross entropy loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.MNIST_TINY)\n",
    "items = get_image_files(path)\n",
    "tds = Datasets(items, [PILImageBW.create, [parent_label, Categorize()]], splits=GrandparentSplitter()(items))\n",
    "dls = tds.dataloaders(bs=4, after_item=[ToTensor(), IntToFloatTensor()], device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastai_encoder = create_fastai_encoder(xresnet18, n_in=1, pretrained=False)\n",
    "model = create_swav_model(fastai_encoder, hidden_size=2048, projection_size=128)\n",
    "learn = Learner(dls, model,\n",
    "                cbs=[SWAV(crop_sizes=[28,16], min_scales=[0.25,0.05], max_scales=[1.0,0.3],\n",
    "                          rotate=False, jitter=False, bw=False, blur=False,stats=None,cuda=False),\n",
    "                     ShortEpochCallback(0.001)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = dls.one_batch()\n",
    "learn._split(b)\n",
    "learn('before_batch')\n",
    "learn.pred = learn.model(*learn.xb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display 2 standard resolution crops and 6 additional low resolution crops, aka multi crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1728x216 with 8 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "axes = learn.swav.show_one()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(8.3347)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.recorder.losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 01 - augmentations.ipynb.\n",
      "Converted 02 - layers.ipynb.\n",
      "Converted 03 - distributed.ipynb.\n",
      "Converted 10 - simclr.ipynb.\n",
      "Converted 11 - moco.ipynb.\n",
      "Converted 12 - byol.ipynb.\n",
      "Converted 13 - swav.ipynb.\n",
      "Converted 20 - clip.ipynb.\n",
      "Converted 21 - clip-moco.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
