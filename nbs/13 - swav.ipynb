{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp vision.swav"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SwAV\n",
    "\n",
    "> **SwAV**: [Unsupervised Learning of Visual Features by Contrasting Cluster Assignments](https://arxiv.org/pdf/2006.09882.pdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastai.vision.all import *\n",
    "from self_supervised.augmentations import *\n",
    "from self_supervised.layers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SwAV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![SwAV](images/swav.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Absract**: Unsupervised image representations have significantly reduced the gap with supervised\n",
    "pretraining, notably with the recent achievements of contrastive learning\n",
    "methods. These contrastive methods typically work online and rely on a large number\n",
    "of explicit pairwise feature comparisons, which is computationally challenging.\n",
    "In this paper, we propose an online algorithm, SwAV, that takes advantage of contrastive\n",
    "methods without requiring to compute pairwise comparisons. Specifically,\n",
    "our method simultaneously clusters the data while enforcing consistency between\n",
    "cluster assignments produced for different augmentations (or “views”) of the same\n",
    "image, instead of comparing features directly as in contrastive learning. Simply put,\n",
    "we use a “swapped” prediction mechanism where we predict the code of a view\n",
    "from the representation of another view. Our method can be trained with large and\n",
    "small batches and can scale to unlimited amounts of data. Compared to previous\n",
    "contrastive methods, our method is more memory efficient since it does not require\n",
    "a large memory bank or a special momentum network. In addition, we also propose\n",
    "a new data augmentation strategy, multi-crop, that uses a mix of views with\n",
    "different resolutions in place of two full-resolution views, without increasing the\n",
    "memory or compute requirements. We validate our findings by achieving 75:3%\n",
    "top-1 accuracy on ImageNet with ResNet-50, as well as surpassing supervised\n",
    "pretraining on all the considered transfer tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SwAVModel(Module):\n",
    "    def __init__(self,encoder,projector,prototypes): \n",
    "        self.encoder,self.projector,self.prototypes = encoder,projector,prototypes\n",
    "    \n",
    "    def forward(self, inputs): \n",
    "        \n",
    "        if not isinstance(inputs, list): inputs = [inputs]\n",
    "            \n",
    "        crop_idxs = torch.cumsum(torch.unique_consecutive(\n",
    "                                torch.tensor([inp.shape[-1] for inp in inputs]),\n",
    "                                return_counts=True)[1], 0)\n",
    "\n",
    "        start_idx = 0\n",
    "        for idx in crop_idxs:\n",
    "            _z = self.encoder(torch.cat(inputs[start_idx: idx]))\n",
    "            if not start_idx: z = _z\n",
    "            else:             z = torch.cat((z, _z))\n",
    "            start_idx = idx\n",
    "        \n",
    "        z = F.normalize(self.projector(z))\n",
    "        return z, self.prototypes(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def create_swav_model(encoder, hidden_size=256, projection_size=128, n_protos=3000):\n",
    "    \"Create SwAV model\"\n",
    "    n_in  = in_channels(encoder)\n",
    "    with torch.no_grad(): representation = encoder(torch.randn((2,n_in,128,128)))\n",
    "    projector = create_mlp_module(representation.size(1), hidden_size, projection_size, bn=True)\n",
    "    prototypes = nn.Linear(projection_size, n_protos, bias=False)\n",
    "    apply_init(projector)\n",
    "    with torch.no_grad():\n",
    "        w = prototypes.weight.data.clone()\n",
    "        prototypes.weight.copy_(F.normalize(w))\n",
    "    return SwAVModel(encoder, projector, prototypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = create_encoder(\"tf_efficientnet_b0_ns\", n_in=3, pretrained=False, pool_type=PoolingType.CatAvgMax)\n",
    "model = create_swav_model(encoder, hidden_size=2048, projection_size=128, n_protos=3000)\n",
    "multi_view_inputs = ([torch.randn(2,3,224,224) for i in range(2)] +\n",
    "                     [torch.randn(2,3,96,96) for i in range(4)])\n",
    "embedding, output = model(multi_view_inputs)\n",
    "norms = model.prototypes.weight.data.norm(dim=1)\n",
    "assert norms.shape[0] == 3000\n",
    "assert [n.item() for n in norms if test_close(n.item(), 1.)] == []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SwAV Callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following parameters can be passed;\n",
    "\n",
    "- **aug_pipelines** list of augmentation pipelines List[Pipeline, Pipeline,...,Pipeline] created using functions from `self_supervised.augmentations` module. Each `Pipeline` should be set to `split_idx=0`.  You can simply use `get_swav_aug_pipelines` utility to get aug_pipelines. SWAV algorithm uses a mix of large and small scale crops.\n",
    "\n",
    "- **crop_assgn_ids** indexes for large crops from **aug_pipelines**, e.g. if you have total of 8 Pipelines in the `aug_pipelines` list and if you define large crops as first 2 Pipelines then indexes would be [0,1], if as first 3 then [0,1,2] and if as last 2 then [6,7], so on.\n",
    "\n",
    "- **K** is queue size. For simplicity K needs to be a multiple of batch size and it needs to be less than total training data. You can try out different values e.g. `bs*2^k` by varying k where bs i batch size. You can pass None to disable queue. Idea is similar to MoCo.\n",
    "\n",
    "- **queue_start_pct** defines when to start using queue in terms of total training percentage, e.g if you train for 100 epochs and if `queue_start_pct` is set to 0.25 then queue will be used starting from epoch 25. You should tune queue size and queue start percentage for your own data and problem. For more information you can refer to [README from official implementation](https://github.com/facebookresearch/swav#training-gets-unstable-when-using-the-queue).\n",
    "\n",
    "- **temp** temperature scaling for cross entropy loss similar to `SimCLR`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SWAV algorithm uses multi-sized-multi-crop views of image. In original paper 2 large crop views and 6 small crop views are used during training. The reason of using smaller crops is to save memory and perhaps it also helps model to learn local features better.\n",
    "\n",
    "You can manually pass a mix of large and small scale Pipeline instances within a list to **aug_pipelines** or you can simply use **get_swav_aug_pipelines()** helper function below:\n",
    "\n",
    "- **num_crops** Number of large and small scale views to be used. \n",
    "- **crop_sizes** Image crop sizes for large and small views. \n",
    "- **min_scales** Min scale to use in RandomResizedCrop for large and small views. \n",
    "- **max_scales** Max scale to use in RandomResizedCrop for large and small views. \n",
    "\n",
    "I highly recommend this [UI from albumentations](https://albumentations-demo.herokuapp.com/) to get a feel about RandomResizedCrop parameters.\n",
    "\n",
    "Let's take the following example `get_swav_aug_pipelines(num_crops=(2,6), crop_sizes=(224,96), min_scales=(0.25,0.05), max_scales=(1.,0.14))`. This will create 2 large scale view augmentations with size 224 and with RandomResizedCrop scales between 0.25-1.0. Additionally, it will create 2 small scale view augmentations with size 96 and with RandomResizedCrop scales between 0.05-0.14.\n",
    "\n",
    "**Note**: Of course, the notion of small and large scale views depend on the values you pass to `crop_sizes`, `min_scales`, and `max_scales`. For example, if I we flip crop sizes from previous example as `crop_sizes=(96,224)`, then in this case first 2 views will have image resolution of 96 and last 6 views will have 224. For reducing confusion it's better to make relative changes, e.g. if you want to try different parameters always try to keep first values for larger resolution views and second values for smaller resolution views.\n",
    "\n",
    "- ****kwargs** This function uses `get_multi_aug_pipelines` which then uses `get_batch_augs`. For more information you may refer to `self_supervised.augmentations` module. kwargs takes any passable argument to `get_batch_augs`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@delegates(get_multi_aug_pipelines, but=['n', 'size', 'resize_scale'])\n",
    "def get_swav_aug_pipelines(num_crops=(2,6), crop_sizes=(224,96), min_scales=(0.25,0.05), max_scales=(1.,0.14), **kwargs): \n",
    "    aug_pipelines = []\n",
    "    for nc, size, mins, maxs in zip(num_crops, crop_sizes, min_scales, max_scales):\n",
    "        aug_pipelines += get_multi_aug_pipelines(n=nc, size=size, resize_scale=(mins,maxs), **kwargs)\n",
    "    return aug_pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SWAV(Callback):\n",
    "    order,run_valid = 9,True\n",
    "    def __init__(self, aug_pipelines=[], crop_assgn_ids=[0,1],\n",
    "                       K=3000, queue_start_pct=0.25, temp=0.07,\n",
    "                       eps=0.05,  n_sinkh_iter=3, print_augs=False):\n",
    "        \n",
    "        store_attr('K,queue_start_pct,crop_assgn_ids,temp,eps,n_sinkh_iter')\n",
    "        self.augs = aug_pipelines\n",
    "        if print_augs: \n",
    "            for aug in self.augs: print(aug)\n",
    "    \n",
    "    \n",
    "    def before_fit(self):\n",
    "        self.learn.loss_func = self.lf\n",
    "        \n",
    "        # init queue\n",
    "        if self.K is not None:\n",
    "            nf = self.learn.model.projector[-1].out_features\n",
    "            self.queue = torch.randn(self.K, nf).to(self.dls.device)\n",
    "            self.queue = nn.functional.normalize(self.queue, dim=1)\n",
    "            self.queue_ptr = 0\n",
    "            \n",
    "    \n",
    "    def before_batch(self):\n",
    "        \"Compute multi crop inputs\"\n",
    "        self.bs = self.x.size(0)\n",
    "        self.learn.xb = ([aug(self.x) for aug in self.augs],)\n",
    "\n",
    "\n",
    "    def after_batch(self):\n",
    "        with torch.no_grad():\n",
    "            w = self.learn.model.prototypes.weight.data.clone()\n",
    "            self.learn.model.prototypes.weight.data.copy_(F.normalize(w))\n",
    "            \n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def sinkhorn_knopp(self, Q, nmb_iters, device=default_device):\n",
    "        \"https://en.wikipedia.org/wiki/Sinkhorn%27s_theorem#Sinkhorn-Knopp_algorithm\"\n",
    "        sum_Q = torch.sum(Q)\n",
    "        Q /= sum_Q\n",
    "\n",
    "        r = (torch.ones(Q.shape[0]) / Q.shape[0]).to(device)\n",
    "        c = (torch.ones(Q.shape[1]) / Q.shape[1]).to(device)\n",
    "\n",
    "        curr_sum = torch.sum(Q, dim=1)\n",
    "\n",
    "        for it in range(nmb_iters):\n",
    "            u = curr_sum\n",
    "            Q *= (r / u).unsqueeze(1)\n",
    "            Q *= (c / torch.sum(Q, dim=0)).unsqueeze(0)\n",
    "            curr_sum = torch.sum(Q, dim=1)\n",
    "        return (Q / torch.sum(Q, dim=0, keepdim=True)).t().float()\n",
    "\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def _dequeue_and_enqueue(self, embedding):\n",
    "        assert self.K % self.bs == 0  # for simplicity\n",
    "        self.queue[self.queue_ptr:self.queue_ptr+self.bs, :] = embedding\n",
    "        self.queue_ptr = (self.queue_ptr + self.bs) % self.K  # move pointer\n",
    "        \n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _compute_codes(self, output):\n",
    "        qs = []\n",
    "        for i in self.crop_assgn_ids:            \n",
    "            # use queue\n",
    "            if (self.K is not None) and (self.learn.pct_train > self.queue_start_pct):\n",
    "                target_b = output[self.bs*i:self.bs*(i+1)]\n",
    "                queue_b = self.learn.model.prototypes(self.queue)\n",
    "                merged_b = torch.cat([target_b, queue_b])\n",
    "                q = torch.exp(merged_b/self.eps).t()\n",
    "                q = self.sinkhorn_knopp(q, self.n_sinkh_iter, q.device)\n",
    "                qs.append(q[:self.bs])\n",
    "            \n",
    "            # don't use queue\n",
    "            else:\n",
    "                target_b = output[self.bs*i:self.bs*(i+1)]\n",
    "                q = torch.exp(target_b/self.eps).t()\n",
    "                q = self.sinkhorn_knopp(q, self.n_sinkh_iter, q.device)\n",
    "                qs.append(q)\n",
    "        return qs\n",
    "        \n",
    "                \n",
    "    def after_pred(self):\n",
    "        \"Compute ps and qs\"\n",
    "        \n",
    "        embedding, output = self.pred\n",
    "        \n",
    "        # Update - no need to store all assignment crops, e.g. just 0 from [0,1]\n",
    "        # Update queue only during training\n",
    "        if (self.K is not None) and (self.learn.training):  self._dequeue_and_enqueue(embedding[:self.bs])\n",
    "            \n",
    "        # Compute codes\n",
    "        qs = self._compute_codes(output)\n",
    "        \n",
    "        # Compute predictions\n",
    "        log_ps = []\n",
    "        for v in np.arange(len(self.augs)):\n",
    "            log_p = F.log_softmax(output[self.bs*v:self.bs*(v+1)] / self.temp, dim=1)\n",
    "            log_ps.append(log_p)\n",
    "        \n",
    "        log_ps, qs = torch.stack(log_ps), torch.stack(qs)\n",
    "        self.learn.pred, self.learn.yb = log_ps, (qs,)\n",
    "    \n",
    "        \n",
    "    def lf(self, pred, *yb):\n",
    "        log_ps, qs, loss = pred, yb[0], 0\n",
    "        t = (qs.unsqueeze(1)*log_ps.unsqueeze(0)).sum(-1).mean(-1)\n",
    "        for i, ti in enumerate(t): loss -= (ti.sum() - ti[i])/(len(ti)-1)/len(t)\n",
    "        return loss\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def show_one(self):\n",
    "        xb = self.learn.xb[0]\n",
    "        i = np.random.choice(self.bs)\n",
    "        images = [aug.decode(b.to('cpu').clone()).clamp(0.1)[i] \n",
    "                      for b, aug in zip(xb, self.augs)]\n",
    "        return show_batch(xb[0], None, images, max_n=len(images), ncols=len(images), nrows=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`crop_sizes` defines the size to be used for original crops and low resolution crops respectively. `num_crops` define `N`: number of original views and `V`: number of low resolution views respectively. `min_scales` and `max_scales` are used for original and low resolution views during random resized crop. `eps` is used during Sinkhorn-Knopp algorithm for calculating the codes and `n_sinkh_iter` is the number of iterations during it's calculation. `temp` is the temperature parameter in cross entropy loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.MNIST_TINY)\n",
    "items = get_image_files(path)\n",
    "tds = Datasets(items, [PILImageBW.create, [parent_label, Categorize()]], splits=GrandparentSplitter()(items))\n",
    "dls = tds.dataloaders(bs=4, after_item=[ToTensor(), IntToFloatTensor()], device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastai_encoder = create_fastai_encoder(xresnet18, n_in=1, pretrained=False)\n",
    "model = create_swav_model(fastai_encoder, hidden_size=2048, projection_size=128)\n",
    "aug_pipelines = get_swav_aug_pipelines(num_crops=[2,6],\n",
    "                                       crop_sizes=[28,16], \n",
    "                                       min_scales=[0.25,0.05],\n",
    "                                       max_scales=[1.0,0.3],\n",
    "                                       rotate=False, jitter=False, bw=False, blur=False, stats=None,cuda=False) \n",
    "learn = Learner(dls, model,\n",
    "                cbs=[SWAV(aug_pipelines=aug_pipelines, crop_assgn_ids=[0,1], K=None), ShortEpochCallback(0.001)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = dls.one_batch()\n",
    "learn._split(b)\n",
    "learn('before_batch')\n",
    "learn.pred = learn.model(*learn.xb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display 2 standard resolution crops and 6 additional low resolution crops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1728x216 with 8 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "axes = learn.swav.show_one()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(8.7588)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.recorder.losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 01 - augmentations.ipynb.\n",
      "Converted 02 - layers.ipynb.\n",
      "Converted 03 - distributed.ipynb.\n",
      "Converted 10 - simclr.ipynb.\n",
      "Converted 11 - moco.ipynb.\n",
      "Converted 12 - byol.ipynb.\n",
      "Converted 13 - swav.ipynb.\n",
      "Converted 20 - clip.ipynb.\n",
      "Converted 21 - clip-moco.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
