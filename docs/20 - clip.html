---

title: CLIP


keywords: fastai
sidebar: home_sidebar

summary: "**CLIP**: <a href='https://arxiv.org/pdf/2103.00020.pdf'>Learning Transferable Visual Models From Natural Language Supervision</a>"
description: "**CLIP**: <a href='https://arxiv.org/pdf/2103.00020.pdf'>Learning Transferable Visual Models From Natural Language Supervision</a>"
nb_path: "nbs/20 - clip.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/20 - clip.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>As of preparing this module official CLIP repo is mainly structured for inference. This module adds the required changes for training keeping in mind all the tricks from the paper and the conversations from the github issues.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Algorithm">Algorithm<a class="anchor-link" href="#Algorithm"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="CLIP">CLIP<a class="anchor-link" href="#CLIP"> </a></h4>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/self_supervised/images/clip.png" alt="CLIP"></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Absract</strong>: State-of-the-art computer vision systems are
trained to predict a fixed set of predetermined
object categories. This restricted form of supervision limits their generality and usability since
additional labeled data is needed to specify any
other visual concept. Learning directly from raw
text about images is a promising alternative which
leverages a much broader source of supervision.
We demonstrate that the simple pre-training task
of predicting which caption goes with which image is an efficient and scalable way to learn SOTA
image representations from scratch on a dataset
of 400 million (image, text) pairs collected from
the internet. After pre-training, natural language
is used to reference learned visual concepts (or
describe new ones) enabling zero-shot transfer
of the model to downstream tasks. We study
the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and
many types of fine-grained object classification.
The model transfers non-trivially to most tasks
and is often competitive with a fully supervised
baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet
zero-shot without needing to use any of the 1.28
million training examples it was trained on. We
release our code and pre-trained model weights at
<a href="https://github.com/OpenAI/CLIP">https://github.com/OpenAI/CLIP</a>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Tokenizer">Tokenizer<a class="anchor-link" href="#Tokenizer"> </a></h3>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="ClipTokenizer" class="doc_header"><code>class</code> <code>ClipTokenizer</code><a href="https://github.com/keremturgutlu/self_supervised/tree/master/self_supervised/multimodal/clip.py#L23" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>ClipTokenizer</code>(<strong><code>context_length</code></strong>=<em><code>77</code></em>) :: <code>DisplayedTransform</code></p>
</blockquote>
<p>Tokenizer from <a href="https://github.com/openai/CLIP/blob/main/clip/simple_tokenizer.py">https://github.com/openai/CLIP/blob/main/clip/simple_tokenizer.py</a></p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Model">Model<a class="anchor-link" href="#Model"> </a></h3>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="vitb32_config" class="doc_header"><code>vitb32_config</code><a href="https://github.com/keremturgutlu/self_supervised/tree/master/self_supervised/multimodal/clip.py#L40" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>vitb32_config</code>(<strong><code>input_res</code></strong>, <strong><code>context_length</code></strong>, <strong><code>vocab_size</code></strong>)</p>
</blockquote>
<p>ViT-B/32 configuration, uses 32x32 patches</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="vitl14_config" class="doc_header"><code>vitl14_config</code><a href="https://github.com/keremturgutlu/self_supervised/tree/master/self_supervised/multimodal/clip.py#L54" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>vitl14_config</code>(<strong><code>input_res</code></strong>, <strong><code>context_length</code></strong>, <strong><code>vocab_size</code></strong>)</p>
</blockquote>
<p>ViT-L/14 configuration, uses 14x14 patches</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="Bottleneck" class="doc_header"><code>class</code> <code>Bottleneck</code><a href="https://github.com/keremturgutlu/self_supervised/tree/master/self_supervised/multimodal/clip.py#L72" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>Bottleneck</code>(<strong><code>inplanes</code></strong>, <strong><code>planes</code></strong>, <strong><code>stride</code></strong>=<em><code>1</code></em>) :: <code>Module</code></p>
</blockquote>
<p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>

<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))

</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<p>:ivar training: Boolean represents whether this module is in training or
                evaluation mode.
:vartype training: bool</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="AttentionPool2d" class="doc_header"><code>class</code> <code>AttentionPool2d</code><a href="https://github.com/keremturgutlu/self_supervised/tree/master/self_supervised/multimodal/clip.py#L118" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>AttentionPool2d</code>(<strong><code>spacial_dim</code></strong>:<code>int</code>, <strong><code>embed_dim</code></strong>:<code>int</code>, <strong><code>num_heads</code></strong>:<code>int</code>, <strong><code>output_dim</code></strong>:<code>int</code>=<em><code>None</code></em>) :: <code>Module</code></p>
</blockquote>
<p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>

<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))

</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<p>:ivar training: Boolean represents whether this module is in training or
                evaluation mode.
:vartype training: bool</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="ModifiedResNet" class="doc_header"><code>class</code> <code>ModifiedResNet</code><a href="https://github.com/keremturgutlu/self_supervised/tree/master/self_supervised/multimodal/clip.py#L155" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>ModifiedResNet</code>(<strong><code>layers</code></strong>, <strong><code>output_dim</code></strong>, <strong><code>heads</code></strong>, <strong><code>input_resolution</code></strong>=<em><code>224</code></em>, <strong><code>width</code></strong>=<em><code>64</code></em>) :: <code>Module</code></p>
</blockquote>
<p>A ResNet class that is similar to torchvision's but contains the following changes:</p>
<ul>
<li>There are now 3 "stem" convolutions as opposed to 1, with an average pool instead of a max pool.</li>
<li>Performs anti-aliasing strided convolutions, where an avgpool is prepended to convolutions with stride &gt; 1</li>
<li>The final pooling layer is a QKV attention instead of an average pool</li>
</ul>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="LayerNorm" class="doc_header"><code>class</code> <code>LayerNorm</code><a href="https://github.com/keremturgutlu/self_supervised/tree/master/self_supervised/multimodal/clip.py#L215" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>LayerNorm</code>(<strong><code>normalized_shape</code></strong>:<code>Union</code>[<code>int</code>, <code>List</code>[<code>int</code>], <code>Size</code>], <strong><code>eps</code></strong>:<code>float</code>=<em><code>1e-05</code></em>, <strong><code>elementwise_affine</code></strong>:<code>bool</code>=<em><code>True</code></em>) :: <a href="/self_supervised/21 - clip-moco.html#LayerNorm"><code>LayerNorm</code></a></p>
</blockquote>
<p>Subclass torch's LayerNorm to handle fp16.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="QuickGELU" class="doc_header"><code>class</code> <code>QuickGELU</code><a href="https://github.com/keremturgutlu/self_supervised/tree/master/self_supervised/multimodal/clip.py#L224" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>QuickGELU</code>() :: <code>Module</code></p>
</blockquote>
<p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>

<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))

</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<p>:ivar training: Boolean represents whether this module is in training or
                evaluation mode.
:vartype training: bool</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="ResidualAttentionBlock" class="doc_header"><code>class</code> <code>ResidualAttentionBlock</code><a href="https://github.com/keremturgutlu/self_supervised/tree/master/self_supervised/multimodal/clip.py#L229" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>ResidualAttentionBlock</code>(<strong><code>d_model</code></strong>:<code>int</code>, <strong><code>n_head</code></strong>:<code>int</code>, <strong><code>attn_mask</code></strong>:<code>Tensor</code>=<em><code>None</code></em>) :: <code>Module</code></p>
</blockquote>
<p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>

<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))

</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<p>:ivar training: Boolean represents whether this module is in training or
                evaluation mode.
:vartype training: bool</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="Transformer" class="doc_header"><code>class</code> <code>Transformer</code><a href="https://github.com/keremturgutlu/self_supervised/tree/master/self_supervised/multimodal/clip.py#L253" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>Transformer</code>(<strong><code>width</code></strong>:<code>int</code>, <strong><code>layers</code></strong>:<code>int</code>, <strong><code>heads</code></strong>:<code>int</code>, <strong><code>attn_mask</code></strong>:<code>Tensor</code>=<em><code>None</code></em>, <strong><code>checkpoint</code></strong>=<em><code>False</code></em>, <strong><code>checkpoint_nchunks</code></strong>=<em><code>2</code></em>) :: <code>Module</code></p>
</blockquote>
<p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>

<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))

</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<p>:ivar training: Boolean represents whether this module is in training or
                evaluation mode.
:vartype training: bool</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="VisualTransformer" class="doc_header"><code>class</code> <code>VisualTransformer</code><a href="https://github.com/keremturgutlu/self_supervised/tree/master/self_supervised/multimodal/clip.py#L267" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>VisualTransformer</code>(<strong><code>input_resolution</code></strong>:<code>int</code>, <strong><code>patch_size</code></strong>:<code>int</code>, <strong><code>width</code></strong>:<code>int</code>, <strong><code>layers</code></strong>:<code>int</code>, <strong><code>heads</code></strong>:<code>int</code>, <strong><code>output_dim</code></strong>:<code>int</code>, <strong>**<code>kwargs</code></strong>) :: <code>Module</code></p>
</blockquote>
<p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>

<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))

</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<p>:ivar training: Boolean represents whether this module is in training or
                evaluation mode.
:vartype training: bool</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="CLIP" class="doc_header"><code>class</code> <code>CLIP</code><a href="https://github.com/keremturgutlu/self_supervised/tree/master/self_supervised/multimodal/clip.py#L304" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>CLIP</code>(<strong><code>embed_dim</code></strong>:<code>int</code>, <strong><code>image_resolution</code></strong>:<code>int</code>, <strong><code>vision_layers</code></strong>:<code>Union</code>[<code>Tuple</code>[<code>int</code>, <code>int</code>, <code>int</code>, <code>int</code>], <code>int</code>], <strong><code>vision_width</code></strong>:<code>int</code>, <strong><code>vision_patch_size</code></strong>:<code>int</code>, <strong><code>context_length</code></strong>:<code>int</code>, <strong><code>vocab_size</code></strong>:<code>int</code>, <strong><code>transformer_width</code></strong>:<code>int</code>, <strong><code>transformer_heads</code></strong>:<code>int</code>, <strong><code>transformer_layers</code></strong>:<code>int</code>, <strong>**<code>kwargs</code></strong>) :: <code>Module</code></p>
</blockquote>
<p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>

<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))

</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<p>:ivar training: Boolean represents whether this module is in training or
                evaluation mode.
:vartype training: bool</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Metric">Metric<a class="anchor-link" href="#Metric"> </a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>A useful proxy metric for tracking training performance and convergence.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="RetrievalAtK" class="doc_header"><code>class</code> <code>RetrievalAtK</code><a href="https://github.com/keremturgutlu/self_supervised/tree/master/self_supervised/multimodal/clip.py#L435" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>RetrievalAtK</code>(<strong><code>k</code></strong>=<em><code>20</code></em>, <strong>**<code>kwargs</code></strong>) :: <code>AccumMetric</code></p>
</blockquote>
<p>Stores predictions and targets on CPU in accumulate to perform final calculations with <code>func</code>.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="CLIP-Callback">CLIP Callback<a class="anchor-link" href="#CLIP-Callback"> </a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Training Tip:</strong> In my own experiments, using <code>CLIPTrainer()</code> leads to faster convergence than <a href="/self_supervised/20 - clip.html#DistributedCLIPTrainer"><code>DistributedCLIPTrainer</code></a>. You should combine <strong>CLIPTrainer, DistributedDataParallel, fp16 and ZeRO optimizer with maximum batch size that fits to your memory</strong> for optimal speed and performance.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Important">Important<a class="anchor-link" href="#Important"> </a></h4><p>To train with gradient <strong>checkpointing + fp16</strong> you need to add 2 lines of code to pytorch source code, until fastai moves to torch version&gt;=1.8.</p>
<p><img src="/self_supervised/./images/this_is_the_way.jpg" alt=""></p>
<ul>
<li><a href="https://github.com/pytorch/pytorch/issues/49738">https://github.com/pytorch/pytorch/issues/49738</a></li>
<li><a href="https://github.com/pytorch/pytorch/pull/49757/files">https://github.com/pytorch/pytorch/pull/49757/files</a></li>
</ul>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="CLIPTrainer" class="doc_header"><code>class</code> <code>CLIPTrainer</code><a href="https://github.com/keremturgutlu/self_supervised/tree/master/self_supervised/multimodal/clip.py#L464" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>CLIPTrainer</code>(<strong><code>after_create</code></strong>=<em><code>None</code></em>, <strong><code>before_fit</code></strong>=<em><code>None</code></em>, <strong><code>before_epoch</code></strong>=<em><code>None</code></em>, <strong><code>before_train</code></strong>=<em><code>None</code></em>, <strong><code>before_batch</code></strong>=<em><code>None</code></em>, <strong><code>after_pred</code></strong>=<em><code>None</code></em>, <strong><code>after_loss</code></strong>=<em><code>None</code></em>, <strong><code>before_backward</code></strong>=<em><code>None</code></em>, <strong><code>before_step</code></strong>=<em><code>None</code></em>, <strong><code>after_cancel_step</code></strong>=<em><code>None</code></em>, <strong><code>after_step</code></strong>=<em><code>None</code></em>, <strong><code>after_cancel_batch</code></strong>=<em><code>None</code></em>, <strong><code>after_batch</code></strong>=<em><code>None</code></em>, <strong><code>after_cancel_train</code></strong>=<em><code>None</code></em>, <strong><code>after_train</code></strong>=<em><code>None</code></em>, <strong><code>before_validate</code></strong>=<em><code>None</code></em>, <strong><code>after_cancel_validate</code></strong>=<em><code>None</code></em>, <strong><code>after_validate</code></strong>=<em><code>None</code></em>, <strong><code>after_cancel_epoch</code></strong>=<em><code>None</code></em>, <strong><code>after_epoch</code></strong>=<em><code>None</code></em>, <strong><code>after_cancel_fit</code></strong>=<em><code>None</code></em>, <strong><code>after_fit</code></strong>=<em><code>None</code></em>) :: <code>Callback</code></p>
</blockquote>
<p>Can be used with or without DistributedDataParallel</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="DistributedCLIPTrainer" class="doc_header"><code>class</code> <code>DistributedCLIPTrainer</code><a href="https://github.com/keremturgutlu/self_supervised/tree/master/self_supervised/multimodal/clip.py#L489" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>DistributedCLIPTrainer</code>(<strong><code>after_create</code></strong>=<em><code>None</code></em>, <strong><code>before_fit</code></strong>=<em><code>None</code></em>, <strong><code>before_epoch</code></strong>=<em><code>None</code></em>, <strong><code>before_train</code></strong>=<em><code>None</code></em>, <strong><code>before_batch</code></strong>=<em><code>None</code></em>, <strong><code>after_pred</code></strong>=<em><code>None</code></em>, <strong><code>after_loss</code></strong>=<em><code>None</code></em>, <strong><code>before_backward</code></strong>=<em><code>None</code></em>, <strong><code>before_step</code></strong>=<em><code>None</code></em>, <strong><code>after_cancel_step</code></strong>=<em><code>None</code></em>, <strong><code>after_step</code></strong>=<em><code>None</code></em>, <strong><code>after_cancel_batch</code></strong>=<em><code>None</code></em>, <strong><code>after_batch</code></strong>=<em><code>None</code></em>, <strong><code>after_cancel_train</code></strong>=<em><code>None</code></em>, <strong><code>after_train</code></strong>=<em><code>None</code></em>, <strong><code>before_validate</code></strong>=<em><code>None</code></em>, <strong><code>after_cancel_validate</code></strong>=<em><code>None</code></em>, <strong><code>after_validate</code></strong>=<em><code>None</code></em>, <strong><code>after_cancel_epoch</code></strong>=<em><code>None</code></em>, <strong><code>after_epoch</code></strong>=<em><code>None</code></em>, <strong><code>after_cancel_fit</code></strong>=<em><code>None</code></em>, <strong><code>after_fit</code></strong>=<em><code>None</code></em>) :: <code>Callback</code></p>
</blockquote>
<p>Distributed implementation of InfoNCE loss, should be used with DistributedDataParallel</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Example-Usage">Example Usage<a class="anchor-link" href="#Example-Usage"> </a></h3>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">num2txt</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;3&#39;</span><span class="p">:</span> <span class="s1">&#39;three&#39;</span><span class="p">,</span> <span class="s1">&#39;7&#39;</span><span class="p">:</span> <span class="s1">&#39;seven&#39;</span><span class="p">}</span>
<span class="k">def</span> <span class="nf">num_to_txt</span><span class="p">(</span><span class="n">o</span><span class="p">):</span> <span class="k">return</span> <span class="n">num2txt</span><span class="p">[</span><span class="n">o</span><span class="p">]</span>
<span class="k">def</span> <span class="nf">dummy_targ</span><span class="p">(</span><span class="n">o</span><span class="p">):</span> <span class="k">return</span> <span class="mi">0</span> <span class="c1"># loss func is not called without it</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">path</span> <span class="o">=</span> <span class="n">untar_data</span><span class="p">(</span><span class="n">URLs</span><span class="o">.</span><span class="n">MNIST_TINY</span><span class="p">)</span>
<span class="n">items</span> <span class="o">=</span> <span class="n">get_image_files</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>
<span class="n">clip_tokenizer</span> <span class="o">=</span> <span class="n">ClipTokenizer</span><span class="p">()</span>
<span class="n">tds</span> <span class="o">=</span> <span class="n">Datasets</span><span class="p">(</span><span class="n">items</span><span class="p">,</span> <span class="p">[</span><span class="n">PILImage</span><span class="o">.</span><span class="n">create</span><span class="p">,</span> <span class="p">[</span><span class="n">parent_label</span><span class="p">,</span> <span class="n">num_to_txt</span><span class="p">],</span> <span class="n">dummy_targ</span><span class="p">],</span> <span class="n">n_inp</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">splits</span><span class="o">=</span><span class="n">GrandparentSplitter</span><span class="p">()(</span><span class="n">items</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">dls</span> <span class="o">=</span> <span class="n">tds</span><span class="o">.</span><span class="n">dataloaders</span><span class="p">(</span><span class="n">bs</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">after_item</span><span class="o">=</span><span class="p">[</span><span class="n">Resize</span><span class="p">(</span><span class="mi">224</span><span class="p">),</span> <span class="n">clip_tokenizer</span><span class="p">,</span> <span class="n">ToTensor</span><span class="p">()],</span> <span class="n">after_batch</span><span class="o">=</span><span class="p">[</span><span class="n">IntToFloatTensor</span><span class="p">()],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cpu&#39;</span><span class="p">)</span>
<span class="n">vitb32_config_dict</span> <span class="o">=</span> <span class="n">vitb32_config</span><span class="p">(</span><span class="mi">224</span><span class="p">,</span> <span class="n">clip_tokenizer</span><span class="o">.</span><span class="n">context_length</span><span class="p">,</span> <span class="n">clip_tokenizer</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">)</span>
<span class="n">clip_model</span> <span class="o">=</span> <span class="n">CLIP</span><span class="p">(</span><span class="o">**</span><span class="n">vitb32_config_dict</span><span class="p">,</span> <span class="n">checkpoint</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">checkpoint_nchunks</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">learner</span> <span class="o">=</span> <span class="n">Learner</span><span class="p">(</span><span class="n">dls</span><span class="p">,</span> <span class="n">clip_model</span><span class="p">,</span> <span class="n">loss_func</span><span class="o">=</span><span class="n">noop</span><span class="p">,</span> <span class="n">cbs</span><span class="o">=</span><span class="p">[</span><span class="n">CLIPTrainer</span><span class="p">(),</span> <span class="n">ShortEpochCallback</span><span class="p">(</span><span class="mf">0.001</span><span class="p">)],</span>
                  <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="n">RetrievalAtK</span><span class="p">(</span><span class="n">k</span><span class="o">=</span><span class="mi">5</span><span class="p">),</span> 
                           <span class="n">RetrievalAtK</span><span class="p">(</span><span class="n">k</span><span class="o">=</span><span class="mi">20</span><span class="p">),</span> 
                           <span class="n">RetrievalAtK</span><span class="p">(</span><span class="n">k</span><span class="o">=</span><span class="s2">&quot;mean&quot;</span><span class="p">),</span>
                           <span class="n">RetrievalAtK</span><span class="p">(</span><span class="n">k</span><span class="o">=</span><span class="s2">&quot;median&quot;</span><span class="p">)])</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learner</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">

    <div>
        <style>
            /* Turns off some styling */
            progress {
                /* gets rid of default border in Firefox and Opera. */
                border: none;
                /* Needs to be in here for Safari polyfill so background images work as expected. */
                background-size: auto;
            }
            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
                background: #F44336;
            }
        </style>
      <progress value='0' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>
      0.00% [0/1 00:00<00:00]
    </div>
    
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>retrieval_at_5</th>
      <th>retrieval_at_20</th>
      <th>mean_retrieval_ranking</th>
      <th>median_retrieval_ranking</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
  </tbody>
</table><p>

    <div>
        <style>
            /* Turns off some styling */
            progress {
                /* gets rid of default border in Firefox and Opera. */
                border: none;
                /* Needs to be in here for Safari polyfill so background images work as expected. */
                background-size: auto;
            }
            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
                background: #F44336;
            }
        </style>
      <progress value='0' class='' max='177' style='width:300px; height:20px; vertical-align: middle;'></progress>
      0.00% [0/177 00:00<00:00]
    </div>
    
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learner</span><span class="o">.</span><span class="n">recorder</span><span class="o">.</span><span class="n">losses</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

</div>
 

