---

title: Title


keywords: fastai
sidebar: home_sidebar



nb_path: "nbs/examples/byol_iwang_256.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/examples/byol_iwang_256.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Bootstrap-Your-Own-Latent-A-New-Approach-to-Self-Supervised-Learning:-https://arxiv.org/pdf/2006.07733.pdf"><strong>Bootstrap Your Own Latent A New Approach to Self-Supervised Learning:</strong> <a href="https://arxiv.org/pdf/2006.07733.pdf">https://arxiv.org/pdf/2006.07733.pdf</a><a class="anchor-link" href="#Bootstrap-Your-Own-Latent-A-New-Approach-to-Self-Supervised-Learning:-https://arxiv.org/pdf/2006.07733.pdf"> </a></h2>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">fastai</span><span class="o">,</span> <span class="nn">fastcore</span><span class="o">,</span> <span class="nn">torch</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">fastai</span><span class="o">.</span><span class="n">__version__</span> <span class="p">,</span> <span class="n">fastcore</span><span class="o">.</span><span class="n">__version__</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">__version__</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(&#39;2.0.8&#39;, &#39;1.0.1&#39;, &#39;1.6.0+cu101&#39;)</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">fastai.vision.all</span> <span class="kn">import</span> <span class="o">*</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Sizes">Sizes<a class="anchor-link" href="#Sizes"> </a></h3><p>Resize -&gt; RandomCrop</p>
<p>320 -&gt; 256 | 224 -&gt; 192 | 160 -&gt; 128</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">resize</span> <span class="o">=</span> <span class="mi">320</span>
<span class="n">size</span> <span class="o">=</span> <span class="mi">256</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="1.-Implementation-Details-(Section-3.2-from-the-paper)">1. Implementation Details (Section 3.2 from the paper)<a class="anchor-link" href="#1.-Implementation-Details-(Section-3.2-from-the-paper)"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="1.1-Image-Augmentations">1.1 Image Augmentations<a class="anchor-link" href="#1.1-Image-Augmentations"> </a></h3><p>Same as SimCLR with optional grayscale</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">kornia</span>
<span class="k">def</span> <span class="nf">get_aug_pipe</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">stats</span><span class="o">=</span><span class="n">imagenet_stats</span><span class="p">,</span> <span class="n">s</span><span class="o">=.</span><span class="mi">6</span><span class="p">):</span>
    <span class="s2">&quot;SimCLR augmentations&quot;</span>
    <span class="n">rrc</span> <span class="o">=</span> <span class="n">kornia</span><span class="o">.</span><span class="n">augmentation</span><span class="o">.</span><span class="n">RandomResizedCrop</span><span class="p">((</span><span class="n">size</span><span class="p">,</span> <span class="n">size</span><span class="p">),</span> <span class="n">scale</span><span class="o">=</span><span class="p">(</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">),</span> <span class="n">ratio</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="o">/</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="o">/</span><span class="mi">3</span><span class="p">))</span>
    <span class="n">rhf</span> <span class="o">=</span> <span class="n">kornia</span><span class="o">.</span><span class="n">augmentation</span><span class="o">.</span><span class="n">RandomHorizontalFlip</span><span class="p">()</span>
    <span class="n">rcj</span> <span class="o">=</span> <span class="n">kornia</span><span class="o">.</span><span class="n">augmentation</span><span class="o">.</span><span class="n">ColorJitter</span><span class="p">(</span><span class="mf">0.8</span><span class="o">*</span><span class="n">s</span><span class="p">,</span> <span class="mf">0.8</span><span class="o">*</span><span class="n">s</span><span class="p">,</span> <span class="mf">0.8</span><span class="o">*</span><span class="n">s</span><span class="p">,</span> <span class="mf">0.2</span><span class="o">*</span><span class="n">s</span><span class="p">)</span>
    <span class="n">rgs</span> <span class="o">=</span> <span class="n">kornia</span><span class="o">.</span><span class="n">augmentation</span><span class="o">.</span><span class="n">RandomGrayscale</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
    
    <span class="n">tfms</span> <span class="o">=</span> <span class="p">[</span><span class="n">rrc</span><span class="p">,</span> <span class="n">rhf</span><span class="p">,</span> <span class="n">rcj</span><span class="p">,</span> <span class="n">rgs</span><span class="p">,</span> <span class="n">Normalize</span><span class="o">.</span><span class="n">from_stats</span><span class="p">(</span><span class="o">*</span><span class="n">stats</span><span class="p">)]</span>
    <span class="n">pipe</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">(</span><span class="n">tfms</span><span class="p">)</span>
    <span class="n">pipe</span><span class="o">.</span><span class="n">split_idx</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">return</span> <span class="n">pipe</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="1.2-Architecture">1.2 Architecture<a class="anchor-link" href="#1.2-Architecture"> </a></h3>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">create_encoder</span><span class="p">(</span><span class="n">arch</span><span class="p">,</span> <span class="n">n_in</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">pretrained</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">cut</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">concat_pool</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="s2">&quot;Create encoder from a given arch backbone&quot;</span>
    <span class="n">encoder</span> <span class="o">=</span> <span class="n">create_body</span><span class="p">(</span><span class="n">arch</span><span class="p">,</span> <span class="n">n_in</span><span class="p">,</span> <span class="n">pretrained</span><span class="p">,</span> <span class="n">cut</span><span class="p">)</span>
    <span class="n">pool</span> <span class="o">=</span> <span class="n">AdaptiveConcatPool2d</span><span class="p">()</span> <span class="k">if</span> <span class="n">concat_pool</span> <span class="k">else</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveAvgPool2d</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">encoder</span><span class="p">,</span> <span class="n">pool</span><span class="p">,</span> <span class="n">Flatten</span><span class="p">())</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">MLP</span><span class="p">(</span><span class="n">Module</span><span class="p">):</span>
    <span class="s2">&quot;MLP module as described in paper&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">projection_size</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span><span class="mi">2048</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">projection_size</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">net</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">BYOLModel</span><span class="p">(</span><span class="n">Module</span><span class="p">):</span>
    <span class="s2">&quot;Compute predictions of v1 and v2&quot;</span> 
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">encoder</span><span class="p">,</span><span class="n">projector</span><span class="p">,</span><span class="n">predictor</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">projector</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">predictor</span> <span class="o">=</span> <span class="n">encoder</span><span class="p">,</span><span class="n">projector</span><span class="p">,</span><span class="n">predictor</span>    

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">v1</span><span class="p">,</span><span class="n">v2</span><span class="p">):</span>
        <span class="n">q1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">predictor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">projector</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">v1</span><span class="p">)))</span>
        <span class="n">q2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">predictor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">projector</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">v2</span><span class="p">)))</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">q1</span><span class="p">,</span><span class="n">q2</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">create_byol_model</span><span class="p">(</span><span class="n">arch</span><span class="o">=</span><span class="n">resnet50</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span><span class="mi">4096</span><span class="p">,</span> <span class="n">pretrained</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">projection_size</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">concat_pool</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="n">encoder</span> <span class="o">=</span> <span class="n">create_encoder</span><span class="p">(</span><span class="n">arch</span><span class="p">,</span> <span class="n">pretrained</span><span class="o">=</span><span class="n">pretrained</span><span class="p">,</span> <span class="n">concat_pool</span><span class="o">=</span><span class="n">concat_pool</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span> 
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">128</span><span class="p">,</span><span class="mi">128</span><span class="p">))</span>
        <span class="n">representation</span> <span class="o">=</span> <span class="n">encoder</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="n">projector</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span><span class="n">representation</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">projection_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span><span class="n">hidden_size</span><span class="p">)</span>     
    <span class="n">predictor</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span><span class="n">projection_size</span><span class="p">,</span> <span class="n">projection_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span><span class="n">hidden_size</span><span class="p">)</span>
    <span class="n">apply_init</span><span class="p">(</span><span class="n">projector</span><span class="p">)</span>
    <span class="n">apply_init</span><span class="p">(</span><span class="n">predictor</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">BYOLModel</span><span class="p">(</span><span class="n">encoder</span><span class="p">,</span> <span class="n">projector</span><span class="p">,</span> <span class="n">predictor</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="1.3-BYOLCallback">1.3 BYOLCallback<a class="anchor-link" href="#1.3-BYOLCallback"> </a></h3>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">_mse_loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">normalize</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">normalize</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="k">return</span> <span class="mi">2</span> <span class="o">-</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span> <span class="o">*</span> <span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">symmetric_mse_loss</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="o">*</span><span class="n">yb</span><span class="p">):</span>
    <span class="p">(</span><span class="n">q1</span><span class="p">,</span><span class="n">q2</span><span class="p">),</span><span class="n">z1</span><span class="p">,</span><span class="n">z2</span> <span class="o">=</span> <span class="n">pred</span><span class="p">,</span><span class="o">*</span><span class="n">yb</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">_mse_loss</span><span class="p">(</span><span class="n">q1</span><span class="p">,</span><span class="n">z2</span><span class="p">)</span> <span class="o">+</span> <span class="n">_mse_loss</span><span class="p">(</span><span class="n">q2</span><span class="p">,</span><span class="n">z1</span><span class="p">))</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">64</span><span class="p">,</span><span class="mi">256</span><span class="p">))</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">64</span><span class="p">,</span><span class="mi">256</span><span class="p">))</span>
<span class="n">test_close</span><span class="p">(</span><span class="n">symmetric_mse_loss</span><span class="p">((</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">),</span><span class="n">y</span><span class="p">,</span><span class="n">x</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span> <span class="c1"># perfect</span>
<span class="n">test_close</span><span class="p">(</span><span class="n">symmetric_mse_loss</span><span class="p">((</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">),</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">),</span> <span class="mi">4</span><span class="p">,</span> <span class="mf">1e-1</span><span class="p">)</span> <span class="c1"># random</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Useful Discussions and Supportive Material:</p>
<ul>
<li><a href="https://www.reddit.com/r/MachineLearning/comments/hju274/d_byol_bootstrap_your_own_latent_cheating/fwohtky/">https://www.reddit.com/r/MachineLearning/comments/hju274/d_byol_bootstrap_your_own_latent_cheating/fwohtky/</a></li>
<li><a href="https://untitled-ai.github.io/understanding-self-supervised-contrastive-learning.html">https://untitled-ai.github.io/understanding-self-supervised-contrastive-learning.html</a></li>
</ul>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">copy</span>

<span class="k">class</span> <span class="nc">BYOLCallback</span><span class="p">(</span><span class="n">Callback</span><span class="p">):</span>
    <span class="s2">&quot;Implementation of https://arxiv.org/pdf/2006.07733.pdf&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">T</span><span class="o">=</span><span class="mf">0.99</span><span class="p">,</span> <span class="n">debug</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">224</span><span class="p">,</span> <span class="o">**</span><span class="n">aug_kwargs</span><span class="p">):</span>        
        <span class="bp">self</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">debug</span> <span class="o">=</span> <span class="n">T</span><span class="p">,</span> <span class="n">debug</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">aug1</span> <span class="o">=</span> <span class="n">get_aug_pipe</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="o">**</span><span class="n">aug_kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">aug2</span> <span class="o">=</span> <span class="n">get_aug_pipe</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="o">**</span><span class="n">aug_kwargs</span><span class="p">)</span>


    <span class="k">def</span> <span class="nf">before_fit</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="s2">&quot;Create target model&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">target_model</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">learn</span><span class="o">.</span><span class="n">model</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dls</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>        
        <span class="bp">self</span><span class="o">.</span><span class="n">T_sched</span> <span class="o">=</span> <span class="n">SchedCos</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="c1"># used in paper</span>
        <span class="c1"># self.T_sched = SchedNo(self.T, 1) # used in open source implementation</span>
  
        
    <span class="k">def</span> <span class="nf">before_batch</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="s2">&quot;Generate 2 views of the same image and calculate target projections for these views&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">debug</span><span class="p">:</span> <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;self.x[0]: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        
        <span class="n">v1</span><span class="p">,</span><span class="n">v2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">aug1</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">aug2</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x</span><span class="o">.</span><span class="n">clone</span><span class="p">())</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">learn</span><span class="o">.</span><span class="n">xb</span> <span class="o">=</span> <span class="p">(</span><span class="n">v1</span><span class="p">,</span><span class="n">v2</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">debug</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;v1[0]: </span><span class="si">{</span><span class="n">v1</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="se">\n</span><span class="s2">v2[0]: </span><span class="si">{</span><span class="n">v2</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">show_one</span><span class="p">()</span>
            <span class="k">assert</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">learn</span><span class="o">.</span><span class="n">xb</span><span class="p">)</span>

        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">z1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">target_model</span><span class="o">.</span><span class="n">projector</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">target_model</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">v1</span><span class="p">))</span>
            <span class="n">z2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">target_model</span><span class="o">.</span><span class="n">projector</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">target_model</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">v2</span><span class="p">))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">learn</span><span class="o">.</span><span class="n">yb</span> <span class="o">=</span> <span class="p">(</span><span class="n">z1</span><span class="p">,</span><span class="n">z2</span><span class="p">)</span>


    <span class="k">def</span> <span class="nf">after_step</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="s2">&quot;Update target model and T&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">T</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">T_sched</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pct_train</span><span class="p">)</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="k">for</span> <span class="n">param_k</span><span class="p">,</span> <span class="n">param_q</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">target_model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">()):</span>
                <span class="n">param_k</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">param_k</span><span class="o">.</span><span class="n">data</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">T</span> <span class="o">+</span> <span class="n">param_q</span><span class="o">.</span><span class="n">data</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
          

    <span class="k">def</span> <span class="nf">show_one</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">b1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">aug1</span><span class="o">.</span><span class="n">normalize</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">to_detach</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">learn</span><span class="o">.</span><span class="n">xb</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
        <span class="n">b2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">aug1</span><span class="o">.</span><span class="n">normalize</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">to_detach</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">learn</span><span class="o">.</span><span class="n">xb</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
        <span class="n">i</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">b1</span><span class="p">))</span>
        <span class="n">show_images</span><span class="p">([</span><span class="n">b1</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="n">b2</span><span class="p">[</span><span class="n">i</span><span class="p">]],</span> <span class="n">nrows</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">after_train</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>      
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">debug</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">show_one</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">after_validate</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>   
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">debug</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">show_one</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="2.-Pretext-Training">2. Pretext Training<a class="anchor-link" href="#2.-Pretext-Training"> </a></h2>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">sqrmom</span><span class="o">=</span><span class="mf">0.99</span>
<span class="n">mom</span><span class="o">=</span><span class="mf">0.95</span>
<span class="n">beta</span><span class="o">=</span><span class="mf">0.</span>
<span class="n">eps</span><span class="o">=</span><span class="mf">1e-4</span>
<span class="n">opt_func</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">ranger</span><span class="p">,</span> <span class="n">mom</span><span class="o">=</span><span class="n">mom</span><span class="p">,</span> <span class="n">sqr_mom</span><span class="o">=</span><span class="n">sqrmom</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="n">beta</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">bs</span><span class="o">=</span><span class="mi">128</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">get_dls</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">bs</span><span class="p">,</span> <span class="n">workers</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">path</span> <span class="o">=</span> <span class="n">URLs</span><span class="o">.</span><span class="n">IMAGEWANG_160</span> <span class="k">if</span> <span class="n">size</span> <span class="o">&lt;=</span> <span class="mi">160</span> <span class="k">else</span> <span class="n">URLs</span><span class="o">.</span><span class="n">IMAGEWANG</span>
    <span class="n">source</span> <span class="o">=</span> <span class="n">untar_data</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>
    
    <span class="n">files</span> <span class="o">=</span> <span class="n">get_image_files</span><span class="p">(</span><span class="n">source</span><span class="p">)</span>
    <span class="n">tfms</span> <span class="o">=</span> <span class="p">[[</span><span class="n">PILImage</span><span class="o">.</span><span class="n">create</span><span class="p">,</span> <span class="n">ToTensor</span><span class="p">,</span> <span class="n">RandomResizedCrop</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">min_scale</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)],</span> 
            <span class="p">[</span><span class="n">parent_label</span><span class="p">,</span> <span class="n">Categorize</span><span class="p">()]]</span>
    
    <span class="n">dsets</span> <span class="o">=</span> <span class="n">Datasets</span><span class="p">(</span><span class="n">files</span><span class="p">,</span> <span class="n">tfms</span><span class="o">=</span><span class="n">tfms</span><span class="p">,</span> <span class="n">splits</span><span class="o">=</span><span class="n">RandomSplitter</span><span class="p">(</span><span class="n">valid_pct</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)(</span><span class="n">files</span><span class="p">))</span>
    
    <span class="n">batch_tfms</span> <span class="o">=</span> <span class="p">[</span><span class="n">IntToFloatTensor</span><span class="p">]</span>
    <span class="n">dls</span> <span class="o">=</span> <span class="n">dsets</span><span class="o">.</span><span class="n">dataloaders</span><span class="p">(</span><span class="n">bs</span><span class="o">=</span><span class="n">bs</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="n">workers</span><span class="p">,</span> <span class="n">after_batch</span><span class="o">=</span><span class="n">batch_tfms</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">dls</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">dls</span> <span class="o">=</span> <span class="n">get_dls</span><span class="p">(</span><span class="n">resize</span><span class="p">,</span> <span class="n">bs</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">create_byol_model</span><span class="p">(</span><span class="n">arch</span><span class="o">=</span><span class="n">xresnet34</span><span class="p">,</span> <span class="n">pretrained</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">learn</span> <span class="o">=</span> <span class="n">Learner</span><span class="p">(</span><span class="n">dls</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">symmetric_mse_loss</span><span class="p">,</span> <span class="n">opt_func</span><span class="o">=</span><span class="n">opt_func</span><span class="p">,</span>
                <span class="n">cbs</span><span class="o">=</span><span class="p">[</span><span class="n">BYOLCallback</span><span class="p">(</span><span class="n">T</span><span class="o">=</span><span class="mf">0.99</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">size</span><span class="p">,</span> <span class="n">debug</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span> <span class="n">TerminateOnNaNCallback</span><span class="p">()])</span>
<span class="n">learn</span><span class="o">.</span><span class="n">to_fp16</span><span class="p">();</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learn</span><span class="o">.</span><span class="n">lr_find</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">

</div>

</div>

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>SuggestedLRs(lr_min=0.017378008365631102, lr_steep=0.0010000000474974513)</pre>
</div>

</div>

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,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
"
>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span>
<span class="n">wd</span><span class="o">=</span><span class="mf">1e-2</span>
<span class="n">epochs</span><span class="o">=</span><span class="mi">100</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learn</span><span class="o">.</span><span class="n">unfreeze</span><span class="p">()</span>
<span class="n">learn</span><span class="o">.</span><span class="n">fit_flat_cos</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">wd</span><span class="o">=</span><span class="n">wd</span><span class="p">,</span> <span class="n">pct_start</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>1.510650</td>
      <td>1.428953</td>
      <td>04:15</td>
    </tr>
    <tr>
      <td>1</td>
      <td>1.189431</td>
      <td>1.137174</td>
      <td>04:13</td>
    </tr>
    <tr>
      <td>2</td>
      <td>1.013760</td>
      <td>1.007405</td>
      <td>04:14</td>
    </tr>
    <tr>
      <td>3</td>
      <td>0.870598</td>
      <td>0.874674</td>
      <td>04:14</td>
    </tr>
    <tr>
      <td>4</td>
      <td>0.782515</td>
      <td>0.789410</td>
      <td>04:18</td>
    </tr>
    <tr>
      <td>5</td>
      <td>0.714516</td>
      <td>0.736312</td>
      <td>04:18</td>
    </tr>
    <tr>
      <td>6</td>
      <td>0.666287</td>
      <td>0.775844</td>
      <td>04:18</td>
    </tr>
    <tr>
      <td>7</td>
      <td>0.637795</td>
      <td>0.718347</td>
      <td>04:19</td>
    </tr>
    <tr>
      <td>8</td>
      <td>0.615426</td>
      <td>0.657362</td>
      <td>04:19</td>
    </tr>
    <tr>
      <td>9</td>
      <td>0.587785</td>
      <td>0.708780</td>
      <td>04:18</td>
    </tr>
    <tr>
      <td>10</td>
      <td>0.585688</td>
      <td>0.667950</td>
      <td>04:19</td>
    </tr>
    <tr>
      <td>11</td>
      <td>0.571569</td>
      <td>0.637994</td>
      <td>04:18</td>
    </tr>
    <tr>
      <td>12</td>
      <td>0.577905</td>
      <td>0.638273</td>
      <td>04:19</td>
    </tr>
    <tr>
      <td>13</td>
      <td>0.566818</td>
      <td>0.614410</td>
      <td>04:17</td>
    </tr>
    <tr>
      <td>14</td>
      <td>0.563347</td>
      <td>0.661758</td>
      <td>04:18</td>
    </tr>
    <tr>
      <td>15</td>
      <td>0.552199</td>
      <td>0.650028</td>
      <td>04:18</td>
    </tr>
    <tr>
      <td>16</td>
      <td>0.544313</td>
      <td>0.671450</td>
      <td>04:19</td>
    </tr>
    <tr>
      <td>17</td>
      <td>0.532093</td>
      <td>0.634962</td>
      <td>04:18</td>
    </tr>
    <tr>
      <td>18</td>
      <td>0.534365</td>
      <td>0.670031</td>
      <td>04:19</td>
    </tr>
    <tr>
      <td>19</td>
      <td>0.520584</td>
      <td>0.715764</td>
      <td>04:19</td>
    </tr>
    <tr>
      <td>20</td>
      <td>0.504949</td>
      <td>0.626124</td>
      <td>04:18</td>
    </tr>
    <tr>
      <td>21</td>
      <td>0.497023</td>
      <td>0.589880</td>
      <td>04:19</td>
    </tr>
    <tr>
      <td>22</td>
      <td>0.483837</td>
      <td>0.654766</td>
      <td>04:19</td>
    </tr>
    <tr>
      <td>23</td>
      <td>0.492508</td>
      <td>0.623388</td>
      <td>04:19</td>
    </tr>
    <tr>
      <td>24</td>
      <td>0.496307</td>
      <td>0.655770</td>
      <td>04:19</td>
    </tr>
    <tr>
      <td>25</td>
      <td>0.492501</td>
      <td>0.659534</td>
      <td>04:18</td>
    </tr>
    <tr>
      <td>26</td>
      <td>0.475777</td>
      <td>0.667866</td>
      <td>04:19</td>
    </tr>
    <tr>
      <td>27</td>
      <td>0.457665</td>
      <td>0.710385</td>
      <td>04:18</td>
    </tr>
    <tr>
      <td>28</td>
      <td>0.452935</td>
      <td>0.695267</td>
      <td>04:18</td>
    </tr>
    <tr>
      <td>29</td>
      <td>0.440915</td>
      <td>0.694055</td>
      <td>04:18</td>
    </tr>
    <tr>
      <td>30</td>
      <td>0.450249</td>
      <td>0.673101</td>
      <td>04:18</td>
    </tr>
    <tr>
      <td>31</td>
      <td>0.444892</td>
      <td>0.638658</td>
      <td>04:17</td>
    </tr>
    <tr>
      <td>32</td>
      <td>0.436403</td>
      <td>0.578198</td>
      <td>04:14</td>
    </tr>
    <tr>
      <td>33</td>
      <td>0.425970</td>
      <td>0.632715</td>
      <td>04:15</td>
    </tr>
    <tr>
      <td>34</td>
      <td>0.415532</td>
      <td>0.653762</td>
      <td>04:14</td>
    </tr>
    <tr>
      <td>35</td>
      <td>0.439449</td>
      <td>0.558321</td>
      <td>04:14</td>
    </tr>
    <tr>
      <td>36</td>
      <td>0.437598</td>
      <td>0.604767</td>
      <td>04:15</td>
    </tr>
    <tr>
      <td>37</td>
      <td>0.423862</td>
      <td>0.707436</td>
      <td>04:15</td>
    </tr>
    <tr>
      <td>38</td>
      <td>0.432040</td>
      <td>0.609531</td>
      <td>04:15</td>
    </tr>
    <tr>
      <td>39</td>
      <td>0.415748</td>
      <td>0.660977</td>
      <td>04:14</td>
    </tr>
    <tr>
      <td>40</td>
      <td>0.408203</td>
      <td>0.596941</td>
      <td>04:15</td>
    </tr>
    <tr>
      <td>41</td>
      <td>0.414880</td>
      <td>0.618571</td>
      <td>04:15</td>
    </tr>
    <tr>
      <td>42</td>
      <td>0.406859</td>
      <td>0.646657</td>
      <td>04:15</td>
    </tr>
    <tr>
      <td>43</td>
      <td>0.408090</td>
      <td>0.731391</td>
      <td>04:14</td>
    </tr>
    <tr>
      <td>44</td>
      <td>0.415170</td>
      <td>0.615768</td>
      <td>04:15</td>
    </tr>
    <tr>
      <td>45</td>
      <td>0.397388</td>
      <td>0.734314</td>
      <td>04:15</td>
    </tr>
    <tr>
      <td>46</td>
      <td>0.381527</td>
      <td>0.650072</td>
      <td>04:15</td>
    </tr>
    <tr>
      <td>47</td>
      <td>0.403838</td>
      <td>0.628381</td>
      <td>04:15</td>
    </tr>
    <tr>
      <td>48</td>
      <td>0.382466</td>
      <td>0.538990</td>
      <td>04:15</td>
    </tr>
    <tr>
      <td>49</td>
      <td>0.372932</td>
      <td>0.549913</td>
      <td>04:15</td>
    </tr>
    <tr>
      <td>50</td>
      <td>0.402139</td>
      <td>0.622850</td>
      <td>04:15</td>
    </tr>
    <tr>
      <td>51</td>
      <td>0.383040</td>
      <td>0.678353</td>
      <td>04:15</td>
    </tr>
    <tr>
      <td>52</td>
      <td>0.391508</td>
      <td>0.628491</td>
      <td>04:15</td>
    </tr>
    <tr>
      <td>53</td>
      <td>0.376283</td>
      <td>0.634476</td>
      <td>04:15</td>
    </tr>
    <tr>
      <td>54</td>
      <td>0.382289</td>
      <td>0.622772</td>
      <td>04:15</td>
    </tr>
    <tr>
      <td>55</td>
      <td>0.355336</td>
      <td>0.704124</td>
      <td>04:14</td>
    </tr>
    <tr>
      <td>56</td>
      <td>0.384981</td>
      <td>0.650805</td>
      <td>04:15</td>
    </tr>
    <tr>
      <td>57</td>
      <td>0.366950</td>
      <td>0.593020</td>
      <td>04:16</td>
    </tr>
    <tr>
      <td>58</td>
      <td>0.358975</td>
      <td>0.632675</td>
      <td>04:15</td>
    </tr>
    <tr>
      <td>59</td>
      <td>0.352353</td>
      <td>0.611346</td>
      <td>04:14</td>
    </tr>
    <tr>
      <td>60</td>
      <td>0.365405</td>
      <td>0.588207</td>
      <td>04:15</td>
    </tr>
    <tr>
      <td>61</td>
      <td>0.356183</td>
      <td>0.654768</td>
      <td>04:15</td>
    </tr>
    <tr>
      <td>62</td>
      <td>0.361821</td>
      <td>0.600484</td>
      <td>04:15</td>
    </tr>
    <tr>
      <td>63</td>
      <td>0.372355</td>
      <td>0.589349</td>
      <td>04:13</td>
    </tr>
    <tr>
      <td>64</td>
      <td>0.370324</td>
      <td>0.573487</td>
      <td>04:15</td>
    </tr>
    <tr>
      <td>65</td>
      <td>0.336848</td>
      <td>0.556879</td>
      <td>04:14</td>
    </tr>
    <tr>
      <td>66</td>
      <td>0.360434</td>
      <td>0.572379</td>
      <td>04:14</td>
    </tr>
    <tr>
      <td>67</td>
      <td>0.364292</td>
      <td>0.593386</td>
      <td>04:15</td>
    </tr>
    <tr>
      <td>68</td>
      <td>0.352744</td>
      <td>0.590681</td>
      <td>04:15</td>
    </tr>
    <tr>
      <td>69</td>
      <td>0.348467</td>
      <td>0.518823</td>
      <td>04:15</td>
    </tr>
    <tr>
      <td>70</td>
      <td>0.353090</td>
      <td>0.667910</td>
      <td>04:15</td>
    </tr>
    <tr>
      <td>71</td>
      <td>0.350583</td>
      <td>0.635499</td>
      <td>04:15</td>
    </tr>
    <tr>
      <td>72</td>
      <td>0.348770</td>
      <td>0.541379</td>
      <td>04:14</td>
    </tr>
    <tr>
      <td>73</td>
      <td>0.342985</td>
      <td>0.563077</td>
      <td>04:15</td>
    </tr>
    <tr>
      <td>74</td>
      <td>0.338370</td>
      <td>0.536818</td>
      <td>04:14</td>
    </tr>
    <tr>
      <td>75</td>
      <td>0.338095</td>
      <td>0.505258</td>
      <td>04:15</td>
    </tr>
    <tr>
      <td>76</td>
      <td>0.351014</td>
      <td>0.605423</td>
      <td>04:14</td>
    </tr>
    <tr>
      <td>77</td>
      <td>0.348474</td>
      <td>0.515493</td>
      <td>04:15</td>
    </tr>
    <tr>
      <td>78</td>
      <td>0.344178</td>
      <td>0.537567</td>
      <td>04:15</td>
    </tr>
    <tr>
      <td>79</td>
      <td>0.353369</td>
      <td>0.534423</td>
      <td>04:15</td>
    </tr>
    <tr>
      <td>80</td>
      <td>0.359703</td>
      <td>0.642740</td>
      <td>04:15</td>
    </tr>
    <tr>
      <td>81</td>
      <td>0.337352</td>
      <td>0.551877</td>
      <td>04:15</td>
    </tr>
    <tr>
      <td>82</td>
      <td>0.310806</td>
      <td>0.720433</td>
      <td>04:18</td>
    </tr>
    <tr>
      <td>83</td>
      <td>0.315536</td>
      <td>0.509390</td>
      <td>04:20</td>
    </tr>
    <tr>
      <td>84</td>
      <td>0.313574</td>
      <td>0.556933</td>
      <td>04:20</td>
    </tr>
    <tr>
      <td>85</td>
      <td>0.330962</td>
      <td>0.517258</td>
      <td>04:19</td>
    </tr>
    <tr>
      <td>86</td>
      <td>0.343858</td>
      <td>0.473978</td>
      <td>04:19</td>
    </tr>
    <tr>
      <td>87</td>
      <td>0.324197</td>
      <td>0.592106</td>
      <td>04:20</td>
    </tr>
    <tr>
      <td>88</td>
      <td>0.332582</td>
      <td>0.565583</td>
      <td>04:20</td>
    </tr>
    <tr>
      <td>89</td>
      <td>0.329625</td>
      <td>0.654608</td>
      <td>04:21</td>
    </tr>
    <tr>
      <td>90</td>
      <td>0.334619</td>
      <td>0.537063</td>
      <td>04:21</td>
    </tr>
    <tr>
      <td>91</td>
      <td>0.327584</td>
      <td>0.601759</td>
      <td>04:21</td>
    </tr>
    <tr>
      <td>92</td>
      <td>0.336925</td>
      <td>0.562072</td>
      <td>04:21</td>
    </tr>
    <tr>
      <td>93</td>
      <td>0.320915</td>
      <td>0.581007</td>
      <td>04:20</td>
    </tr>
    <tr>
      <td>94</td>
      <td>0.342443</td>
      <td>0.559198</td>
      <td>04:21</td>
    </tr>
    <tr>
      <td>95</td>
      <td>0.327763</td>
      <td>0.634018</td>
      <td>04:21</td>
    </tr>
    <tr>
      <td>96</td>
      <td>0.317442</td>
      <td>0.582927</td>
      <td>04:21</td>
    </tr>
    <tr>
      <td>97</td>
      <td>0.315093</td>
      <td>0.534479</td>
      <td>04:20</td>
    </tr>
    <tr>
      <td>98</td>
      <td>0.344822</td>
      <td>0.602893</td>
      <td>04:20</td>
    </tr>
    <tr>
      <td>99</td>
      <td>0.326197</td>
      <td>0.498655</td>
      <td>04:20</td>
    </tr>
  </tbody>
</table>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">save_name</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;byol_iwang_sz</span><span class="si">{</span><span class="n">size</span><span class="si">}</span><span class="s1">_epc</span><span class="si">{</span><span class="n">epochs</span><span class="si">}</span><span class="s1">&#39;</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learn</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">save_name</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">learn</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="n">learn</span><span class="o">.</span><span class="n">path</span><span class="o">/</span><span class="n">learn</span><span class="o">.</span><span class="n">model_dir</span><span class="o">/</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">save_name</span><span class="si">}</span><span class="s1">_encoder.pth&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learn</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">save_name</span><span class="p">);</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">lr</span><span class="o">=</span><span class="mf">1e-4</span>
<span class="n">wd</span><span class="o">=</span><span class="mf">1e-2</span>
<span class="n">epochs</span><span class="o">=</span><span class="mi">100</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learn</span><span class="o">.</span><span class="n">unfreeze</span><span class="p">()</span>
<span class="n">learn</span><span class="o">.</span><span class="n">fit_flat_cos</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">wd</span><span class="o">=</span><span class="n">wd</span><span class="p">,</span> <span class="n">pct_start</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0.519158</td>
      <td>0.628242</td>
      <td>04:19</td>
    </tr>
    <tr>
      <td>1</td>
      <td>0.549050</td>
      <td>0.726779</td>
      <td>04:19</td>
    </tr>
    <tr>
      <td>2</td>
      <td>0.545667</td>
      <td>0.614504</td>
      <td>04:17</td>
    </tr>
    <tr>
      <td>3</td>
      <td>0.562666</td>
      <td>0.669120</td>
      <td>04:18</td>
    </tr>
    <tr>
      <td>4</td>
      <td>0.508613</td>
      <td>0.688010</td>
      <td>04:19</td>
    </tr>
    <tr>
      <td>5</td>
      <td>0.536102</td>
      <td>0.744773</td>
      <td>04:18</td>
    </tr>
    <tr>
      <td>6</td>
      <td>0.541971</td>
      <td>0.726328</td>
      <td>04:17</td>
    </tr>
    <tr>
      <td>7</td>
      <td>0.511115</td>
      <td>0.628922</td>
      <td>04:13</td>
    </tr>
    <tr>
      <td>8</td>
      <td>0.530850</td>
      <td>0.678011</td>
      <td>04:12</td>
    </tr>
    <tr>
      <td>9</td>
      <td>0.526052</td>
      <td>0.656479</td>
      <td>04:14</td>
    </tr>
    <tr>
      <td>10</td>
      <td>0.528839</td>
      <td>0.620612</td>
      <td>04:13</td>
    </tr>
    <tr>
      <td>11</td>
      <td>0.558364</td>
      <td>0.675659</td>
      <td>04:13</td>
    </tr>
    <tr>
      <td>12</td>
      <td>0.548368</td>
      <td>0.736750</td>
      <td>04:13</td>
    </tr>
    <tr>
      <td>13</td>
      <td>0.518878</td>
      <td>0.724713</td>
      <td>04:11</td>
    </tr>
    <tr>
      <td>14</td>
      <td>0.537066</td>
      <td>0.680293</td>
      <td>04:13</td>
    </tr>
    <tr>
      <td>15</td>
      <td>0.488179</td>
      <td>0.701762</td>
      <td>04:13</td>
    </tr>
    <tr>
      <td>16</td>
      <td>0.546877</td>
      <td>0.557397</td>
      <td>04:13</td>
    </tr>
    <tr>
      <td>17</td>
      <td>0.552370</td>
      <td>0.847853</td>
      <td>04:14</td>
    </tr>
    <tr>
      <td>18</td>
      <td>0.551189</td>
      <td>0.676739</td>
      <td>04:11</td>
    </tr>
    <tr>
      <td>19</td>
      <td>0.528872</td>
      <td>0.604637</td>
      <td>04:13</td>
    </tr>
    <tr>
      <td>20</td>
      <td>0.567332</td>
      <td>0.659197</td>
      <td>04:12</td>
    </tr>
    <tr>
      <td>21</td>
      <td>0.538007</td>
      <td>0.698788</td>
      <td>04:13</td>
    </tr>
    <tr>
      <td>22</td>
      <td>0.546437</td>
      <td>0.746676</td>
      <td>04:13</td>
    </tr>
    <tr>
      <td>23</td>
      <td>0.532490</td>
      <td>0.669640</td>
      <td>04:12</td>
    </tr>
    <tr>
      <td>24</td>
      <td>0.541545</td>
      <td>0.623996</td>
      <td>04:13</td>
    </tr>
    <tr>
      <td>25</td>
      <td>0.518695</td>
      <td>0.875697</td>
      <td>04:13</td>
    </tr>
    <tr>
      <td>26</td>
      <td>0.543459</td>
      <td>0.787592</td>
      <td>04:12</td>
    </tr>
    <tr>
      <td>27</td>
      <td>0.562594</td>
      <td>0.725982</td>
      <td>04:12</td>
    </tr>
    <tr>
      <td>28</td>
      <td>0.526342</td>
      <td>0.745401</td>
      <td>04:13</td>
    </tr>
    <tr>
      <td>29</td>
      <td>0.544669</td>
      <td>0.675511</td>
      <td>04:13</td>
    </tr>
    <tr>
      <td>30</td>
      <td>0.522315</td>
      <td>0.722699</td>
      <td>04:12</td>
    </tr>
    <tr>
      <td>31</td>
      <td>0.546437</td>
      <td>0.801447</td>
      <td>04:13</td>
    </tr>
    <tr>
      <td>32</td>
      <td>0.503957</td>
      <td>0.643209</td>
      <td>04:13</td>
    </tr>
    <tr>
      <td>33</td>
      <td>0.493882</td>
      <td>0.733770</td>
      <td>04:12</td>
    </tr>
    <tr>
      <td>34</td>
      <td>0.549889</td>
      <td>0.722888</td>
      <td>04:12</td>
    </tr>
    <tr>
      <td>35</td>
      <td>0.523531</td>
      <td>0.688972</td>
      <td>04:14</td>
    </tr>
    <tr>
      <td>36</td>
      <td>0.534279</td>
      <td>0.752815</td>
      <td>04:12</td>
    </tr>
    <tr>
      <td>37</td>
      <td>0.539623</td>
      <td>0.642777</td>
      <td>04:14</td>
    </tr>
    <tr>
      <td>38</td>
      <td>0.533936</td>
      <td>0.663726</td>
      <td>04:13</td>
    </tr>
    <tr>
      <td>39</td>
      <td>0.514049</td>
      <td>0.714655</td>
      <td>04:13</td>
    </tr>
    <tr>
      <td>40</td>
      <td>0.528506</td>
      <td>0.730735</td>
      <td>04:13</td>
    </tr>
    <tr>
      <td>41</td>
      <td>0.524455</td>
      <td>0.599572</td>
      <td>04:13</td>
    </tr>
    <tr>
      <td>42</td>
      <td>0.541315</td>
      <td>0.733151</td>
      <td>04:14</td>
    </tr>
    <tr>
      <td>43</td>
      <td>0.517550</td>
      <td>0.740385</td>
      <td>04:13</td>
    </tr>
    <tr>
      <td>44</td>
      <td>0.507592</td>
      <td>0.738724</td>
      <td>04:13</td>
    </tr>
    <tr>
      <td>45</td>
      <td>0.516882</td>
      <td>0.666788</td>
      <td>04:14</td>
    </tr>
    <tr>
      <td>46</td>
      <td>0.536027</td>
      <td>0.615597</td>
      <td>04:13</td>
    </tr>
    <tr>
      <td>47</td>
      <td>0.555497</td>
      <td>0.640408</td>
      <td>04:14</td>
    </tr>
    <tr>
      <td>48</td>
      <td>0.535616</td>
      <td>0.750397</td>
      <td>04:12</td>
    </tr>
    <tr>
      <td>49</td>
      <td>0.522329</td>
      <td>0.611259</td>
      <td>04:12</td>
    </tr>
    <tr>
      <td>50</td>
      <td>0.535572</td>
      <td>0.601873</td>
      <td>04:12</td>
    </tr>
    <tr>
      <td>51</td>
      <td>0.535535</td>
      <td>0.759740</td>
      <td>04:13</td>
    </tr>
    <tr>
      <td>52</td>
      <td>0.545863</td>
      <td>0.741486</td>
      <td>04:13</td>
    </tr>
    <tr>
      <td>53</td>
      <td>0.524647</td>
      <td>0.850955</td>
      <td>04:13</td>
    </tr>
    <tr>
      <td>54</td>
      <td>0.524400</td>
      <td>0.655291</td>
      <td>04:12</td>
    </tr>
    <tr>
      <td>55</td>
      <td>0.506782</td>
      <td>0.570880</td>
      <td>04:12</td>
    </tr>
    <tr>
      <td>56</td>
      <td>0.539459</td>
      <td>0.758154</td>
      <td>04:11</td>
    </tr>
    <tr>
      <td>57</td>
      <td>0.547187</td>
      <td>0.636952</td>
      <td>04:13</td>
    </tr>
    <tr>
      <td>58</td>
      <td>0.494504</td>
      <td>0.715809</td>
      <td>04:14</td>
    </tr>
    <tr>
      <td>59</td>
      <td>0.532562</td>
      <td>0.518848</td>
      <td>04:14</td>
    </tr>
    <tr>
      <td>60</td>
      <td>0.523814</td>
      <td>0.637461</td>
      <td>04:13</td>
    </tr>
    <tr>
      <td>61</td>
      <td>0.518878</td>
      <td>0.578066</td>
      <td>04:13</td>
    </tr>
    <tr>
      <td>62</td>
      <td>0.532009</td>
      <td>0.761286</td>
      <td>04:13</td>
    </tr>
    <tr>
      <td>63</td>
      <td>0.567297</td>
      <td>0.549323</td>
      <td>04:12</td>
    </tr>
    <tr>
      <td>64</td>
      <td>0.534799</td>
      <td>0.570944</td>
      <td>04:13</td>
    </tr>
    <tr>
      <td>65</td>
      <td>0.518220</td>
      <td>0.666568</td>
      <td>04:13</td>
    </tr>
    <tr>
      <td>66</td>
      <td>0.521687</td>
      <td>0.594923</td>
      <td>04:12</td>
    </tr>
    <tr>
      <td>67</td>
      <td>0.531712</td>
      <td>0.775418</td>
      <td>04:13</td>
    </tr>
    <tr>
      <td>68</td>
      <td>0.554268</td>
      <td>0.629959</td>
      <td>04:13</td>
    </tr>
    <tr>
      <td>69</td>
      <td>0.529651</td>
      <td>0.655741</td>
      <td>04:13</td>
    </tr>
    <tr>
      <td>70</td>
      <td>0.517277</td>
      <td>0.683746</td>
      <td>04:13</td>
    </tr>
    <tr>
      <td>71</td>
      <td>0.492031</td>
      <td>0.563093</td>
      <td>04:12</td>
    </tr>
    <tr>
      <td>72</td>
      <td>0.537813</td>
      <td>0.713130</td>
      <td>04:13</td>
    </tr>
    <tr>
      <td>73</td>
      <td>0.531758</td>
      <td>0.469511</td>
      <td>04:13</td>
    </tr>
    <tr>
      <td>74</td>
      <td>0.524010</td>
      <td>0.663819</td>
      <td>04:13</td>
    </tr>
    <tr>
      <td>75</td>
      <td>0.543731</td>
      <td>0.613636</td>
      <td>04:14</td>
    </tr>
    <tr>
      <td>76</td>
      <td>0.550026</td>
      <td>0.635526</td>
      <td>04:13</td>
    </tr>
    <tr>
      <td>77</td>
      <td>0.524632</td>
      <td>0.758017</td>
      <td>04:13</td>
    </tr>
    <tr>
      <td>78</td>
      <td>0.533476</td>
      <td>0.617694</td>
      <td>04:12</td>
    </tr>
    <tr>
      <td>79</td>
      <td>0.539704</td>
      <td>0.728712</td>
      <td>04:13</td>
    </tr>
    <tr>
      <td>80</td>
      <td>0.527345</td>
      <td>0.637875</td>
      <td>04:12</td>
    </tr>
    <tr>
      <td>81</td>
      <td>0.553183</td>
      <td>0.595747</td>
      <td>04:14</td>
    </tr>
    <tr>
      <td>82</td>
      <td>0.546089</td>
      <td>0.576809</td>
      <td>04:13</td>
    </tr>
    <tr>
      <td>83</td>
      <td>0.531447</td>
      <td>0.618709</td>
      <td>04:13</td>
    </tr>
    <tr>
      <td>84</td>
      <td>0.535296</td>
      <td>0.639590</td>
      <td>04:12</td>
    </tr>
    <tr>
      <td>85</td>
      <td>0.514978</td>
      <td>0.739810</td>
      <td>04:13</td>
    </tr>
    <tr>
      <td>86</td>
      <td>0.485444</td>
      <td>0.639926</td>
      <td>04:13</td>
    </tr>
    <tr>
      <td>87</td>
      <td>0.508214</td>
      <td>0.649567</td>
      <td>04:12</td>
    </tr>
    <tr>
      <td>88</td>
      <td>0.524849</td>
      <td>0.825525</td>
      <td>04:13</td>
    </tr>
    <tr>
      <td>89</td>
      <td>0.509814</td>
      <td>0.568600</td>
      <td>04:12</td>
    </tr>
    <tr>
      <td>90</td>
      <td>0.535432</td>
      <td>0.752268</td>
      <td>04:13</td>
    </tr>
    <tr>
      <td>91</td>
      <td>0.536279</td>
      <td>0.785309</td>
      <td>04:13</td>
    </tr>
    <tr>
      <td>92</td>
      <td>0.534026</td>
      <td>0.511915</td>
      <td>04:13</td>
    </tr>
    <tr>
      <td>93</td>
      <td>0.514055</td>
      <td>0.721888</td>
      <td>04:13</td>
    </tr>
    <tr>
      <td>94</td>
      <td>0.522669</td>
      <td>0.691959</td>
      <td>04:12</td>
    </tr>
    <tr>
      <td>95</td>
      <td>0.531257</td>
      <td>0.694616</td>
      <td>04:13</td>
    </tr>
    <tr>
      <td>96</td>
      <td>0.517773</td>
      <td>0.639445</td>
      <td>04:13</td>
    </tr>
    <tr>
      <td>97</td>
      <td>0.531781</td>
      <td>0.560430</td>
      <td>04:13</td>
    </tr>
    <tr>
      <td>98</td>
      <td>0.524362</td>
      <td>0.702402</td>
      <td>04:13</td>
    </tr>
    <tr>
      <td>99</td>
      <td>0.517447</td>
      <td>0.598186</td>
      <td>04:13</td>
    </tr>
  </tbody>
</table>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">save_name</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;byol_iwang_sz</span><span class="si">{</span><span class="n">size</span><span class="si">}</span><span class="s1">_epc200&#39;</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learn</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">save_name</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">learn</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="n">learn</span><span class="o">.</span><span class="n">path</span><span class="o">/</span><span class="n">learn</span><span class="o">.</span><span class="n">model_dir</span><span class="o">/</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">save_name</span><span class="si">}</span><span class="s1">_encoder.pth&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">lr</span><span class="o">=</span><span class="mf">1e-4</span>
<span class="n">wd</span><span class="o">=</span><span class="mf">1e-2</span>
<span class="n">epochs</span><span class="o">=</span><span class="mi">30</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learn</span><span class="o">.</span><span class="n">unfreeze</span><span class="p">()</span>
<span class="n">learn</span><span class="o">.</span><span class="n">fit_flat_cos</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">wd</span><span class="o">=</span><span class="n">wd</span><span class="p">,</span> <span class="n">pct_start</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">

    <div>
        <style>
            /* Turns off some styling */
            progress {
                /* gets rid of default border in Firefox and Opera. */
                border: none;
                /* Needs to be in here for Safari polyfill so background images work as expected. */
                background-size: auto;
            }
            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
                background: #F44336;
            }
        </style>
      <progress value='29' class='' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>
      96.67% [29/30 2:03:55<04:16]
    </div>
    
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0.528907</td>
      <td>0.646549</td>
      <td>04:13</td>
    </tr>
    <tr>
      <td>1</td>
      <td>0.530606</td>
      <td>0.724795</td>
      <td>04:12</td>
    </tr>
    <tr>
      <td>2</td>
      <td>0.518430</td>
      <td>0.589998</td>
      <td>04:15</td>
    </tr>
    <tr>
      <td>3</td>
      <td>0.529843</td>
      <td>0.643619</td>
      <td>04:16</td>
    </tr>
    <tr>
      <td>4</td>
      <td>0.542198</td>
      <td>0.625090</td>
      <td>04:16</td>
    </tr>
    <tr>
      <td>5</td>
      <td>0.528748</td>
      <td>0.609527</td>
      <td>04:16</td>
    </tr>
    <tr>
      <td>6</td>
      <td>0.535782</td>
      <td>0.634512</td>
      <td>04:16</td>
    </tr>
    <tr>
      <td>7</td>
      <td>0.531877</td>
      <td>0.683111</td>
      <td>04:15</td>
    </tr>
    <tr>
      <td>8</td>
      <td>0.519696</td>
      <td>0.574612</td>
      <td>04:16</td>
    </tr>
    <tr>
      <td>9</td>
      <td>0.530617</td>
      <td>0.544109</td>
      <td>04:16</td>
    </tr>
    <tr>
      <td>10</td>
      <td>0.529557</td>
      <td>0.767183</td>
      <td>04:17</td>
    </tr>
    <tr>
      <td>11</td>
      <td>0.541937</td>
      <td>0.628681</td>
      <td>04:16</td>
    </tr>
    <tr>
      <td>12</td>
      <td>0.536410</td>
      <td>0.634613</td>
      <td>04:16</td>
    </tr>
    <tr>
      <td>13</td>
      <td>0.514949</td>
      <td>0.777293</td>
      <td>04:17</td>
    </tr>
    <tr>
      <td>14</td>
      <td>0.536896</td>
      <td>0.597643</td>
      <td>04:17</td>
    </tr>
    <tr>
      <td>15</td>
      <td>0.527864</td>
      <td>0.638974</td>
      <td>04:16</td>
    </tr>
    <tr>
      <td>16</td>
      <td>0.515085</td>
      <td>0.654626</td>
      <td>04:15</td>
    </tr>
    <tr>
      <td>17</td>
      <td>0.542489</td>
      <td>0.581779</td>
      <td>04:16</td>
    </tr>
    <tr>
      <td>18</td>
      <td>0.533649</td>
      <td>0.584234</td>
      <td>04:16</td>
    </tr>
    <tr>
      <td>19</td>
      <td>0.506855</td>
      <td>0.788878</td>
      <td>04:15</td>
    </tr>
    <tr>
      <td>20</td>
      <td>0.518647</td>
      <td>0.621040</td>
      <td>04:16</td>
    </tr>
    <tr>
      <td>21</td>
      <td>0.518384</td>
      <td>0.655662</td>
      <td>04:17</td>
    </tr>
    <tr>
      <td>22</td>
      <td>0.525624</td>
      <td>0.844663</td>
      <td>04:16</td>
    </tr>
    <tr>
      <td>23</td>
      <td>0.544119</td>
      <td>0.664328</td>
      <td>04:16</td>
    </tr>
    <tr>
      <td>24</td>
      <td>0.544992</td>
      <td>0.594243</td>
      <td>04:16</td>
    </tr>
    <tr>
      <td>25</td>
      <td>0.527378</td>
      <td>0.643575</td>
      <td>04:17</td>
    </tr>
    <tr>
      <td>26</td>
      <td>0.513017</td>
      <td>0.666018</td>
      <td>04:16</td>
    </tr>
    <tr>
      <td>27</td>
      <td>0.530246</td>
      <td>0.645612</td>
      <td>04:17</td>
    </tr>
    <tr>
      <td>28</td>
      <td>0.521460</td>
      <td>0.502634</td>
      <td>04:16</td>
    </tr>
  </tbody>
</table><p>

    <div>
        <style>
            /* Turns off some styling */
            progress {
                /* gets rid of default border in Firefox and Opera. */
                border: none;
                /* Needs to be in here for Safari polyfill so background images work as expected. */
                background-size: auto;
            }
            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
                background: #F44336;
            }
        </style>
      <progress value='155' class='' max='185' style='width:300px; height:20px; vertical-align: middle;'></progress>
      83.78% [155/185 03:12<00:37 0.5423]
    </div>
    
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">save_name</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;byol_iwang_sz</span><span class="si">{</span><span class="n">size</span><span class="si">}</span><span class="s1">_epc230&#39;</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learn</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">save_name</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">learn</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="n">learn</span><span class="o">.</span><span class="n">path</span><span class="o">/</span><span class="n">learn</span><span class="o">.</span><span class="n">model_dir</span><span class="o">/</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">save_name</span><span class="si">}</span><span class="s1">_encoder.pth&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">lr</span><span class="o">=</span><span class="mf">5e-5</span>
<span class="n">wd</span><span class="o">=</span><span class="mf">1e-2</span>
<span class="n">epochs</span><span class="o">=</span><span class="mi">30</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learn</span><span class="o">.</span><span class="n">unfreeze</span><span class="p">()</span>
<span class="n">learn</span><span class="o">.</span><span class="n">fit_flat_cos</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">wd</span><span class="o">=</span><span class="n">wd</span><span class="p">,</span> <span class="n">pct_start</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">save_name</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;byol_iwang_sz</span><span class="si">{</span><span class="n">size</span><span class="si">}</span><span class="s1">_epc260&#39;</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learn</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">save_name</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">learn</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="n">learn</span><span class="o">.</span><span class="n">path</span><span class="o">/</span><span class="n">learn</span><span class="o">.</span><span class="n">model_dir</span><span class="o">/</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">save_name</span><span class="si">}</span><span class="s1">_encoder.pth&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learn</span><span class="o">.</span><span class="n">recorder</span><span class="o">.</span><span class="n">plot_loss</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">save_name</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>&#39;byol_iwang_sz256_epc230&#39;</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="3.-Downstream-Task---Image-Classification">3. Downstream Task - Image Classification<a class="anchor-link" href="#3.-Downstream-Task---Image-Classification"> </a></h2>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">get_dls</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">bs</span><span class="p">,</span> <span class="n">workers</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">path</span> <span class="o">=</span> <span class="n">URLs</span><span class="o">.</span><span class="n">IMAGEWANG_160</span> <span class="k">if</span> <span class="n">size</span> <span class="o">&lt;=</span> <span class="mi">160</span> <span class="k">else</span> <span class="n">URLs</span><span class="o">.</span><span class="n">IMAGEWANG</span>
    <span class="n">source</span> <span class="o">=</span> <span class="n">untar_data</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>
    <span class="n">files</span> <span class="o">=</span> <span class="n">get_image_files</span><span class="p">(</span><span class="n">source</span><span class="p">,</span> <span class="n">folders</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;train&#39;</span><span class="p">,</span> <span class="s1">&#39;val&#39;</span><span class="p">])</span>
    <span class="n">splits</span> <span class="o">=</span> <span class="n">GrandparentSplitter</span><span class="p">(</span><span class="n">valid_name</span><span class="o">=</span><span class="s1">&#39;val&#39;</span><span class="p">)(</span><span class="n">files</span><span class="p">)</span>
    
    <span class="n">item_aug</span> <span class="o">=</span> <span class="p">[</span><span class="n">RandomResizedCrop</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">min_scale</span><span class="o">=</span><span class="mf">0.35</span><span class="p">),</span> <span class="n">FlipItem</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)]</span>
    <span class="n">tfms</span> <span class="o">=</span> <span class="p">[[</span><span class="n">PILImage</span><span class="o">.</span><span class="n">create</span><span class="p">,</span> <span class="n">ToTensor</span><span class="p">,</span> <span class="o">*</span><span class="n">item_aug</span><span class="p">],</span> 
            <span class="p">[</span><span class="n">parent_label</span><span class="p">,</span> <span class="n">Categorize</span><span class="p">()]]</span>
    
    <span class="n">dsets</span> <span class="o">=</span> <span class="n">Datasets</span><span class="p">(</span><span class="n">files</span><span class="p">,</span> <span class="n">tfms</span><span class="o">=</span><span class="n">tfms</span><span class="p">,</span> <span class="n">splits</span><span class="o">=</span><span class="n">splits</span><span class="p">)</span>
    
    <span class="n">batch_tfms</span> <span class="o">=</span> <span class="p">[</span><span class="n">IntToFloatTensor</span><span class="p">,</span> <span class="n">Normalize</span><span class="o">.</span><span class="n">from_stats</span><span class="p">(</span><span class="o">*</span><span class="n">imagenet_stats</span><span class="p">)]</span>
    <span class="n">dls</span> <span class="o">=</span> <span class="n">dsets</span><span class="o">.</span><span class="n">dataloaders</span><span class="p">(</span><span class="n">bs</span><span class="o">=</span><span class="n">bs</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="n">workers</span><span class="p">,</span> <span class="n">after_batch</span><span class="o">=</span><span class="n">batch_tfms</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">dls</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">do_train</span><span class="p">(</span><span class="n">epochs</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">runs</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>  <span class="n">lr</span><span class="o">=</span><span class="mf">2e-2</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">size</span><span class="p">,</span> <span class="n">bs</span><span class="o">=</span><span class="n">bs</span><span class="p">,</span> <span class="n">save_name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">dls</span> <span class="o">=</span> <span class="n">get_dls</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">bs</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">run</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">runs</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Run: </span><span class="si">{</span><span class="n">run</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
        <span class="n">learn</span> <span class="o">=</span> <span class="n">cnn_learner</span><span class="p">(</span><span class="n">dls</span><span class="p">,</span> <span class="n">xresnet34</span><span class="p">,</span> <span class="n">opt_func</span><span class="o">=</span><span class="n">opt_func</span><span class="p">,</span> <span class="n">normalize</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="n">accuracy</span><span class="p">,</span><span class="n">top_k_accuracy</span><span class="p">],</span> <span class="n">loss_func</span><span class="o">=</span><span class="n">LabelSmoothingCrossEntropy</span><span class="p">(),</span>
                <span class="n">pretrained</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="c1">#         learn.to_fp16()</span>
        
        <span class="k">if</span> <span class="n">save_name</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">state_dict</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">learn</span><span class="o">.</span><span class="n">path</span><span class="o">/</span><span class="n">learn</span><span class="o">.</span><span class="n">model_dir</span><span class="o">/</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">save_name</span><span class="si">}</span><span class="s1">_encoder.pth&#39;</span><span class="p">)</span>
            <span class="n">learn</span><span class="o">.</span><span class="n">model</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">state_dict</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Model loaded...&quot;</span><span class="p">)</span>
            
        <span class="n">learn</span><span class="o">.</span><span class="n">unfreeze</span><span class="p">()</span>
        <span class="n">learn</span><span class="o">.</span><span class="n">fit_flat_cos</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">wd</span><span class="o">=</span><span class="n">wd</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="ImageWang-Leaderboard">ImageWang Leaderboard<a class="anchor-link" href="#ImageWang-Leaderboard"> </a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>sz-256</strong></p>
<p><strong>Contrastive Learning</strong></p>
<ul>
<li>5 epochs:  67.70%</li>
<li>20 epochs: 70.03%</li>
<li>80 epochs: 70.71%</li>
<li>200 epochs: 71.78%</li>
</ul>
<p><strong>BYOL</strong></p>
<ul>
<li>5 epochs: 64.74%</li>
<li>20 epochs: <strong>71.01%</strong></li>
<li>80 epochs: <strong>72.58%</strong></li>
<li>200 epochs: <strong>72.13%</strong></li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="5-epochs">5 epochs<a class="anchor-link" href="#5-epochs"> </a></h3>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># link: https://github.com/KeremTurgutlu/self_supervised/blob/252269827da41b41091cf0db533b65c0d1312f85/nbs/byol_iwang_192.ipynb</span>
<span class="n">save_name</span> <span class="o">=</span> <span class="s1">&#39;byol_iwang_sz192_epc230&#39;</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">lr</span> <span class="o">=</span> <span class="mf">1e-2</span>
<span class="n">wd</span><span class="o">=</span><span class="mf">1e-2</span>
<span class="n">bs</span><span class="o">=</span><span class="mi">128</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">runs</span> <span class="o">=</span> <span class="mi">5</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">do_train</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">runs</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span> <span class="n">bs</span><span class="o">=</span><span class="n">bs</span><span class="p">,</span> <span class="n">save_name</span><span class="o">=</span><span class="n">save_name</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Run: 0
Model loaded...
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>accuracy</th>
      <th>top_k_accuracy</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>1.391003</td>
      <td>2.091779</td>
      <td>0.427335</td>
      <td>0.883431</td>
      <td>01:30</td>
    </tr>
    <tr>
      <td>1</td>
      <td>1.126729</td>
      <td>1.863697</td>
      <td>0.508272</td>
      <td>0.891066</td>
      <td>01:30</td>
    </tr>
    <tr>
      <td>2</td>
      <td>1.041537</td>
      <td>1.929433</td>
      <td>0.500636</td>
      <td>0.876050</td>
      <td>01:30</td>
    </tr>
    <tr>
      <td>3</td>
      <td>1.000860</td>
      <td>1.786491</td>
      <td>0.550267</td>
      <td>0.880631</td>
      <td>01:30</td>
    </tr>
    <tr>
      <td>4</td>
      <td>0.887278</td>
      <td>1.526213</td>
      <td>0.657165</td>
      <td>0.930008</td>
      <td>01:30</td>
    </tr>
  </tbody>
</table>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Run: 1
Model loaded...
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>accuracy</th>
      <th>top_k_accuracy</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>1.409500</td>
      <td>1.993958</td>
      <td>0.466531</td>
      <td>0.882413</td>
      <td>01:30</td>
    </tr>
    <tr>
      <td>1</td>
      <td>1.115309</td>
      <td>1.930350</td>
      <td>0.485365</td>
      <td>0.883431</td>
      <td>01:31</td>
    </tr>
    <tr>
      <td>2</td>
      <td>1.045545</td>
      <td>1.777895</td>
      <td>0.535251</td>
      <td>0.901756</td>
      <td>01:29</td>
    </tr>
    <tr>
      <td>3</td>
      <td>0.992903</td>
      <td>1.793896</td>
      <td>0.551285</td>
      <td>0.909901</td>
      <td>01:30</td>
    </tr>
    <tr>
      <td>4</td>
      <td>0.889791</td>
      <td>1.577747</td>
      <td>0.637312</td>
      <td>0.929244</td>
      <td>01:29</td>
    </tr>
  </tbody>
</table>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Run: 2
Model loaded...
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>accuracy</th>
      <th>top_k_accuracy</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>1.372096</td>
      <td>2.030257</td>
      <td>0.441843</td>
      <td>0.861797</td>
      <td>01:30</td>
    </tr>
    <tr>
      <td>1</td>
      <td>1.124751</td>
      <td>1.903099</td>
      <td>0.495546</td>
      <td>0.889539</td>
      <td>01:29</td>
    </tr>
    <tr>
      <td>2</td>
      <td>1.052113</td>
      <td>1.827768</td>
      <td>0.517689</td>
      <td>0.904810</td>
      <td>01:30</td>
    </tr>
    <tr>
      <td>3</td>
      <td>1.004309</td>
      <td>1.782487</td>
      <td>0.548486</td>
      <td>0.900484</td>
      <td>01:29</td>
    </tr>
    <tr>
      <td>4</td>
      <td>0.911459</td>
      <td>1.576011</td>
      <td>0.631967</td>
      <td>0.933316</td>
      <td>01:30</td>
    </tr>
  </tbody>
</table>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Run: 3
Model loaded...
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>accuracy</th>
      <th>top_k_accuracy</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>1.388013</td>
      <td>1.947444</td>
      <td>0.468567</td>
      <td>0.876813</td>
      <td>01:30</td>
    </tr>
    <tr>
      <td>1</td>
      <td>1.138622</td>
      <td>1.827905</td>
      <td>0.532451</td>
      <td>0.903283</td>
      <td>01:30</td>
    </tr>
    <tr>
      <td>2</td>
      <td>1.078687</td>
      <td>1.856698</td>
      <td>0.525070</td>
      <td>0.876813</td>
      <td>01:30</td>
    </tr>
    <tr>
      <td>3</td>
      <td>1.033799</td>
      <td>1.874331</td>
      <td>0.509544</td>
      <td>0.890303</td>
      <td>01:30</td>
    </tr>
    <tr>
      <td>4</td>
      <td>0.911943</td>
      <td>1.542056</td>
      <td>0.646729</td>
      <td>0.932553</td>
      <td>01:29</td>
    </tr>
  </tbody>
</table>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Run: 4
Model loaded...
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>accuracy</th>
      <th>top_k_accuracy</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>1.365908</td>
      <td>1.891052</td>
      <td>0.499109</td>
      <td>0.886485</td>
      <td>01:29</td>
    </tr>
    <tr>
      <td>1</td>
      <td>1.130437</td>
      <td>1.819628</td>
      <td>0.524306</td>
      <td>0.884449</td>
      <td>01:29</td>
    </tr>
    <tr>
      <td>2</td>
      <td>1.058648</td>
      <td>1.709278</td>
      <td>0.576228</td>
      <td>0.911937</td>
      <td>01:30</td>
    </tr>
    <tr>
      <td>3</td>
      <td>1.025294</td>
      <td>1.846014</td>
      <td>0.525834</td>
      <td>0.907610</td>
      <td>01:30</td>
    </tr>
    <tr>
      <td>4</td>
      <td>0.897821</td>
      <td>1.503069</td>
      <td>0.664291</td>
      <td>0.932807</td>
      <td>01:29</td>
    </tr>
  </tbody>
</table>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">([</span><span class="mf">0.657165</span><span class="p">,</span><span class="mf">0.637312</span><span class="p">,</span><span class="mf">0.631967</span><span class="p">,</span><span class="mf">0.646729</span><span class="p">,</span><span class="mf">0.664291</span><span class="p">])</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>0.6474928</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="20-epochs">20 epochs<a class="anchor-link" href="#20-epochs"> </a></h3>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">lr</span><span class="o">=</span><span class="mf">2e-2</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">runs</span> <span class="o">=</span> <span class="mi">3</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">do_train</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">runs</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span> <span class="n">save_name</span><span class="o">=</span><span class="n">save_name</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Run: 0
Model loaded...
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>accuracy</th>
      <th>top_k_accuracy</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>1.394857</td>
      <td>2.268882</td>
      <td>0.362179</td>
      <td>0.816238</td>
      <td>01:30</td>
    </tr>
    <tr>
      <td>1</td>
      <td>1.250595</td>
      <td>2.007836</td>
      <td>0.441079</td>
      <td>0.847544</td>
      <td>01:30</td>
    </tr>
    <tr>
      <td>2</td>
      <td>1.197666</td>
      <td>1.981842</td>
      <td>0.435225</td>
      <td>0.890557</td>
      <td>01:30</td>
    </tr>
    <tr>
      <td>3</td>
      <td>1.178281</td>
      <td>2.283340</td>
      <td>0.363960</td>
      <td>0.809112</td>
      <td>01:30</td>
    </tr>
    <tr>
      <td>4</td>
      <td>1.079914</td>
      <td>2.048681</td>
      <td>0.447697</td>
      <td>0.842708</td>
      <td>01:30</td>
    </tr>
    <tr>
      <td>5</td>
      <td>1.029400</td>
      <td>1.822048</td>
      <td>0.520234</td>
      <td>0.880122</td>
      <td>01:30</td>
    </tr>
    <tr>
      <td>6</td>
      <td>0.999156</td>
      <td>1.931383</td>
      <td>0.490965</td>
      <td>0.862815</td>
      <td>01:30</td>
    </tr>
    <tr>
      <td>7</td>
      <td>0.950720</td>
      <td>1.722235</td>
      <td>0.564266</td>
      <td>0.912700</td>
      <td>01:31</td>
    </tr>
    <tr>
      <td>8</td>
      <td>0.925281</td>
      <td>1.785434</td>
      <td>0.554849</td>
      <td>0.913973</td>
      <td>01:30</td>
    </tr>
    <tr>
      <td>9</td>
      <td>0.910516</td>
      <td>1.705481</td>
      <td>0.588954</td>
      <td>0.892339</td>
      <td>01:29</td>
    </tr>
    <tr>
      <td>10</td>
      <td>0.894766</td>
      <td>1.696346</td>
      <td>0.594553</td>
      <td>0.908883</td>
      <td>01:30</td>
    </tr>
    <tr>
      <td>11</td>
      <td>0.874665</td>
      <td>1.790053</td>
      <td>0.561466</td>
      <td>0.897429</td>
      <td>01:29</td>
    </tr>
    <tr>
      <td>12</td>
      <td>0.872798</td>
      <td>1.684260</td>
      <td>0.584882</td>
      <td>0.913719</td>
      <td>01:30</td>
    </tr>
    <tr>
      <td>13</td>
      <td>0.855163</td>
      <td>1.714244</td>
      <td>0.588190</td>
      <td>0.903283</td>
      <td>01:30</td>
    </tr>
    <tr>
      <td>14</td>
      <td>0.844536</td>
      <td>1.686695</td>
      <td>0.604225</td>
      <td>0.911428</td>
      <td>01:30</td>
    </tr>
    <tr>
      <td>15</td>
      <td>0.824601</td>
      <td>1.650516</td>
      <td>0.627641</td>
      <td>0.908119</td>
      <td>01:30</td>
    </tr>
    <tr>
      <td>16</td>
      <td>0.790868</td>
      <td>1.495154</td>
      <td>0.675744</td>
      <td>0.938661</td>
      <td>01:29</td>
    </tr>
    <tr>
      <td>17</td>
      <td>0.752194</td>
      <td>1.512185</td>
      <td>0.674472</td>
      <td>0.930008</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>18</td>
      <td>0.719591</td>
      <td>1.437772</td>
      <td>0.697124</td>
      <td>0.933571</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>19</td>
      <td>0.696321</td>
      <td>1.391006</td>
      <td>0.711631</td>
      <td>0.939170</td>
      <td>01:28</td>
    </tr>
  </tbody>
</table>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Run: 1
Model loaded...
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>accuracy</th>
      <th>top_k_accuracy</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>1.396147</td>
      <td>2.352397</td>
      <td>0.308984</td>
      <td>0.789259</td>
      <td>01:29</td>
    </tr>
    <tr>
      <td>1</td>
      <td>1.211235</td>
      <td>2.210926</td>
      <td>0.381777</td>
      <td>0.829728</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>2</td>
      <td>1.163799</td>
      <td>2.104971</td>
      <td>0.415118</td>
      <td>0.831255</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>3</td>
      <td>1.131004</td>
      <td>2.178773</td>
      <td>0.407228</td>
      <td>0.817256</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>4</td>
      <td>1.059999</td>
      <td>2.143348</td>
      <td>0.409264</td>
      <td>0.857725</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>5</td>
      <td>1.026634</td>
      <td>2.056903</td>
      <td>0.440825</td>
      <td>0.832018</td>
      <td>01:27</td>
    </tr>
    <tr>
      <td>6</td>
      <td>0.973074</td>
      <td>1.927981</td>
      <td>0.502927</td>
      <td>0.886740</td>
      <td>01:27</td>
    </tr>
    <tr>
      <td>7</td>
      <td>0.949476</td>
      <td>1.829196</td>
      <td>0.532451</td>
      <td>0.892848</td>
      <td>01:27</td>
    </tr>
    <tr>
      <td>8</td>
      <td>0.919149</td>
      <td>1.717618</td>
      <td>0.580300</td>
      <td>0.902774</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>9</td>
      <td>0.909597</td>
      <td>1.691422</td>
      <td>0.576483</td>
      <td>0.911428</td>
      <td>01:27</td>
    </tr>
    <tr>
      <td>10</td>
      <td>0.891183</td>
      <td>1.672958</td>
      <td>0.597862</td>
      <td>0.906337</td>
      <td>01:27</td>
    </tr>
    <tr>
      <td>11</td>
      <td>0.870522</td>
      <td>1.698570</td>
      <td>0.597353</td>
      <td>0.905319</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>12</td>
      <td>0.866029</td>
      <td>1.599372</td>
      <td>0.626623</td>
      <td>0.916773</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>13</td>
      <td>0.848924</td>
      <td>1.629906</td>
      <td>0.617969</td>
      <td>0.920081</td>
      <td>01:27</td>
    </tr>
    <tr>
      <td>14</td>
      <td>0.843436</td>
      <td>1.715721</td>
      <td>0.598117</td>
      <td>0.909392</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>15</td>
      <td>0.820289</td>
      <td>1.866688</td>
      <td>0.538050</td>
      <td>0.889794</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>16</td>
      <td>0.786086</td>
      <td>1.612719</td>
      <td>0.634767</td>
      <td>0.911173</td>
      <td>01:27</td>
    </tr>
    <tr>
      <td>17</td>
      <td>0.744055</td>
      <td>1.544407</td>
      <td>0.665309</td>
      <td>0.928226</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>18</td>
      <td>0.712789</td>
      <td>1.419991</td>
      <td>0.703741</td>
      <td>0.938916</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>19</td>
      <td>0.694215</td>
      <td>1.421353</td>
      <td>0.705269</td>
      <td>0.936116</td>
      <td>01:28</td>
    </tr>
  </tbody>
</table>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Run: 2
Model loaded...
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>accuracy</th>
      <th>top_k_accuracy</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>1.414950</td>
      <td>2.177913</td>
      <td>0.367524</td>
      <td>0.831764</td>
      <td>01:27</td>
    </tr>
    <tr>
      <td>1</td>
      <td>1.215868</td>
      <td>2.084969</td>
      <td>0.413337</td>
      <td>0.856452</td>
      <td>01:27</td>
    </tr>
    <tr>
      <td>2</td>
      <td>1.160794</td>
      <td>2.215767</td>
      <td>0.394502</td>
      <td>0.798422</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>3</td>
      <td>1.103339</td>
      <td>2.063161</td>
      <td>0.432426</td>
      <td>0.850089</td>
      <td>01:27</td>
    </tr>
    <tr>
      <td>4</td>
      <td>1.034577</td>
      <td>1.835640</td>
      <td>0.511835</td>
      <td>0.903538</td>
      <td>01:27</td>
    </tr>
    <tr>
      <td>5</td>
      <td>1.000270</td>
      <td>1.676985</td>
      <td>0.575464</td>
      <td>0.931789</td>
      <td>01:27</td>
    </tr>
    <tr>
      <td>6</td>
      <td>0.959487</td>
      <td>1.691361</td>
      <td>0.570629</td>
      <td>0.908628</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>7</td>
      <td>0.943315</td>
      <td>1.799891</td>
      <td>0.546195</td>
      <td>0.906847</td>
      <td>01:27</td>
    </tr>
    <tr>
      <td>8</td>
      <td>0.925966</td>
      <td>1.657880</td>
      <td>0.597353</td>
      <td>0.914228</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>9</td>
      <td>0.895879</td>
      <td>1.921196</td>
      <td>0.512344</td>
      <td>0.883685</td>
      <td>01:27</td>
    </tr>
    <tr>
      <td>10</td>
      <td>0.882046</td>
      <td>1.727123</td>
      <td>0.578773</td>
      <td>0.911937</td>
      <td>01:29</td>
    </tr>
    <tr>
      <td>11</td>
      <td>0.868748</td>
      <td>1.714249</td>
      <td>0.595062</td>
      <td>0.900229</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>12</td>
      <td>0.853746</td>
      <td>1.737140</td>
      <td>0.590736</td>
      <td>0.890303</td>
      <td>01:27</td>
    </tr>
    <tr>
      <td>13</td>
      <td>0.849947</td>
      <td>1.715284</td>
      <td>0.604479</td>
      <td>0.887503</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>14</td>
      <td>0.833915</td>
      <td>1.675274</td>
      <td>0.607534</td>
      <td>0.895902</td>
      <td>01:29</td>
    </tr>
    <tr>
      <td>15</td>
      <td>0.822880</td>
      <td>1.630758</td>
      <td>0.631458</td>
      <td>0.918300</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>16</td>
      <td>0.785896</td>
      <td>1.569622</td>
      <td>0.643166</td>
      <td>0.919318</td>
      <td>01:27</td>
    </tr>
    <tr>
      <td>17</td>
      <td>0.742758</td>
      <td>1.464388</td>
      <td>0.687452</td>
      <td>0.931535</td>
      <td>01:27</td>
    </tr>
    <tr>
      <td>18</td>
      <td>0.708674</td>
      <td>1.394059</td>
      <td>0.712395</td>
      <td>0.941970</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>19</td>
      <td>0.691944</td>
      <td>1.389416</td>
      <td>0.713413</td>
      <td>0.940443</td>
      <td>01:28</td>
    </tr>
  </tbody>
</table>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">([</span><span class="mf">0.711631</span><span class="p">,</span> <span class="mf">0.705269</span><span class="p">,</span> <span class="mf">0.713413</span><span class="p">])</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>0.7101043333333333</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="80-epochs">80 epochs<a class="anchor-link" href="#80-epochs"> </a></h3>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">epochs</span> <span class="o">=</span> <span class="mi">80</span>
<span class="n">runs</span> <span class="o">=</span> <span class="mi">1</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">do_train</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">runs</span><span class="p">,</span> <span class="n">save_name</span><span class="o">=</span><span class="n">save_name</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Run: 0
Model loaded...
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>accuracy</th>
      <th>top_k_accuracy</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>1.432139</td>
      <td>2.193268</td>
      <td>0.389667</td>
      <td>0.822347</td>
      <td>01:27</td>
    </tr>
    <tr>
      <td>1</td>
      <td>1.197331</td>
      <td>2.040368</td>
      <td>0.457877</td>
      <td>0.869432</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>2</td>
      <td>1.140320</td>
      <td>2.065905</td>
      <td>0.434971</td>
      <td>0.833291</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>3</td>
      <td>1.079537</td>
      <td>2.285829</td>
      <td>0.366760</td>
      <td>0.780860</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>4</td>
      <td>1.028553</td>
      <td>1.889869</td>
      <td>0.505218</td>
      <td>0.874523</td>
      <td>01:27</td>
    </tr>
    <tr>
      <td>5</td>
      <td>0.982782</td>
      <td>1.932606</td>
      <td>0.504709</td>
      <td>0.874268</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>6</td>
      <td>0.954037</td>
      <td>1.926916</td>
      <td>0.494273</td>
      <td>0.869941</td>
      <td>01:27</td>
    </tr>
    <tr>
      <td>7</td>
      <td>0.924774</td>
      <td>1.748468</td>
      <td>0.578010</td>
      <td>0.905065</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>8</td>
      <td>0.913467</td>
      <td>1.764108</td>
      <td>0.563502</td>
      <td>0.919318</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>9</td>
      <td>0.899805</td>
      <td>1.687788</td>
      <td>0.593281</td>
      <td>0.915246</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>10</td>
      <td>0.879273</td>
      <td>1.647418</td>
      <td>0.603970</td>
      <td>0.920845</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>11</td>
      <td>0.868013</td>
      <td>1.798377</td>
      <td>0.564011</td>
      <td>0.885467</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>12</td>
      <td>0.848206</td>
      <td>1.693629</td>
      <td>0.605243</td>
      <td>0.910155</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>13</td>
      <td>0.837403</td>
      <td>1.593628</td>
      <td>0.628659</td>
      <td>0.917282</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>14</td>
      <td>0.842437</td>
      <td>1.722781</td>
      <td>0.592517</td>
      <td>0.899466</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>15</td>
      <td>0.824219</td>
      <td>1.630168</td>
      <td>0.627132</td>
      <td>0.904810</td>
      <td>01:27</td>
    </tr>
    <tr>
      <td>16</td>
      <td>0.816686</td>
      <td>1.479528</td>
      <td>0.665564</td>
      <td>0.941970</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>17</td>
      <td>0.801881</td>
      <td>1.632363</td>
      <td>0.635022</td>
      <td>0.912191</td>
      <td>01:27</td>
    </tr>
    <tr>
      <td>18</td>
      <td>0.806963</td>
      <td>1.873198</td>
      <td>0.556885</td>
      <td>0.874014</td>
      <td>01:27</td>
    </tr>
    <tr>
      <td>19</td>
      <td>0.792268</td>
      <td>1.533428</td>
      <td>0.663019</td>
      <td>0.927462</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>20</td>
      <td>0.778066</td>
      <td>1.608427</td>
      <td>0.650038</td>
      <td>0.915500</td>
      <td>01:27</td>
    </tr>
    <tr>
      <td>21</td>
      <td>0.783447</td>
      <td>1.647912</td>
      <td>0.625095</td>
      <td>0.909646</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>22</td>
      <td>0.773404</td>
      <td>1.672443</td>
      <td>0.632476</td>
      <td>0.893103</td>
      <td>01:27</td>
    </tr>
    <tr>
      <td>23</td>
      <td>0.770990</td>
      <td>1.700409</td>
      <td>0.616442</td>
      <td>0.892084</td>
      <td>01:27</td>
    </tr>
    <tr>
      <td>24</td>
      <td>0.758653</td>
      <td>1.632939</td>
      <td>0.630186</td>
      <td>0.919827</td>
      <td>01:27</td>
    </tr>
    <tr>
      <td>25</td>
      <td>0.762572</td>
      <td>1.489764</td>
      <td>0.680580</td>
      <td>0.925935</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>26</td>
      <td>0.754279</td>
      <td>1.714878</td>
      <td>0.617460</td>
      <td>0.901247</td>
      <td>01:27</td>
    </tr>
    <tr>
      <td>27</td>
      <td>0.756023</td>
      <td>1.577708</td>
      <td>0.640876</td>
      <td>0.936880</td>
      <td>01:27</td>
    </tr>
    <tr>
      <td>28</td>
      <td>0.750489</td>
      <td>1.698645</td>
      <td>0.620514</td>
      <td>0.891830</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>29</td>
      <td>0.741153</td>
      <td>1.625965</td>
      <td>0.653347</td>
      <td>0.905828</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>30</td>
      <td>0.739615</td>
      <td>1.515328</td>
      <td>0.676763</td>
      <td>0.932298</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>31</td>
      <td>0.738460</td>
      <td>1.655755</td>
      <td>0.636549</td>
      <td>0.913719</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>32</td>
      <td>0.731166</td>
      <td>1.600546</td>
      <td>0.664546</td>
      <td>0.917791</td>
      <td>01:29</td>
    </tr>
    <tr>
      <td>33</td>
      <td>0.732359</td>
      <td>1.567721</td>
      <td>0.668109</td>
      <td>0.920081</td>
      <td>01:29</td>
    </tr>
    <tr>
      <td>34</td>
      <td>0.726722</td>
      <td>1.518010</td>
      <td>0.679308</td>
      <td>0.923390</td>
      <td>01:29</td>
    </tr>
    <tr>
      <td>35</td>
      <td>0.729638</td>
      <td>1.682911</td>
      <td>0.632476</td>
      <td>0.904047</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>36</td>
      <td>0.723345</td>
      <td>1.543455</td>
      <td>0.667091</td>
      <td>0.926953</td>
      <td>01:29</td>
    </tr>
    <tr>
      <td>37</td>
      <td>0.717692</td>
      <td>1.589628</td>
      <td>0.670654</td>
      <td>0.905319</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>38</td>
      <td>0.718837</td>
      <td>1.565381</td>
      <td>0.672945</td>
      <td>0.913719</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>39</td>
      <td>0.717006</td>
      <td>1.672232</td>
      <td>0.638076</td>
      <td>0.907356</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>40</td>
      <td>0.715885</td>
      <td>1.545427</td>
      <td>0.676508</td>
      <td>0.921863</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>41</td>
      <td>0.721414</td>
      <td>1.572315</td>
      <td>0.661491</td>
      <td>0.923899</td>
      <td>01:29</td>
    </tr>
    <tr>
      <td>42</td>
      <td>0.707397</td>
      <td>1.558331</td>
      <td>0.668109</td>
      <td>0.923899</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>43</td>
      <td>0.708705</td>
      <td>1.531734</td>
      <td>0.669381</td>
      <td>0.909901</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>44</td>
      <td>0.701443</td>
      <td>1.526106</td>
      <td>0.676508</td>
      <td>0.920590</td>
      <td>01:29</td>
    </tr>
    <tr>
      <td>45</td>
      <td>0.700447</td>
      <td>1.748029</td>
      <td>0.626623</td>
      <td>0.897175</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>46</td>
      <td>0.701972</td>
      <td>1.712000</td>
      <td>0.613388</td>
      <td>0.905319</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>47</td>
      <td>0.706223</td>
      <td>1.484633</td>
      <td>0.692034</td>
      <td>0.927462</td>
      <td>01:27</td>
    </tr>
    <tr>
      <td>48</td>
      <td>0.695572</td>
      <td>1.651447</td>
      <td>0.650293</td>
      <td>0.908628</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>49</td>
      <td>0.694339</td>
      <td>1.591578</td>
      <td>0.669381</td>
      <td>0.917791</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>50</td>
      <td>0.693374</td>
      <td>1.609692</td>
      <td>0.652838</td>
      <td>0.903792</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>51</td>
      <td>0.686288</td>
      <td>1.662011</td>
      <td>0.646220</td>
      <td>0.899466</td>
      <td>01:27</td>
    </tr>
    <tr>
      <td>52</td>
      <td>0.698129</td>
      <td>1.550228</td>
      <td>0.687961</td>
      <td>0.907101</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>53</td>
      <td>0.696243</td>
      <td>1.549233</td>
      <td>0.669891</td>
      <td>0.912191</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>54</td>
      <td>0.691393</td>
      <td>1.528988</td>
      <td>0.677781</td>
      <td>0.918554</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>55</td>
      <td>0.693175</td>
      <td>1.561037</td>
      <td>0.675490</td>
      <td>0.914991</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>56</td>
      <td>0.687709</td>
      <td>1.576256</td>
      <td>0.673454</td>
      <td>0.914482</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>57</td>
      <td>0.689715</td>
      <td>1.647571</td>
      <td>0.651565</td>
      <td>0.901502</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>58</td>
      <td>0.691966</td>
      <td>1.532402</td>
      <td>0.684653</td>
      <td>0.919572</td>
      <td>01:29</td>
    </tr>
    <tr>
      <td>59</td>
      <td>0.683949</td>
      <td>1.611360</td>
      <td>0.654110</td>
      <td>0.910919</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>60</td>
      <td>0.684631</td>
      <td>1.537633</td>
      <td>0.678544</td>
      <td>0.916773</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>61</td>
      <td>0.680817</td>
      <td>1.583998</td>
      <td>0.666327</td>
      <td>0.921100</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>62</td>
      <td>0.679585</td>
      <td>1.502602</td>
      <td>0.693052</td>
      <td>0.925172</td>
      <td>01:29</td>
    </tr>
    <tr>
      <td>63</td>
      <td>0.676107</td>
      <td>1.485575</td>
      <td>0.699415</td>
      <td>0.927971</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>64</td>
      <td>0.671204</td>
      <td>1.551963</td>
      <td>0.673454</td>
      <td>0.915246</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>65</td>
      <td>0.667676</td>
      <td>1.530650</td>
      <td>0.682362</td>
      <td>0.917791</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>66</td>
      <td>0.659556</td>
      <td>1.509322</td>
      <td>0.692288</td>
      <td>0.921100</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>67</td>
      <td>0.648171</td>
      <td>1.563007</td>
      <td>0.672945</td>
      <td>0.918554</td>
      <td>01:29</td>
    </tr>
    <tr>
      <td>68</td>
      <td>0.647005</td>
      <td>1.537795</td>
      <td>0.683380</td>
      <td>0.918809</td>
      <td>01:27</td>
    </tr>
    <tr>
      <td>69</td>
      <td>0.641521</td>
      <td>1.554988</td>
      <td>0.684653</td>
      <td>0.921354</td>
      <td>01:29</td>
    </tr>
    <tr>
      <td>70</td>
      <td>0.636487</td>
      <td>1.556873</td>
      <td>0.676253</td>
      <td>0.912446</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>71</td>
      <td>0.630556</td>
      <td>1.505618</td>
      <td>0.694833</td>
      <td>0.917536</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>72</td>
      <td>0.626521</td>
      <td>1.419826</td>
      <td>0.716213</td>
      <td>0.927971</td>
      <td>01:29</td>
    </tr>
    <tr>
      <td>73</td>
      <td>0.624272</td>
      <td>1.458859</td>
      <td>0.706287</td>
      <td>0.926699</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>74</td>
      <td>0.623385</td>
      <td>1.399117</td>
      <td>0.729193</td>
      <td>0.940443</td>
      <td>01:29</td>
    </tr>
    <tr>
      <td>75</td>
      <td>0.621087</td>
      <td>1.437445</td>
      <td>0.713922</td>
      <td>0.931026</td>
      <td>01:27</td>
    </tr>
    <tr>
      <td>76</td>
      <td>0.618802</td>
      <td>1.420812</td>
      <td>0.719012</td>
      <td>0.933062</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>77</td>
      <td>0.618094</td>
      <td>1.387226</td>
      <td>0.728430</td>
      <td>0.934080</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>78</td>
      <td>0.620908</td>
      <td>1.389570</td>
      <td>0.728684</td>
      <td>0.935862</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>79</td>
      <td>0.617747</td>
      <td>1.404366</td>
      <td>0.725884</td>
      <td>0.932553</td>
      <td>01:28</td>
    </tr>
  </tbody>
</table>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="200-epochs">200 epochs<a class="anchor-link" href="#200-epochs"> </a></h3>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">epochs</span> <span class="o">=</span> <span class="mi">200</span>
<span class="n">runs</span> <span class="o">=</span> <span class="mi">1</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">do_train</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">runs</span><span class="p">,</span> <span class="n">save_name</span><span class="o">=</span><span class="n">save_name</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Run: 0
Model loaded...
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>accuracy</th>
      <th>top_k_accuracy</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>1.419125</td>
      <td>2.284880</td>
      <td>0.369814</td>
      <td>0.811911</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>1</td>
      <td>1.211017</td>
      <td>2.350174</td>
      <td>0.315856</td>
      <td>0.819038</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>2</td>
      <td>1.153036</td>
      <td>1.992249</td>
      <td>0.446424</td>
      <td>0.873759</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>3</td>
      <td>1.085376</td>
      <td>1.840743</td>
      <td>0.503181</td>
      <td>0.895393</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>4</td>
      <td>1.022951</td>
      <td>2.173820</td>
      <td>0.395266</td>
      <td>0.836345</td>
      <td>01:27</td>
    </tr>
    <tr>
      <td>5</td>
      <td>0.989262</td>
      <td>1.895496</td>
      <td>0.511581</td>
      <td>0.879868</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>6</td>
      <td>0.967016</td>
      <td>1.675152</td>
      <td>0.583864</td>
      <td>0.913719</td>
      <td>01:27</td>
    </tr>
    <tr>
      <td>7</td>
      <td>0.946929</td>
      <td>1.838096</td>
      <td>0.516162</td>
      <td>0.907356</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>8</td>
      <td>0.922163</td>
      <td>1.730152</td>
      <td>0.566556</td>
      <td>0.919318</td>
      <td>01:29</td>
    </tr>
    <tr>
      <td>9</td>
      <td>0.903864</td>
      <td>1.832088</td>
      <td>0.549249</td>
      <td>0.873505</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>10</td>
      <td>0.882698</td>
      <td>1.640119</td>
      <td>0.605498</td>
      <td>0.915246</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>11</td>
      <td>0.868896</td>
      <td>1.713113</td>
      <td>0.584627</td>
      <td>0.900484</td>
      <td>01:29</td>
    </tr>
    <tr>
      <td>12</td>
      <td>0.858259</td>
      <td>1.779322</td>
      <td>0.562484</td>
      <td>0.904556</td>
      <td>01:27</td>
    </tr>
    <tr>
      <td>13</td>
      <td>0.855642</td>
      <td>1.623850</td>
      <td>0.609315</td>
      <td>0.920590</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>14</td>
      <td>0.830629</td>
      <td>1.643499</td>
      <td>0.621278</td>
      <td>0.912700</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>15</td>
      <td>0.825483</td>
      <td>1.679459</td>
      <td>0.605498</td>
      <td>0.912955</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>16</td>
      <td>0.816543</td>
      <td>1.601887</td>
      <td>0.629931</td>
      <td>0.932553</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>17</td>
      <td>0.812537</td>
      <td>1.760885</td>
      <td>0.588699</td>
      <td>0.900738</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>18</td>
      <td>0.801353</td>
      <td>1.869138</td>
      <td>0.559175</td>
      <td>0.890557</td>
      <td>01:27</td>
    </tr>
    <tr>
      <td>19</td>
      <td>0.788338</td>
      <td>1.595085</td>
      <td>0.647238</td>
      <td>0.916518</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>20</td>
      <td>0.793619</td>
      <td>1.777891</td>
      <td>0.593281</td>
      <td>0.887758</td>
      <td>01:27</td>
    </tr>
    <tr>
      <td>21</td>
      <td>0.784781</td>
      <td>1.684735</td>
      <td>0.610333</td>
      <td>0.906083</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>22</td>
      <td>0.780073</td>
      <td>1.547550</td>
      <td>0.667854</td>
      <td>0.923136</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>23</td>
      <td>0.774738</td>
      <td>1.553903</td>
      <td>0.649529</td>
      <td>0.914228</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>24</td>
      <td>0.758743</td>
      <td>1.503605</td>
      <td>0.677272</td>
      <td>0.937389</td>
      <td>01:27</td>
    </tr>
    <tr>
      <td>25</td>
      <td>0.756246</td>
      <td>1.573706</td>
      <td>0.652329</td>
      <td>0.923899</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>26</td>
      <td>0.748144</td>
      <td>1.675284</td>
      <td>0.618987</td>
      <td>0.913209</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>27</td>
      <td>0.753061</td>
      <td>1.650802</td>
      <td>0.636040</td>
      <td>0.905574</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>28</td>
      <td>0.745643</td>
      <td>1.651385</td>
      <td>0.631713</td>
      <td>0.912191</td>
      <td>01:27</td>
    </tr>
    <tr>
      <td>29</td>
      <td>0.743422</td>
      <td>1.667220</td>
      <td>0.630440</td>
      <td>0.896920</td>
      <td>01:29</td>
    </tr>
    <tr>
      <td>30</td>
      <td>0.741664</td>
      <td>1.686204</td>
      <td>0.621787</td>
      <td>0.902265</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>31</td>
      <td>0.743482</td>
      <td>1.559853</td>
      <td>0.664800</td>
      <td>0.918045</td>
      <td>01:27</td>
    </tr>
    <tr>
      <td>32</td>
      <td>0.737715</td>
      <td>1.604192</td>
      <td>0.651311</td>
      <td>0.914482</td>
      <td>01:27</td>
    </tr>
    <tr>
      <td>33</td>
      <td>0.734155</td>
      <td>1.636156</td>
      <td>0.658692</td>
      <td>0.912191</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>34</td>
      <td>0.728535</td>
      <td>1.754433</td>
      <td>0.597353</td>
      <td>0.890812</td>
      <td>01:27</td>
    </tr>
    <tr>
      <td>35</td>
      <td>0.726517</td>
      <td>1.609321</td>
      <td>0.655638</td>
      <td>0.916518</td>
      <td>01:29</td>
    </tr>
    <tr>
      <td>36</td>
      <td>0.726188</td>
      <td>1.690681</td>
      <td>0.643675</td>
      <td>0.897175</td>
      <td>01:27</td>
    </tr>
    <tr>
      <td>37</td>
      <td>0.719141</td>
      <td>1.569075</td>
      <td>0.667091</td>
      <td>0.911682</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>38</td>
      <td>0.717585</td>
      <td>1.559077</td>
      <td>0.665818</td>
      <td>0.922118</td>
      <td>01:27</td>
    </tr>
    <tr>
      <td>39</td>
      <td>0.715208</td>
      <td>1.639970</td>
      <td>0.651056</td>
      <td>0.904810</td>
      <td>01:27</td>
    </tr>
    <tr>
      <td>40</td>
      <td>0.718166</td>
      <td>1.672077</td>
      <td>0.634004</td>
      <td>0.903029</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>41</td>
      <td>0.704955</td>
      <td>1.587227</td>
      <td>0.667091</td>
      <td>0.905065</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>42</td>
      <td>0.717658</td>
      <td>1.595332</td>
      <td>0.660982</td>
      <td>0.905319</td>
      <td>01:27</td>
    </tr>
    <tr>
      <td>43</td>
      <td>0.712193</td>
      <td>1.623456</td>
      <td>0.654620</td>
      <td>0.910664</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>44</td>
      <td>0.707499</td>
      <td>1.594872</td>
      <td>0.649784</td>
      <td>0.917282</td>
      <td>01:27</td>
    </tr>
    <tr>
      <td>45</td>
      <td>0.707156</td>
      <td>1.670306</td>
      <td>0.640366</td>
      <td>0.910664</td>
      <td>01:27</td>
    </tr>
    <tr>
      <td>46</td>
      <td>0.696481</td>
      <td>1.625322</td>
      <td>0.650038</td>
      <td>0.909137</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>47</td>
      <td>0.702462</td>
      <td>1.709067</td>
      <td>0.629931</td>
      <td>0.905319</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>48</td>
      <td>0.704311</td>
      <td>1.756469</td>
      <td>0.616442</td>
      <td>0.870705</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>49</td>
      <td>0.693788</td>
      <td>1.531168</td>
      <td>0.669381</td>
      <td>0.917791</td>
      <td>01:29</td>
    </tr>
    <tr>
      <td>50</td>
      <td>0.698404</td>
      <td>1.660893</td>
      <td>0.642912</td>
      <td>0.899466</td>
      <td>01:29</td>
    </tr>
    <tr>
      <td>51</td>
      <td>0.687217</td>
      <td>1.633951</td>
      <td>0.645711</td>
      <td>0.905574</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>52</td>
      <td>0.695636</td>
      <td>1.615643</td>
      <td>0.668618</td>
      <td>0.912700</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>53</td>
      <td>0.690168</td>
      <td>1.669456</td>
      <td>0.651565</td>
      <td>0.901247</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>54</td>
      <td>0.688847</td>
      <td>1.614989</td>
      <td>0.659201</td>
      <td>0.895393</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>55</td>
      <td>0.692354</td>
      <td>1.811942</td>
      <td>0.589972</td>
      <td>0.869687</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>56</td>
      <td>0.700194</td>
      <td>1.650914</td>
      <td>0.647493</td>
      <td>0.916264</td>
      <td>01:29</td>
    </tr>
    <tr>
      <td>57</td>
      <td>0.686995</td>
      <td>1.603166</td>
      <td>0.672690</td>
      <td>0.911173</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>58</td>
      <td>0.684166</td>
      <td>1.596993</td>
      <td>0.671672</td>
      <td>0.908119</td>
      <td>01:29</td>
    </tr>
    <tr>
      <td>59</td>
      <td>0.685026</td>
      <td>1.600340</td>
      <td>0.673708</td>
      <td>0.899211</td>
      <td>01:29</td>
    </tr>
    <tr>
      <td>60</td>
      <td>0.685189</td>
      <td>1.704077</td>
      <td>0.636294</td>
      <td>0.893612</td>
      <td>01:29</td>
    </tr>
    <tr>
      <td>61</td>
      <td>0.686637</td>
      <td>1.635834</td>
      <td>0.652583</td>
      <td>0.905319</td>
      <td>01:29</td>
    </tr>
    <tr>
      <td>62</td>
      <td>0.685463</td>
      <td>1.584968</td>
      <td>0.673708</td>
      <td>0.908628</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>63</td>
      <td>0.685734</td>
      <td>1.650136</td>
      <td>0.651056</td>
      <td>0.903283</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>64</td>
      <td>0.684149</td>
      <td>1.586770</td>
      <td>0.664291</td>
      <td>0.915500</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>65</td>
      <td>0.693077</td>
      <td>1.626995</td>
      <td>0.646220</td>
      <td>0.899211</td>
      <td>01:27</td>
    </tr>
    <tr>
      <td>66</td>
      <td>0.684762</td>
      <td>1.762507</td>
      <td>0.615424</td>
      <td>0.879104</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>67</td>
      <td>0.675092</td>
      <td>1.628694</td>
      <td>0.656656</td>
      <td>0.903029</td>
      <td>01:27</td>
    </tr>
    <tr>
      <td>68</td>
      <td>0.682115</td>
      <td>1.610352</td>
      <td>0.656147</td>
      <td>0.906847</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>69</td>
      <td>0.675551</td>
      <td>1.613912</td>
      <td>0.658183</td>
      <td>0.914482</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>70</td>
      <td>0.683207</td>
      <td>1.593386</td>
      <td>0.671672</td>
      <td>0.916009</td>
      <td>01:27</td>
    </tr>
    <tr>
      <td>71</td>
      <td>0.678684</td>
      <td>1.668024</td>
      <td>0.638839</td>
      <td>0.906847</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>72</td>
      <td>0.673538</td>
      <td>1.631247</td>
      <td>0.652838</td>
      <td>0.904047</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>73</td>
      <td>0.676347</td>
      <td>1.536053</td>
      <td>0.689234</td>
      <td>0.922881</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>74</td>
      <td>0.678187</td>
      <td>1.605529</td>
      <td>0.666327</td>
      <td>0.903792</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>75</td>
      <td>0.676095</td>
      <td>1.606716</td>
      <td>0.668363</td>
      <td>0.909901</td>
      <td>01:27</td>
    </tr>
    <tr>
      <td>76</td>
      <td>0.678888</td>
      <td>1.674362</td>
      <td>0.638076</td>
      <td>0.895139</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>77</td>
      <td>0.682046</td>
      <td>1.665397</td>
      <td>0.646220</td>
      <td>0.892594</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>78</td>
      <td>0.679791</td>
      <td>1.670219</td>
      <td>0.652329</td>
      <td>0.894375</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>79</td>
      <td>0.679825</td>
      <td>1.551011</td>
      <td>0.678544</td>
      <td>0.912700</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>80</td>
      <td>0.668632</td>
      <td>1.690552</td>
      <td>0.643421</td>
      <td>0.886231</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>81</td>
      <td>0.665336</td>
      <td>1.532332</td>
      <td>0.678035</td>
      <td>0.918045</td>
      <td>01:29</td>
    </tr>
    <tr>
      <td>82</td>
      <td>0.678226</td>
      <td>1.730807</td>
      <td>0.628659</td>
      <td>0.885976</td>
      <td>01:27</td>
    </tr>
    <tr>
      <td>83</td>
      <td>0.679253</td>
      <td>1.603326</td>
      <td>0.665564</td>
      <td>0.902265</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>84</td>
      <td>0.679932</td>
      <td>1.607385</td>
      <td>0.662510</td>
      <td>0.908628</td>
      <td>01:27</td>
    </tr>
    <tr>
      <td>85</td>
      <td>0.676861</td>
      <td>1.679614</td>
      <td>0.642657</td>
      <td>0.898447</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>86</td>
      <td>0.672381</td>
      <td>1.608386</td>
      <td>0.666073</td>
      <td>0.898447</td>
      <td>01:29</td>
    </tr>
    <tr>
      <td>87</td>
      <td>0.676610</td>
      <td>1.591930</td>
      <td>0.667600</td>
      <td>0.903283</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>88</td>
      <td>0.676376</td>
      <td>1.564327</td>
      <td>0.688979</td>
      <td>0.914737</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>89</td>
      <td>0.669363</td>
      <td>1.624508</td>
      <td>0.660728</td>
      <td>0.894375</td>
      <td>01:27</td>
    </tr>
    <tr>
      <td>90</td>
      <td>0.673465</td>
      <td>1.629204</td>
      <td>0.655383</td>
      <td>0.914737</td>
      <td>01:27</td>
    </tr>
    <tr>
      <td>91</td>
      <td>0.673167</td>
      <td>1.517852</td>
      <td>0.688725</td>
      <td>0.922627</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>92</td>
      <td>0.665968</td>
      <td>1.674923</td>
      <td>0.638076</td>
      <td>0.892084</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>93</td>
      <td>0.670067</td>
      <td>1.611571</td>
      <td>0.663528</td>
      <td>0.907865</td>
      <td>01:27</td>
    </tr>
    <tr>
      <td>94</td>
      <td>0.663029</td>
      <td>1.571563</td>
      <td>0.671163</td>
      <td>0.916009</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>95</td>
      <td>0.676404</td>
      <td>1.665003</td>
      <td>0.651056</td>
      <td>0.911173</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>96</td>
      <td>0.675332</td>
      <td>1.712104</td>
      <td>0.643166</td>
      <td>0.884194</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>97</td>
      <td>0.670424</td>
      <td>1.661967</td>
      <td>0.646475</td>
      <td>0.902265</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>98</td>
      <td>0.664591</td>
      <td>1.681199</td>
      <td>0.642912</td>
      <td>0.886994</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>99</td>
      <td>0.664242</td>
      <td>1.673738</td>
      <td>0.644693</td>
      <td>0.899975</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>100</td>
      <td>0.671140</td>
      <td>1.659061</td>
      <td>0.658183</td>
      <td>0.887249</td>
      <td>01:27</td>
    </tr>
    <tr>
      <td>101</td>
      <td>0.666756</td>
      <td>1.546077</td>
      <td>0.680326</td>
      <td>0.918045</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>102</td>
      <td>0.665545</td>
      <td>1.544732</td>
      <td>0.682107</td>
      <td>0.919827</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>103</td>
      <td>0.666967</td>
      <td>1.698950</td>
      <td>0.642403</td>
      <td>0.898447</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>104</td>
      <td>0.666164</td>
      <td>1.523669</td>
      <td>0.689997</td>
      <td>0.917282</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>105</td>
      <td>0.665388</td>
      <td>1.611875</td>
      <td>0.665309</td>
      <td>0.898956</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>106</td>
      <td>0.666055</td>
      <td>1.581103</td>
      <td>0.664546</td>
      <td>0.912700</td>
      <td>01:27</td>
    </tr>
    <tr>
      <td>107</td>
      <td>0.667301</td>
      <td>1.558092</td>
      <td>0.677781</td>
      <td>0.916264</td>
      <td>01:29</td>
    </tr>
    <tr>
      <td>108</td>
      <td>0.665353</td>
      <td>1.468322</td>
      <td>0.706796</td>
      <td>0.928481</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>109</td>
      <td>0.662387</td>
      <td>1.664793</td>
      <td>0.651565</td>
      <td>0.886231</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>110</td>
      <td>0.672887</td>
      <td>1.653189</td>
      <td>0.659964</td>
      <td>0.899466</td>
      <td>01:27</td>
    </tr>
    <tr>
      <td>111</td>
      <td>0.665801</td>
      <td>1.588114</td>
      <td>0.671418</td>
      <td>0.916518</td>
      <td>01:27</td>
    </tr>
    <tr>
      <td>112</td>
      <td>0.663041</td>
      <td>1.553074</td>
      <td>0.687707</td>
      <td>0.915755</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>113</td>
      <td>0.663433</td>
      <td>1.643208</td>
      <td>0.656910</td>
      <td>0.896157</td>
      <td>01:27</td>
    </tr>
    <tr>
      <td>114</td>
      <td>0.666839</td>
      <td>1.724967</td>
      <td>0.642912</td>
      <td>0.902011</td>
      <td>01:27</td>
    </tr>
    <tr>
      <td>115</td>
      <td>0.669500</td>
      <td>1.714362</td>
      <td>0.636294</td>
      <td>0.901502</td>
      <td>01:29</td>
    </tr>
    <tr>
      <td>116</td>
      <td>0.660423</td>
      <td>1.681586</td>
      <td>0.651820</td>
      <td>0.887249</td>
      <td>01:27</td>
    </tr>
    <tr>
      <td>117</td>
      <td>0.665521</td>
      <td>1.719384</td>
      <td>0.637567</td>
      <td>0.883431</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>118</td>
      <td>0.664295</td>
      <td>1.670798</td>
      <td>0.654874</td>
      <td>0.908628</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>119</td>
      <td>0.660529</td>
      <td>1.645041</td>
      <td>0.641639</td>
      <td>0.902774</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>120</td>
      <td>0.668270</td>
      <td>1.651782</td>
      <td>0.646729</td>
      <td>0.902774</td>
      <td>01:27</td>
    </tr>
    <tr>
      <td>121</td>
      <td>0.665796</td>
      <td>1.625318</td>
      <td>0.666836</td>
      <td>0.903029</td>
      <td>01:29</td>
    </tr>
    <tr>
      <td>122</td>
      <td>0.664879</td>
      <td>1.680226</td>
      <td>0.645966</td>
      <td>0.889539</td>
      <td>01:27</td>
    </tr>
    <tr>
      <td>123</td>
      <td>0.661540</td>
      <td>1.563351</td>
      <td>0.679308</td>
      <td>0.910410</td>
      <td>01:27</td>
    </tr>
    <tr>
      <td>124</td>
      <td>0.662917</td>
      <td>1.714144</td>
      <td>0.640112</td>
      <td>0.882667</td>
      <td>01:27</td>
    </tr>
    <tr>
      <td>125</td>
      <td>0.667197</td>
      <td>1.645634</td>
      <td>0.644693</td>
      <td>0.907356</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>126</td>
      <td>0.664029</td>
      <td>1.577662</td>
      <td>0.670909</td>
      <td>0.910919</td>
      <td>01:27</td>
    </tr>
    <tr>
      <td>127</td>
      <td>0.658326</td>
      <td>1.705815</td>
      <td>0.640366</td>
      <td>0.889794</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>128</td>
      <td>0.661676</td>
      <td>1.627367</td>
      <td>0.659710</td>
      <td>0.892339</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>129</td>
      <td>0.664522</td>
      <td>1.627986</td>
      <td>0.662764</td>
      <td>0.907610</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>130</td>
      <td>0.667950</td>
      <td>1.625453</td>
      <td>0.664546</td>
      <td>0.890812</td>
      <td>01:27</td>
    </tr>
    <tr>
      <td>131</td>
      <td>0.659152</td>
      <td>1.503289</td>
      <td>0.694070</td>
      <td>0.919318</td>
      <td>01:29</td>
    </tr>
    <tr>
      <td>132</td>
      <td>0.662584</td>
      <td>1.590076</td>
      <td>0.664291</td>
      <td>0.909901</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>133</td>
      <td>0.659218</td>
      <td>1.672748</td>
      <td>0.650293</td>
      <td>0.904301</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>134</td>
      <td>0.651600</td>
      <td>1.684460</td>
      <td>0.649275</td>
      <td>0.898956</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>135</td>
      <td>0.663836</td>
      <td>1.719736</td>
      <td>0.630949</td>
      <td>0.904047</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>136</td>
      <td>0.661332</td>
      <td>1.705491</td>
      <td>0.635531</td>
      <td>0.890303</td>
      <td>01:27</td>
    </tr>
    <tr>
      <td>137</td>
      <td>0.660871</td>
      <td>1.644665</td>
      <td>0.664037</td>
      <td>0.900484</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>138</td>
      <td>0.663970</td>
      <td>1.658278</td>
      <td>0.659964</td>
      <td>0.904810</td>
      <td>01:27</td>
    </tr>
    <tr>
      <td>139</td>
      <td>0.664424</td>
      <td>1.580670</td>
      <td>0.671163</td>
      <td>0.902011</td>
      <td>01:27</td>
    </tr>
    <tr>
      <td>140</td>
      <td>0.665642</td>
      <td>1.625064</td>
      <td>0.657419</td>
      <td>0.916009</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>141</td>
      <td>0.659839</td>
      <td>1.636359</td>
      <td>0.661491</td>
      <td>0.898702</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>142</td>
      <td>0.653701</td>
      <td>1.663407</td>
      <td>0.651311</td>
      <td>0.898447</td>
      <td>01:29</td>
    </tr>
    <tr>
      <td>143</td>
      <td>0.660808</td>
      <td>1.511316</td>
      <td>0.689997</td>
      <td>0.920081</td>
      <td>01:29</td>
    </tr>
    <tr>
      <td>144</td>
      <td>0.662686</td>
      <td>1.664680</td>
      <td>0.661746</td>
      <td>0.890812</td>
      <td>01:27</td>
    </tr>
    <tr>
      <td>145</td>
      <td>0.662603</td>
      <td>1.629860</td>
      <td>0.653601</td>
      <td>0.906083</td>
      <td>01:27</td>
    </tr>
    <tr>
      <td>146</td>
      <td>0.663252</td>
      <td>1.683745</td>
      <td>0.645966</td>
      <td>0.885976</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>147</td>
      <td>0.660530</td>
      <td>1.717509</td>
      <td>0.637567</td>
      <td>0.895648</td>
      <td>01:27</td>
    </tr>
    <tr>
      <td>148</td>
      <td>0.658391</td>
      <td>1.653062</td>
      <td>0.659710</td>
      <td>0.883685</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>149</td>
      <td>0.657494</td>
      <td>1.735441</td>
      <td>0.630695</td>
      <td>0.886740</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>150</td>
      <td>0.656552</td>
      <td>1.693063</td>
      <td>0.639857</td>
      <td>0.890812</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>151</td>
      <td>0.656756</td>
      <td>1.607776</td>
      <td>0.674472</td>
      <td>0.902265</td>
      <td>01:27</td>
    </tr>
    <tr>
      <td>152</td>
      <td>0.660255</td>
      <td>1.534656</td>
      <td>0.680580</td>
      <td>0.921863</td>
      <td>01:29</td>
    </tr>
    <tr>
      <td>153</td>
      <td>0.662490</td>
      <td>1.636573</td>
      <td>0.668109</td>
      <td>0.898702</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>154</td>
      <td>0.652353</td>
      <td>1.681437</td>
      <td>0.654874</td>
      <td>0.885976</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>155</td>
      <td>0.653090</td>
      <td>1.567325</td>
      <td>0.674472</td>
      <td>0.909901</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>156</td>
      <td>0.659007</td>
      <td>1.617906</td>
      <td>0.665564</td>
      <td>0.905574</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>157</td>
      <td>0.652265</td>
      <td>1.579891</td>
      <td>0.671418</td>
      <td>0.919827</td>
      <td>01:27</td>
    </tr>
    <tr>
      <td>158</td>
      <td>0.651727</td>
      <td>1.592979</td>
      <td>0.676508</td>
      <td>0.898956</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>159</td>
      <td>0.654164</td>
      <td>1.641384</td>
      <td>0.658183</td>
      <td>0.901502</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>160</td>
      <td>0.648111</td>
      <td>1.606161</td>
      <td>0.666836</td>
      <td>0.910155</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>161</td>
      <td>0.645283</td>
      <td>1.656360</td>
      <td>0.661237</td>
      <td>0.912191</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>162</td>
      <td>0.646008</td>
      <td>1.588648</td>
      <td>0.661491</td>
      <td>0.919572</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>163</td>
      <td>0.639558</td>
      <td>1.638041</td>
      <td>0.660219</td>
      <td>0.906592</td>
      <td>01:27</td>
    </tr>
    <tr>
      <td>164</td>
      <td>0.635897</td>
      <td>1.572298</td>
      <td>0.683889</td>
      <td>0.899466</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>165</td>
      <td>0.639987</td>
      <td>1.496499</td>
      <td>0.698142</td>
      <td>0.919318</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>166</td>
      <td>0.632009</td>
      <td>1.630252</td>
      <td>0.669636</td>
      <td>0.909646</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>167</td>
      <td>0.632678</td>
      <td>1.593675</td>
      <td>0.679562</td>
      <td>0.905828</td>
      <td>01:27</td>
    </tr>
    <tr>
      <td>168</td>
      <td>0.633376</td>
      <td>1.676565</td>
      <td>0.648257</td>
      <td>0.903283</td>
      <td>01:27</td>
    </tr>
    <tr>
      <td>169</td>
      <td>0.629954</td>
      <td>1.562266</td>
      <td>0.677781</td>
      <td>0.906337</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>170</td>
      <td>0.630421</td>
      <td>1.616270</td>
      <td>0.669127</td>
      <td>0.907356</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>171</td>
      <td>0.627126</td>
      <td>1.556655</td>
      <td>0.679817</td>
      <td>0.922627</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>172</td>
      <td>0.623009</td>
      <td>1.500058</td>
      <td>0.702723</td>
      <td>0.921100</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>173</td>
      <td>0.622113</td>
      <td>1.557891</td>
      <td>0.683635</td>
      <td>0.907101</td>
      <td>01:27</td>
    </tr>
    <tr>
      <td>174</td>
      <td>0.620301</td>
      <td>1.525506</td>
      <td>0.692288</td>
      <td>0.918554</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>175</td>
      <td>0.620560</td>
      <td>1.504064</td>
      <td>0.698397</td>
      <td>0.919827</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>176</td>
      <td>0.618204</td>
      <td>1.527725</td>
      <td>0.700942</td>
      <td>0.908883</td>
      <td>01:27</td>
    </tr>
    <tr>
      <td>177</td>
      <td>0.615627</td>
      <td>1.527674</td>
      <td>0.699160</td>
      <td>0.915246</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>178</td>
      <td>0.615488</td>
      <td>1.523962</td>
      <td>0.698906</td>
      <td>0.911428</td>
      <td>01:29</td>
    </tr>
    <tr>
      <td>179</td>
      <td>0.613471</td>
      <td>1.584440</td>
      <td>0.689743</td>
      <td>0.916264</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>180</td>
      <td>0.614224</td>
      <td>1.509639</td>
      <td>0.698397</td>
      <td>0.915246</td>
      <td>01:27</td>
    </tr>
    <tr>
      <td>181</td>
      <td>0.613165</td>
      <td>1.458711</td>
      <td>0.716722</td>
      <td>0.926699</td>
      <td>01:29</td>
    </tr>
    <tr>
      <td>182</td>
      <td>0.612978</td>
      <td>1.454100</td>
      <td>0.716976</td>
      <td>0.923390</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>183</td>
      <td>0.610257</td>
      <td>1.417066</td>
      <td>0.725884</td>
      <td>0.932807</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>184</td>
      <td>0.611986</td>
      <td>1.461092</td>
      <td>0.714177</td>
      <td>0.922627</td>
      <td>01:27</td>
    </tr>
    <tr>
      <td>185</td>
      <td>0.610788</td>
      <td>1.421878</td>
      <td>0.718503</td>
      <td>0.933571</td>
      <td>01:27</td>
    </tr>
    <tr>
      <td>186</td>
      <td>0.610520</td>
      <td>1.450849</td>
      <td>0.719267</td>
      <td>0.925426</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>187</td>
      <td>0.609228</td>
      <td>1.444587</td>
      <td>0.719776</td>
      <td>0.920845</td>
      <td>01:29</td>
    </tr>
    <tr>
      <td>188</td>
      <td>0.609320</td>
      <td>1.427077</td>
      <td>0.726139</td>
      <td>0.926953</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>189</td>
      <td>0.607644</td>
      <td>1.473217</td>
      <td>0.713413</td>
      <td>0.920081</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>190</td>
      <td>0.607555</td>
      <td>1.457168</td>
      <td>0.719267</td>
      <td>0.923899</td>
      <td>01:27</td>
    </tr>
    <tr>
      <td>191</td>
      <td>0.608744</td>
      <td>1.443608</td>
      <td>0.721049</td>
      <td>0.926190</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>192</td>
      <td>0.608430</td>
      <td>1.445655</td>
      <td>0.719267</td>
      <td>0.930008</td>
      <td>01:27</td>
    </tr>
    <tr>
      <td>193</td>
      <td>0.607131</td>
      <td>1.443593</td>
      <td>0.721812</td>
      <td>0.926699</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>194</td>
      <td>0.607190</td>
      <td>1.437117</td>
      <td>0.723339</td>
      <td>0.929244</td>
      <td>01:27</td>
    </tr>
    <tr>
      <td>195</td>
      <td>0.607104</td>
      <td>1.440241</td>
      <td>0.723085</td>
      <td>0.927971</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>196</td>
      <td>0.607825</td>
      <td>1.423626</td>
      <td>0.727666</td>
      <td>0.928226</td>
      <td>01:27</td>
    </tr>
    <tr>
      <td>197</td>
      <td>0.606993</td>
      <td>1.419526</td>
      <td>0.726393</td>
      <td>0.930008</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>198</td>
      <td>0.608011</td>
      <td>1.451483</td>
      <td>0.722576</td>
      <td>0.923645</td>
      <td>01:28</td>
    </tr>
    <tr>
      <td>199</td>
      <td>0.607159</td>
      <td>1.439214</td>
      <td>0.721303</td>
      <td>0.929499</td>
      <td>01:28</td>
    </tr>
  </tbody>
</table>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

</div>
 

