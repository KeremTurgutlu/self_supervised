---

title: CLIP-MoCo


keywords: fastai
sidebar: home_sidebar

summary: "CLIP: <a href='https://arxiv.org/pdf/2103.00020.pdf'>Learning Transferable Visual Models From Natural Language Supervision</a>"
description: "CLIP: <a href='https://arxiv.org/pdf/2103.00020.pdf'>Learning Transferable Visual Models From Natural Language Supervision</a>"
nb_path: "nbs/21 - clip-moco.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/21 - clip-moco.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This module combines CLIP and MoCo for increasing negative samples. This is useful when there is no available compute such as GPUs with large memory to support large batch sizes or multi-gpu machines to leverage distributed infonce loss implementation.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Algorithm">Algorithm<a class="anchor-link" href="#Algorithm"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="CLIP">CLIP<a class="anchor-link" href="#CLIP"> </a></h4>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/self_supervised/images/clip.png" alt="CLIP"></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="MoCo">MoCo<a class="anchor-link" href="#MoCo"> </a></h4>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/self_supervised/images/moco.png" alt=""></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Tokenizer">Tokenizer<a class="anchor-link" href="#Tokenizer"> </a></h3>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="ClipTokenizer" class="doc_header"><code>class</code> <code>ClipTokenizer</code><a href="https://github.com/keremturgutlu/self_supervised/tree/main/self_supervised/multimodal/clip_moco.py#L23" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>ClipTokenizer</code>(<strong><code>context_length</code></strong>=<em><code>77</code></em>) :: <code>DisplayedTransform</code></p>
</blockquote>
<p>Tokenizer from <a href="https://github.com/openai/CLIP/blob/main/clip/simple_tokenizer.py">https://github.com/openai/CLIP/blob/main/clip/simple_tokenizer.py</a></p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Model">Model<a class="anchor-link" href="#Model"> </a></h3>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="vitb32_config" class="doc_header"><code>vitb32_config</code><a href="https://github.com/keremturgutlu/self_supervised/tree/main/self_supervised/multimodal/clip_moco.py#L40" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>vitb32_config</code>(<strong><code>input_res</code></strong>, <strong><code>context_length</code></strong>, <strong><code>vocab_size</code></strong>)</p>
</blockquote>
<p>ViT-B/32 configuration, uses 32x32 patches</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="vitl14_config" class="doc_header"><code>vitl14_config</code><a href="https://github.com/keremturgutlu/self_supervised/tree/main/self_supervised/multimodal/clip_moco.py#L54" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>vitl14_config</code>(<strong><code>input_res</code></strong>, <strong><code>context_length</code></strong>, <strong><code>vocab_size</code></strong>)</p>
</blockquote>
<p>ViT-L/14 configuration, uses 14x14 patches</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="Bottleneck" class="doc_header"><code>class</code> <code>Bottleneck</code><a href="https://github.com/keremturgutlu/self_supervised/tree/main/self_supervised/multimodal/clip_moco.py#L72" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>Bottleneck</code>(<strong><code>inplanes</code></strong>, <strong><code>planes</code></strong>, <strong><code>stride</code></strong>=<em><code>1</code></em>) :: <code>Module</code></p>
</blockquote>
<p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>

<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))

</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<p>:ivar training: Boolean represents whether this module is in training or
                evaluation mode.
:vartype training: bool</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="AttentionPool2d" class="doc_header"><code>class</code> <code>AttentionPool2d</code><a href="https://github.com/keremturgutlu/self_supervised/tree/main/self_supervised/multimodal/clip_moco.py#L118" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>AttentionPool2d</code>(<strong><code>spacial_dim</code></strong>:<code>int</code>, <strong><code>embed_dim</code></strong>:<code>int</code>, <strong><code>num_heads</code></strong>:<code>int</code>, <strong><code>output_dim</code></strong>:<code>int</code>=<em><code>None</code></em>) :: <code>Module</code></p>
</blockquote>
<p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>

<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))

</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<p>:ivar training: Boolean represents whether this module is in training or
                evaluation mode.
:vartype training: bool</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="ModifiedResNet" class="doc_header"><code>class</code> <code>ModifiedResNet</code><a href="https://github.com/keremturgutlu/self_supervised/tree/main/self_supervised/multimodal/clip_moco.py#L155" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>ModifiedResNet</code>(<strong><code>layers</code></strong>, <strong><code>output_dim</code></strong>, <strong><code>heads</code></strong>, <strong><code>input_resolution</code></strong>=<em><code>224</code></em>, <strong><code>width</code></strong>=<em><code>64</code></em>) :: <code>Module</code></p>
</blockquote>
<p>A ResNet class that is similar to torchvision's but contains the following changes:</p>
<ul>
<li>There are now 3 "stem" convolutions as opposed to 1, with an average pool instead of a max pool.</li>
<li>Performs anti-aliasing strided convolutions, where an avgpool is prepended to convolutions with stride &gt; 1</li>
<li>The final pooling layer is a QKV attention instead of an average pool</li>
</ul>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="LayerNorm" class="doc_header"><code>class</code> <code>LayerNorm</code><a href="https://github.com/keremturgutlu/self_supervised/tree/main/self_supervised/multimodal/clip_moco.py#L215" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>LayerNorm</code>(<strong><code>normalized_shape</code></strong>:<code>Union</code>[<code>int</code>, List[int]<code>,</code>Size<code>\], **</code>eps<code>**:</code>float<code>=*</code>1e-05<code>*, **</code>elementwise_affine<code>**:</code>bool<code>=*</code>True<code>*, **</code>device<code>**=*</code>None<code>*, **</code>dtype<code>**=*</code>None<code>*) :: [</code>LayerNorm`](/self_supervised/21 - clip-moco.html#LayerNorm)</p>
</blockquote>
<p>Subclass torch's LayerNorm to handle fp16.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="QuickGELU" class="doc_header"><code>class</code> <code>QuickGELU</code><a href="https://github.com/keremturgutlu/self_supervised/tree/main/self_supervised/multimodal/clip_moco.py#L224" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>QuickGELU</code>() :: <code>Module</code></p>
</blockquote>
<p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>

<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))

</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<p>:ivar training: Boolean represents whether this module is in training or
                evaluation mode.
:vartype training: bool</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="ResidualAttentionBlock" class="doc_header"><code>class</code> <code>ResidualAttentionBlock</code><a href="https://github.com/keremturgutlu/self_supervised/tree/main/self_supervised/multimodal/clip_moco.py#L229" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>ResidualAttentionBlock</code>(<strong><code>d_model</code></strong>:<code>int</code>, <strong><code>n_head</code></strong>:<code>int</code>, <strong><code>attn_mask</code></strong>:<code>Tensor</code>=<em><code>None</code></em>) :: <code>Module</code></p>
</blockquote>
<p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>

<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))

</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<p>:ivar training: Boolean represents whether this module is in training or
                evaluation mode.
:vartype training: bool</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="Transformer" class="doc_header"><code>class</code> <code>Transformer</code><a href="https://github.com/keremturgutlu/self_supervised/tree/main/self_supervised/multimodal/clip_moco.py#L253" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>Transformer</code>(<strong><code>width</code></strong>:<code>int</code>, <strong><code>layers</code></strong>:<code>int</code>, <strong><code>heads</code></strong>:<code>int</code>, <strong><code>attn_mask</code></strong>:<code>Tensor</code>=<em><code>None</code></em>, <strong><code>checkpoint</code></strong>=<em><code>False</code></em>, <strong><code>checkpoint_nchunks</code></strong>=<em><code>2</code></em>) :: <code>Module</code></p>
</blockquote>
<p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>

<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))

</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<p>:ivar training: Boolean represents whether this module is in training or
                evaluation mode.
:vartype training: bool</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="VisualTransformer" class="doc_header"><code>class</code> <code>VisualTransformer</code><a href="https://github.com/keremturgutlu/self_supervised/tree/main/self_supervised/multimodal/clip_moco.py#L267" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>VisualTransformer</code>(<strong><code>input_resolution</code></strong>:<code>int</code>, <strong><code>patch_size</code></strong>:<code>int</code>, <strong><code>width</code></strong>:<code>int</code>, <strong><code>layers</code></strong>:<code>int</code>, <strong><code>heads</code></strong>:<code>int</code>, <strong><code>output_dim</code></strong>:<code>int</code>, <strong>**<code>kwargs</code></strong>) :: <code>Module</code></p>
</blockquote>
<p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>

<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))

</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<p>:ivar training: Boolean represents whether this module is in training or
                evaluation mode.
:vartype training: bool</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="CLIPMOCO" class="doc_header"><code>class</code> <code>CLIPMOCO</code><a href="https://github.com/keremturgutlu/self_supervised/tree/main/self_supervised/multimodal/clip_moco.py#L304" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>CLIPMOCO</code>(<strong><code>embed_dim</code></strong>:<code>int</code>, <strong><code>image_resolution</code></strong>:<code>int</code>, <strong><code>vision_layers</code></strong>:<code>Union</code>[Tuple[int, int, int, int]<code>,</code>int<code>\], **</code>vision_width<code>**:</code>int<code>, **</code>vision_patch_size<code>**:</code>int<code>, **</code>context_length<code>**:</code>int<code>, **</code>vocab_size<code>**:</code>int<code>, **</code>transformer_width<code>**:</code>int<code>, **</code>transformer_heads<code>**:</code>int<code>, **</code>transformer_layers<code>**:</code>int<code>, **</code>K<code>**=*</code>4096<code>*, **</code>m<code>**=*</code>0.999<code>*, **\*\*</code>kwargs<code>**) ::</code>Module`</p>
</blockquote>
<p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>

<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))

</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<p>:ivar training: Boolean represents whether this module is in training or
                evaluation mode.
:vartype training: bool</p>
<table>
<thead><tr>
<th></th>
<th>Type</th>
<th>Default</th>
<th>Details</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong><code>embed_dim</code></strong></td>
<td><code>int</code></td>
<td></td>
<td><em>No Content</em></td>
</tr>
<tr>
<td><strong><code>image_resolution</code></strong></td>
<td><code>int</code></td>
<td></td>
<td>vision</td>
</tr>
<tr>
<td><strong><code>vision_layers</code></strong></td>
<td><code>Tuple[int, int, int, int], int]</code></td>
<td></td>
<td><em>No Content</em></td>
</tr>
<tr>
<td><strong><code>vision_width</code></strong></td>
<td><code>int</code></td>
<td></td>
<td><em>No Content</em></td>
</tr>
<tr>
<td><strong><code>vision_patch_size</code></strong></td>
<td><code>int</code></td>
<td></td>
<td><em>No Content</em></td>
</tr>
<tr>
<td><strong><code>context_length</code></strong></td>
<td><code>int</code></td>
<td></td>
<td>text</td>
</tr>
<tr>
<td><strong><code>vocab_size</code></strong></td>
<td><code>int</code></td>
<td></td>
<td><em>No Content</em></td>
</tr>
<tr>
<td><strong><code>transformer_width</code></strong></td>
<td><code>int</code></td>
<td></td>
<td><em>No Content</em></td>
</tr>
<tr>
<td><strong><code>transformer_heads</code></strong></td>
<td><code>int</code></td>
<td></td>
<td><em>No Content</em></td>
</tr>
<tr>
<td><strong><code>transformer_layers</code></strong></td>
<td><code>int</code></td>
<td></td>
<td><em>No Content</em></td>
</tr>
<tr>
<td><strong><code>K</code></strong></td>
<td><code>int</code></td>
<td><code>4096</code></td>
<td><em>No Content</em></td>
</tr>
<tr>
<td><strong><code>m</code></strong></td>
<td><code>float</code></td>
<td><code>999</code></td>
<td><em>No Content</em></td>
</tr>
<tr>
<td><strong><code>kwargs</code></strong></td>
<td></td>
<td></td>
<td><em>No Content</em></td>
</tr>
</tbody>
</table>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Metric">Metric<a class="anchor-link" href="#Metric"> </a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>A useful proxy metric for tracking training performance and convergence.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="RetrievalAtK" class="doc_header"><code>class</code> <code>RetrievalAtK</code><a href="https://github.com/keremturgutlu/self_supervised/tree/main/self_supervised/multimodal/clip_moco.py#L496" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>RetrievalAtK</code>(<strong><code>k</code></strong>=<em><code>20</code></em>, <strong>**<code>kwargs</code></strong>) :: <code>AccumMetric</code></p>
</blockquote>
<p>Stores predictions and targets on CPU in accumulate to perform final calculations with <code>func</code>.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="CLIP-MoCo-Callback">CLIP-MoCo Callback<a class="anchor-link" href="#CLIP-MoCo-Callback"> </a></h3>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="CLIPMOCOTrainer" class="doc_header"><code>class</code> <code>CLIPMOCOTrainer</code><a href="https://github.com/keremturgutlu/self_supervised/tree/main/self_supervised/multimodal/clip_moco.py#L525" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>CLIPMOCOTrainer</code>(<strong><code>after_create</code></strong>=<em><code>None</code></em>, <strong><code>before_fit</code></strong>=<em><code>None</code></em>, <strong><code>before_epoch</code></strong>=<em><code>None</code></em>, <strong><code>before_train</code></strong>=<em><code>None</code></em>, <strong><code>before_batch</code></strong>=<em><code>None</code></em>, <strong><code>after_pred</code></strong>=<em><code>None</code></em>, <strong><code>after_loss</code></strong>=<em><code>None</code></em>, <strong><code>before_backward</code></strong>=<em><code>None</code></em>, <strong><code>before_step</code></strong>=<em><code>None</code></em>, <strong><code>after_cancel_step</code></strong>=<em><code>None</code></em>, <strong><code>after_step</code></strong>=<em><code>None</code></em>, <strong><code>after_cancel_batch</code></strong>=<em><code>None</code></em>, <strong><code>after_batch</code></strong>=<em><code>None</code></em>, <strong><code>after_cancel_train</code></strong>=<em><code>None</code></em>, <strong><code>after_train</code></strong>=<em><code>None</code></em>, <strong><code>before_validate</code></strong>=<em><code>None</code></em>, <strong><code>after_cancel_validate</code></strong>=<em><code>None</code></em>, <strong><code>after_validate</code></strong>=<em><code>None</code></em>, <strong><code>after_cancel_epoch</code></strong>=<em><code>None</code></em>, <strong><code>after_epoch</code></strong>=<em><code>None</code></em>, <strong><code>after_cancel_fit</code></strong>=<em><code>None</code></em>, <strong><code>after_fit</code></strong>=<em><code>None</code></em>) :: <code>Callback</code></p>
</blockquote>
<p>MoCo Loss for CLIP. Can be used with or without DistributedDataParallel</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Example-Usage">Example Usage<a class="anchor-link" href="#Example-Usage"> </a></h3>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">num2txt</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;3&#39;</span><span class="p">:</span> <span class="s1">&#39;three&#39;</span><span class="p">,</span> <span class="s1">&#39;7&#39;</span><span class="p">:</span> <span class="s1">&#39;seven&#39;</span><span class="p">}</span>
<span class="k">def</span> <span class="nf">num_to_txt</span><span class="p">(</span><span class="n">o</span><span class="p">):</span> <span class="k">return</span> <span class="n">num2txt</span><span class="p">[</span><span class="n">o</span><span class="p">]</span>
<span class="k">def</span> <span class="nf">dummy_targ</span><span class="p">(</span><span class="n">o</span><span class="p">):</span> <span class="k">return</span> <span class="mi">0</span> <span class="c1"># loss func is not called without it</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">path</span> <span class="o">=</span> <span class="n">untar_data</span><span class="p">(</span><span class="n">URLs</span><span class="o">.</span><span class="n">MNIST_TINY</span><span class="p">)</span>
<span class="n">items</span> <span class="o">=</span> <span class="n">get_image_files</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>
<span class="n">clip_tokenizer</span> <span class="o">=</span> <span class="n">ClipTokenizer</span><span class="p">()</span>
<span class="n">tds</span> <span class="o">=</span> <span class="n">Datasets</span><span class="p">(</span><span class="n">items</span><span class="p">,</span> <span class="p">[</span><span class="n">PILImage</span><span class="o">.</span><span class="n">create</span><span class="p">,</span> <span class="p">[</span><span class="n">parent_label</span><span class="p">,</span> <span class="n">num_to_txt</span><span class="p">],</span> <span class="n">dummy_targ</span><span class="p">],</span> <span class="n">n_inp</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">splits</span><span class="o">=</span><span class="n">GrandparentSplitter</span><span class="p">()(</span><span class="n">items</span><span class="p">))</span>
<span class="n">dls</span> <span class="o">=</span> <span class="n">tds</span><span class="o">.</span><span class="n">dataloaders</span><span class="p">(</span><span class="n">bs</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">after_item</span><span class="o">=</span><span class="p">[</span><span class="n">Resize</span><span class="p">(</span><span class="mi">224</span><span class="p">),</span> <span class="n">clip_tokenizer</span><span class="p">,</span> <span class="n">ToTensor</span><span class="p">()],</span> <span class="n">after_batch</span><span class="o">=</span><span class="p">[</span><span class="n">IntToFloatTensor</span><span class="p">()],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cpu&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">vitb32_config_dict</span> <span class="o">=</span> <span class="n">vitb32_config</span><span class="p">(</span><span class="mi">224</span><span class="p">,</span> <span class="n">clip_tokenizer</span><span class="o">.</span><span class="n">context_length</span><span class="p">,</span> <span class="n">clip_tokenizer</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">)</span>
<span class="n">clip_model</span> <span class="o">=</span> <span class="n">CLIPMOCO</span><span class="p">(</span><span class="n">K</span><span class="o">=</span><span class="mi">4096</span><span class="p">,</span><span class="n">m</span><span class="o">=</span><span class="mf">0.999</span><span class="p">,</span> <span class="o">**</span><span class="n">vitb32_config_dict</span><span class="p">,</span> <span class="n">checkpoint</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">checkpoint_nchunks</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">learner</span> <span class="o">=</span> <span class="n">Learner</span><span class="p">(</span><span class="n">dls</span><span class="p">,</span> <span class="n">clip_model</span><span class="p">,</span> <span class="n">loss_func</span><span class="o">=</span><span class="n">noop</span><span class="p">,</span> <span class="n">cbs</span><span class="o">=</span><span class="p">[</span><span class="n">CLIPMOCOTrainer</span><span class="p">(),</span> <span class="n">ShortEpochCallback</span><span class="p">(</span><span class="mf">0.001</span><span class="p">)],</span>
                  <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="n">RetrievalAtK</span><span class="p">(</span><span class="n">k</span><span class="o">=</span><span class="mi">5</span><span class="p">),</span> 
                           <span class="n">RetrievalAtK</span><span class="p">(</span><span class="n">k</span><span class="o">=</span><span class="mi">20</span><span class="p">),</span> 
                           <span class="n">RetrievalAtK</span><span class="p">(</span><span class="n">k</span><span class="o">=</span><span class="s2">&quot;mean&quot;</span><span class="p">),</span>
                           <span class="n">RetrievalAtK</span><span class="p">(</span><span class="n">k</span><span class="o">=</span><span class="s2">&quot;median&quot;</span><span class="p">)])</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learner</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea "></div>

</div>

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>CLIPMOCO (Input shape: 2 x torch.Size([2, 77]))
============================================================================
Layer (type)         Output Shape         Param #    Trainable 
============================================================================
                     2 x 768 x 7 x 7     
Conv2d                                    2359296    True      
LayerNorm                                 1536       True      
LayerNorm                                 1536       True      
____________________________________________________________________________
                     2 x 1 x 3072        
Linear                                    2362368    True      
QuickGELU                                                      
____________________________________________________________________________
                     2 x 1 x 768         
Linear                                    2360064    True      
LayerNorm                                 1536       True      
LayerNorm                                 1536       True      
____________________________________________________________________________
                     2 x 1 x 3072        
Linear                                    2362368    True      
QuickGELU                                                      
____________________________________________________________________________
                     2 x 1 x 768         
Linear                                    2360064    True      
LayerNorm                                 1536       True      
LayerNorm                                 1536       True      
____________________________________________________________________________
                     2 x 1 x 3072        
Linear                                    2362368    True      
QuickGELU                                                      
____________________________________________________________________________
                     2 x 1 x 768         
Linear                                    2360064    True      
LayerNorm                                 1536       True      
LayerNorm                                 1536       True      
____________________________________________________________________________
                     2 x 1 x 3072        
Linear                                    2362368    True      
QuickGELU                                                      
____________________________________________________________________________
                     2 x 1 x 768         
Linear                                    2360064    True      
LayerNorm                                 1536       True      
LayerNorm                                 1536       True      
____________________________________________________________________________
                     2 x 1 x 3072        
Linear                                    2362368    True      
QuickGELU                                                      
____________________________________________________________________________
                     2 x 1 x 768         
Linear                                    2360064    True      
LayerNorm                                 1536       True      
LayerNorm                                 1536       True      
____________________________________________________________________________
                     2 x 1 x 3072        
Linear                                    2362368    True      
QuickGELU                                                      
____________________________________________________________________________
                     2 x 1 x 768         
Linear                                    2360064    True      
LayerNorm                                 1536       True      
LayerNorm                                 1536       True      
____________________________________________________________________________
                     2 x 1 x 3072        
Linear                                    2362368    True      
QuickGELU                                                      
____________________________________________________________________________
                     2 x 1 x 768         
Linear                                    2360064    True      
LayerNorm                                 1536       True      
LayerNorm                                 1536       True      
____________________________________________________________________________
                     2 x 1 x 3072        
Linear                                    2362368    True      
QuickGELU                                                      
____________________________________________________________________________
                     2 x 1 x 768         
Linear                                    2360064    True      
LayerNorm                                 1536       True      
LayerNorm                                 1536       True      
____________________________________________________________________________
                     2 x 1 x 3072        
Linear                                    2362368    True      
QuickGELU                                                      
____________________________________________________________________________
                     2 x 1 x 768         
Linear                                    2360064    True      
LayerNorm                                 1536       True      
LayerNorm                                 1536       True      
____________________________________________________________________________
                     2 x 1 x 3072        
Linear                                    2362368    True      
QuickGELU                                                      
____________________________________________________________________________
                     2 x 1 x 768         
Linear                                    2360064    True      
LayerNorm                                 1536       True      
LayerNorm                                 1536       True      
____________________________________________________________________________
                     2 x 1 x 3072        
Linear                                    2362368    True      
QuickGELU                                                      
____________________________________________________________________________
                     2 x 1 x 768         
Linear                                    2360064    True      
LayerNorm                                 1536       True      
LayerNorm                                 1536       True      
____________________________________________________________________________
                     2 x 1 x 3072        
Linear                                    2362368    True      
QuickGELU                                                      
____________________________________________________________________________
                     2 x 1 x 768         
Linear                                    2360064    True      
LayerNorm                                 1536       True      
LayerNorm                                 1536       True      
LayerNorm                                 1024       True      
____________________________________________________________________________
                     2 x 1 x 2048        
Linear                                    1050624    True      
QuickGELU                                                      
____________________________________________________________________________
                     2 x 1 x 512         
Linear                                    1049088    True      
LayerNorm                                 1024       True      
LayerNorm                                 1024       True      
____________________________________________________________________________
                     2 x 1 x 2048        
Linear                                    1050624    True      
QuickGELU                                                      
____________________________________________________________________________
                     2 x 1 x 512         
Linear                                    1049088    True      
LayerNorm                                 1024       True      
LayerNorm                                 1024       True      
____________________________________________________________________________
                     2 x 1 x 2048        
Linear                                    1050624    True      
QuickGELU                                                      
____________________________________________________________________________
                     2 x 1 x 512         
Linear                                    1049088    True      
LayerNorm                                 1024       True      
LayerNorm                                 1024       True      
____________________________________________________________________________
                     2 x 1 x 2048        
Linear                                    1050624    True      
QuickGELU                                                      
____________________________________________________________________________
                     2 x 1 x 512         
Linear                                    1049088    True      
LayerNorm                                 1024       True      
LayerNorm                                 1024       True      
____________________________________________________________________________
                     2 x 1 x 2048        
Linear                                    1050624    True      
QuickGELU                                                      
____________________________________________________________________________
                     2 x 1 x 512         
Linear                                    1049088    True      
LayerNorm                                 1024       True      
LayerNorm                                 1024       True      
____________________________________________________________________________
                     2 x 1 x 2048        
Linear                                    1050624    True      
QuickGELU                                                      
____________________________________________________________________________
                     2 x 1 x 512         
Linear                                    1049088    True      
LayerNorm                                 1024       True      
LayerNorm                                 1024       True      
____________________________________________________________________________
                     2 x 1 x 2048        
Linear                                    1050624    True      
QuickGELU                                                      
____________________________________________________________________________
                     2 x 1 x 512         
Linear                                    1049088    True      
LayerNorm                                 1024       True      
LayerNorm                                 1024       True      
____________________________________________________________________________
                     2 x 1 x 2048        
Linear                                    1050624    True      
QuickGELU                                                      
____________________________________________________________________________
                     2 x 1 x 512         
Linear                                    1049088    True      
LayerNorm                                 1024       True      
LayerNorm                                 1024       True      
____________________________________________________________________________
                     2 x 1 x 2048        
Linear                                    1050624    True      
QuickGELU                                                      
____________________________________________________________________________
                     2 x 1 x 512         
Linear                                    1049088    True      
LayerNorm                                 1024       True      
LayerNorm                                 1024       True      
____________________________________________________________________________
                     2 x 1 x 2048        
Linear                                    1050624    True      
QuickGELU                                                      
____________________________________________________________________________
                     2 x 1 x 512         
Linear                                    1049088    True      
LayerNorm                                 1024       True      
LayerNorm                                 1024       True      
____________________________________________________________________________
                     2 x 1 x 2048        
Linear                                    1050624    True      
QuickGELU                                                      
____________________________________________________________________________
                     2 x 1 x 512         
Linear                                    1049088    True      
LayerNorm                                 1024       True      
LayerNorm                                 1024       True      
____________________________________________________________________________
                     2 x 1 x 2048        
Linear                                    1050624    True      
QuickGELU                                                      
____________________________________________________________________________
                     2 x 1 x 512         
Linear                                    1049088    True      
LayerNorm                                 1024       True      
____________________________________________________________________________
                     2 x 77 x 512        
Embedding                                 25296896   True      
LayerNorm                                 1024       True      
____________________________________________________________________________
                     2 x 768 x 7 x 7     
Conv2d                                    2359296    False     
LayerNorm                                 1536       False     
LayerNorm                                 1536       False     
____________________________________________________________________________
                     2 x 1 x 3072        
Linear                                    2362368    False     
QuickGELU                                                      
____________________________________________________________________________
                     2 x 1 x 768         
Linear                                    2360064    False     
LayerNorm                                 1536       False     
LayerNorm                                 1536       False     
____________________________________________________________________________
                     2 x 1 x 3072        
Linear                                    2362368    False     
QuickGELU                                                      
____________________________________________________________________________
                     2 x 1 x 768         
Linear                                    2360064    False     
LayerNorm                                 1536       False     
LayerNorm                                 1536       False     
____________________________________________________________________________
                     2 x 1 x 3072        
Linear                                    2362368    False     
QuickGELU                                                      
____________________________________________________________________________
                     2 x 1 x 768         
Linear                                    2360064    False     
LayerNorm                                 1536       False     
LayerNorm                                 1536       False     
____________________________________________________________________________
                     2 x 1 x 3072        
Linear                                    2362368    False     
QuickGELU                                                      
____________________________________________________________________________
                     2 x 1 x 768         
Linear                                    2360064    False     
LayerNorm                                 1536       False     
LayerNorm                                 1536       False     
____________________________________________________________________________
                     2 x 1 x 3072        
Linear                                    2362368    False     
QuickGELU                                                      
____________________________________________________________________________
                     2 x 1 x 768         
Linear                                    2360064    False     
LayerNorm                                 1536       False     
LayerNorm                                 1536       False     
____________________________________________________________________________
                     2 x 1 x 3072        
Linear                                    2362368    False     
QuickGELU                                                      
____________________________________________________________________________
                     2 x 1 x 768         
Linear                                    2360064    False     
LayerNorm                                 1536       False     
LayerNorm                                 1536       False     
____________________________________________________________________________
                     2 x 1 x 3072        
Linear                                    2362368    False     
QuickGELU                                                      
____________________________________________________________________________
                     2 x 1 x 768         
Linear                                    2360064    False     
LayerNorm                                 1536       False     
LayerNorm                                 1536       False     
____________________________________________________________________________
                     2 x 1 x 3072        
Linear                                    2362368    False     
QuickGELU                                                      
____________________________________________________________________________
                     2 x 1 x 768         
Linear                                    2360064    False     
LayerNorm                                 1536       False     
LayerNorm                                 1536       False     
____________________________________________________________________________
                     2 x 1 x 3072        
Linear                                    2362368    False     
QuickGELU                                                      
____________________________________________________________________________
                     2 x 1 x 768         
Linear                                    2360064    False     
LayerNorm                                 1536       False     
LayerNorm                                 1536       False     
____________________________________________________________________________
                     2 x 1 x 3072        
Linear                                    2362368    False     
QuickGELU                                                      
____________________________________________________________________________
                     2 x 1 x 768         
Linear                                    2360064    False     
LayerNorm                                 1536       False     
LayerNorm                                 1536       False     
____________________________________________________________________________
                     2 x 1 x 3072        
Linear                                    2362368    False     
QuickGELU                                                      
____________________________________________________________________________
                     2 x 1 x 768         
Linear                                    2360064    False     
LayerNorm                                 1536       False     
LayerNorm                                 1536       False     
____________________________________________________________________________
                     2 x 1 x 3072        
Linear                                    2362368    False     
QuickGELU                                                      
____________________________________________________________________________
                     2 x 1 x 768         
Linear                                    2360064    False     
LayerNorm                                 1536       False     
LayerNorm                                 1536       False     
LayerNorm                                 1024       False     
____________________________________________________________________________
                     2 x 1 x 2048        
Linear                                    1050624    False     
QuickGELU                                                      
____________________________________________________________________________
                     2 x 1 x 512         
Linear                                    1049088    False     
LayerNorm                                 1024       False     
LayerNorm                                 1024       False     
____________________________________________________________________________
                     2 x 1 x 2048        
Linear                                    1050624    False     
QuickGELU                                                      
____________________________________________________________________________
                     2 x 1 x 512         
Linear                                    1049088    False     
LayerNorm                                 1024       False     
LayerNorm                                 1024       False     
____________________________________________________________________________
                     2 x 1 x 2048        
Linear                                    1050624    False     
QuickGELU                                                      
____________________________________________________________________________
                     2 x 1 x 512         
Linear                                    1049088    False     
LayerNorm                                 1024       False     
LayerNorm                                 1024       False     
____________________________________________________________________________
                     2 x 1 x 2048        
Linear                                    1050624    False     
QuickGELU                                                      
____________________________________________________________________________
                     2 x 1 x 512         
Linear                                    1049088    False     
LayerNorm                                 1024       False     
LayerNorm                                 1024       False     
____________________________________________________________________________
                     2 x 1 x 2048        
Linear                                    1050624    False     
QuickGELU                                                      
____________________________________________________________________________
                     2 x 1 x 512         
Linear                                    1049088    False     
LayerNorm                                 1024       False     
LayerNorm                                 1024       False     
____________________________________________________________________________
                     2 x 1 x 2048        
Linear                                    1050624    False     
QuickGELU                                                      
____________________________________________________________________________
                     2 x 1 x 512         
Linear                                    1049088    False     
LayerNorm                                 1024       False     
LayerNorm                                 1024       False     
____________________________________________________________________________
                     2 x 1 x 2048        
Linear                                    1050624    False     
QuickGELU                                                      
____________________________________________________________________________
                     2 x 1 x 512         
Linear                                    1049088    False     
LayerNorm                                 1024       False     
LayerNorm                                 1024       False     
____________________________________________________________________________
                     2 x 1 x 2048        
Linear                                    1050624    False     
QuickGELU                                                      
____________________________________________________________________________
                     2 x 1 x 512         
Linear                                    1049088    False     
LayerNorm                                 1024       False     
LayerNorm                                 1024       False     
____________________________________________________________________________
                     2 x 1 x 2048        
Linear                                    1050624    False     
QuickGELU                                                      
____________________________________________________________________________
                     2 x 1 x 512         
Linear                                    1049088    False     
LayerNorm                                 1024       False     
LayerNorm                                 1024       False     
____________________________________________________________________________
                     2 x 1 x 2048        
Linear                                    1050624    False     
QuickGELU                                                      
____________________________________________________________________________
                     2 x 1 x 512         
Linear                                    1049088    False     
LayerNorm                                 1024       False     
LayerNorm                                 1024       False     
____________________________________________________________________________
                     2 x 1 x 2048        
Linear                                    1050624    False     
QuickGELU                                                      
____________________________________________________________________________
                     2 x 1 x 512         
Linear                                    1049088    False     
LayerNorm                                 1024       False     
LayerNorm                                 1024       False     
____________________________________________________________________________
                     2 x 1 x 2048        
Linear                                    1050624    False     
QuickGELU                                                      
____________________________________________________________________________
                     2 x 1 x 512         
Linear                                    1049088    False     
LayerNorm                                 1024       False     
____________________________________________________________________________

Total params: 193,876,992
Total trainable params: 109,587,456
Total non-trainable params: 84,289,536

Optimizer used: &lt;function Adam at 0x7fbd8d0189e0&gt;
Loss function: &lt;bound method CLIPMOCOTrainer.lf of CLIPMOCOTrainer&gt;

Callbacks:
  - TrainEvalCallback
  - ShortEpochCallback
  - CLIPMOCOTrainer
  - Recorder
  - ProgressCallback</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

</div>
 

