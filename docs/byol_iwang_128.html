---

title: Title


keywords: fastai
sidebar: home_sidebar



nb_path: "nbs/examples/byol_iwang_128.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/examples/byol_iwang_128.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Bootstrap-Your-Own-Latent-A-New-Approach-to-Self-Supervised-Learning:-https://arxiv.org/pdf/2006.07733.pdf"><strong>Bootstrap Your Own Latent A New Approach to Self-Supervised Learning:</strong> <a href="https://arxiv.org/pdf/2006.07733.pdf">https://arxiv.org/pdf/2006.07733.pdf</a><a class="anchor-link" href="#Bootstrap-Your-Own-Latent-A-New-Approach-to-Self-Supervised-Learning:-https://arxiv.org/pdf/2006.07733.pdf"> </a></h2>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">fastai</span><span class="o">,</span> <span class="nn">fastcore</span><span class="o">,</span> <span class="nn">torch</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">fastai</span><span class="o">.</span><span class="n">__version__</span> <span class="p">,</span> <span class="n">fastcore</span><span class="o">.</span><span class="n">__version__</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">__version__</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(&#39;2.0.8&#39;, &#39;1.0.1&#39;, &#39;1.6.0+cu101&#39;)</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">fastai.vision.all</span> <span class="kn">import</span> <span class="o">*</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Sizes">Sizes<a class="anchor-link" href="#Sizes"> </a></h3><p>Resize -&gt; RandomCrop</p>
<p>256 -&gt; 224 | 224 -&gt; 192 | 160 -&gt; 128</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">resize</span> <span class="o">=</span> <span class="mi">160</span>
<span class="n">size</span> <span class="o">=</span> <span class="mi">128</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="1.-Implementation-Details-(Section-3.2-from-the-paper)">1. Implementation Details (Section 3.2 from the paper)<a class="anchor-link" href="#1.-Implementation-Details-(Section-3.2-from-the-paper)"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="1.1-Image-Augmentations">1.1 Image Augmentations<a class="anchor-link" href="#1.1-Image-Augmentations"> </a></h3><p>Same as SimCLR with optional grayscale</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">kornia</span>
<span class="k">def</span> <span class="nf">get_aug_pipe</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">stats</span><span class="o">=</span><span class="n">imagenet_stats</span><span class="p">,</span> <span class="n">s</span><span class="o">=.</span><span class="mi">6</span><span class="p">):</span>
    <span class="s2">&quot;SimCLR augmentations&quot;</span>
    <span class="n">rrc</span> <span class="o">=</span> <span class="n">kornia</span><span class="o">.</span><span class="n">augmentation</span><span class="o">.</span><span class="n">RandomResizedCrop</span><span class="p">((</span><span class="n">size</span><span class="p">,</span> <span class="n">size</span><span class="p">),</span> <span class="n">scale</span><span class="o">=</span><span class="p">(</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">),</span> <span class="n">ratio</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="o">/</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="o">/</span><span class="mi">3</span><span class="p">))</span>
    <span class="n">rhf</span> <span class="o">=</span> <span class="n">kornia</span><span class="o">.</span><span class="n">augmentation</span><span class="o">.</span><span class="n">RandomHorizontalFlip</span><span class="p">()</span>
    <span class="n">rcj</span> <span class="o">=</span> <span class="n">kornia</span><span class="o">.</span><span class="n">augmentation</span><span class="o">.</span><span class="n">ColorJitter</span><span class="p">(</span><span class="mf">0.8</span><span class="o">*</span><span class="n">s</span><span class="p">,</span> <span class="mf">0.8</span><span class="o">*</span><span class="n">s</span><span class="p">,</span> <span class="mf">0.8</span><span class="o">*</span><span class="n">s</span><span class="p">,</span> <span class="mf">0.2</span><span class="o">*</span><span class="n">s</span><span class="p">)</span>
    <span class="n">rgs</span> <span class="o">=</span> <span class="n">kornia</span><span class="o">.</span><span class="n">augmentation</span><span class="o">.</span><span class="n">RandomGrayscale</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
    
    <span class="n">tfms</span> <span class="o">=</span> <span class="p">[</span><span class="n">rrc</span><span class="p">,</span> <span class="n">rhf</span><span class="p">,</span> <span class="n">rcj</span><span class="p">,</span> <span class="n">rgs</span><span class="p">,</span> <span class="n">Normalize</span><span class="o">.</span><span class="n">from_stats</span><span class="p">(</span><span class="o">*</span><span class="n">stats</span><span class="p">)]</span>
    <span class="n">pipe</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">(</span><span class="n">tfms</span><span class="p">)</span>
    <span class="n">pipe</span><span class="o">.</span><span class="n">split_idx</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">return</span> <span class="n">pipe</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="1.2-Architecture">1.2 Architecture<a class="anchor-link" href="#1.2-Architecture"> </a></h3>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">create_encoder</span><span class="p">(</span><span class="n">arch</span><span class="p">,</span> <span class="n">n_in</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">pretrained</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">cut</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">concat_pool</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="s2">&quot;Create encoder from a given arch backbone&quot;</span>
    <span class="n">encoder</span> <span class="o">=</span> <span class="n">create_body</span><span class="p">(</span><span class="n">arch</span><span class="p">,</span> <span class="n">n_in</span><span class="p">,</span> <span class="n">pretrained</span><span class="p">,</span> <span class="n">cut</span><span class="p">)</span>
    <span class="n">pool</span> <span class="o">=</span> <span class="n">AdaptiveConcatPool2d</span><span class="p">()</span> <span class="k">if</span> <span class="n">concat_pool</span> <span class="k">else</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveAvgPool2d</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">encoder</span><span class="p">,</span> <span class="n">pool</span><span class="p">,</span> <span class="n">Flatten</span><span class="p">())</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">MLP</span><span class="p">(</span><span class="n">Module</span><span class="p">):</span>
    <span class="s2">&quot;MLP module as described in paper&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">projection_size</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span><span class="mi">2048</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">projection_size</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">net</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">BYOLModel</span><span class="p">(</span><span class="n">Module</span><span class="p">):</span>
    <span class="s2">&quot;Compute predictions of v1 and v2&quot;</span> 
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">encoder</span><span class="p">,</span><span class="n">projector</span><span class="p">,</span><span class="n">predictor</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">projector</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">predictor</span> <span class="o">=</span> <span class="n">encoder</span><span class="p">,</span><span class="n">projector</span><span class="p">,</span><span class="n">predictor</span>    

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">v1</span><span class="p">,</span><span class="n">v2</span><span class="p">):</span>
        <span class="n">q1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">predictor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">projector</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">v1</span><span class="p">)))</span>
        <span class="n">q2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">predictor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">projector</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">v2</span><span class="p">)))</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">q1</span><span class="p">,</span><span class="n">q2</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">create_byol_model</span><span class="p">(</span><span class="n">arch</span><span class="o">=</span><span class="n">resnet50</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span><span class="mi">4096</span><span class="p">,</span> <span class="n">pretrained</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">projection_size</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">concat_pool</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="n">encoder</span> <span class="o">=</span> <span class="n">create_encoder</span><span class="p">(</span><span class="n">arch</span><span class="p">,</span> <span class="n">pretrained</span><span class="o">=</span><span class="n">pretrained</span><span class="p">,</span> <span class="n">concat_pool</span><span class="o">=</span><span class="n">concat_pool</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span> 
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">128</span><span class="p">,</span><span class="mi">128</span><span class="p">))</span>
        <span class="n">representation</span> <span class="o">=</span> <span class="n">encoder</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="n">projector</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span><span class="n">representation</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">projection_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span><span class="n">hidden_size</span><span class="p">)</span>     
    <span class="n">predictor</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span><span class="n">projection_size</span><span class="p">,</span> <span class="n">projection_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span><span class="n">hidden_size</span><span class="p">)</span>
    <span class="n">apply_init</span><span class="p">(</span><span class="n">projector</span><span class="p">)</span>
    <span class="n">apply_init</span><span class="p">(</span><span class="n">predictor</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">BYOLModel</span><span class="p">(</span><span class="n">encoder</span><span class="p">,</span> <span class="n">projector</span><span class="p">,</span> <span class="n">predictor</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="1.3-BYOLCallback">1.3 BYOLCallback<a class="anchor-link" href="#1.3-BYOLCallback"> </a></h3>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">_mse_loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">normalize</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">normalize</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="k">return</span> <span class="mi">2</span> <span class="o">-</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span> <span class="o">*</span> <span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">symmetric_mse_loss</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="o">*</span><span class="n">yb</span><span class="p">):</span>
    <span class="p">(</span><span class="n">q1</span><span class="p">,</span><span class="n">q2</span><span class="p">),</span><span class="n">z1</span><span class="p">,</span><span class="n">z2</span> <span class="o">=</span> <span class="n">pred</span><span class="p">,</span><span class="o">*</span><span class="n">yb</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">_mse_loss</span><span class="p">(</span><span class="n">q1</span><span class="p">,</span><span class="n">z2</span><span class="p">)</span> <span class="o">+</span> <span class="n">_mse_loss</span><span class="p">(</span><span class="n">q2</span><span class="p">,</span><span class="n">z1</span><span class="p">))</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">64</span><span class="p">,</span><span class="mi">256</span><span class="p">))</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">64</span><span class="p">,</span><span class="mi">256</span><span class="p">))</span>
<span class="n">test_close</span><span class="p">(</span><span class="n">symmetric_mse_loss</span><span class="p">((</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">),</span><span class="n">y</span><span class="p">,</span><span class="n">x</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span> <span class="c1"># perfect</span>
<span class="n">test_close</span><span class="p">(</span><span class="n">symmetric_mse_loss</span><span class="p">((</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">),</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">),</span> <span class="mi">4</span><span class="p">,</span> <span class="mf">1e-1</span><span class="p">)</span> <span class="c1"># random</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Useful Discussions and Supportive Material:</p>
<ul>
<li><a href="https://www.reddit.com/r/MachineLearning/comments/hju274/d_byol_bootstrap_your_own_latent_cheating/fwohtky/">https://www.reddit.com/r/MachineLearning/comments/hju274/d_byol_bootstrap_your_own_latent_cheating/fwohtky/</a></li>
<li><a href="https://untitled-ai.github.io/understanding-self-supervised-contrastive-learning.html">https://untitled-ai.github.io/understanding-self-supervised-contrastive-learning.html</a></li>
</ul>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">copy</span>

<span class="k">class</span> <span class="nc">BYOLCallback</span><span class="p">(</span><span class="n">Callback</span><span class="p">):</span>
    <span class="s2">&quot;Implementation of https://arxiv.org/pdf/2006.07733.pdf&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">T</span><span class="o">=</span><span class="mf">0.99</span><span class="p">,</span> <span class="n">debug</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">224</span><span class="p">,</span> <span class="o">**</span><span class="n">aug_kwargs</span><span class="p">):</span>        
        <span class="bp">self</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">debug</span> <span class="o">=</span> <span class="n">T</span><span class="p">,</span> <span class="n">debug</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">aug1</span> <span class="o">=</span> <span class="n">get_aug_pipe</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="o">**</span><span class="n">aug_kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">aug2</span> <span class="o">=</span> <span class="n">get_aug_pipe</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="o">**</span><span class="n">aug_kwargs</span><span class="p">)</span>


    <span class="k">def</span> <span class="nf">before_fit</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="s2">&quot;Create target model&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">target_model</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">learn</span><span class="o">.</span><span class="n">model</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dls</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>        
        <span class="bp">self</span><span class="o">.</span><span class="n">T_sched</span> <span class="o">=</span> <span class="n">SchedCos</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="c1"># used in paper</span>
        <span class="c1"># self.T_sched = SchedNo(self.T, 1) # used in open source implementation</span>
  
        
    <span class="k">def</span> <span class="nf">before_batch</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="s2">&quot;Generate 2 views of the same image and calculate target projections for these views&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">debug</span><span class="p">:</span> <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;self.x[0]: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        
        <span class="n">v1</span><span class="p">,</span><span class="n">v2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">aug1</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">aug2</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x</span><span class="o">.</span><span class="n">clone</span><span class="p">())</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">learn</span><span class="o">.</span><span class="n">xb</span> <span class="o">=</span> <span class="p">(</span><span class="n">v1</span><span class="p">,</span><span class="n">v2</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">debug</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;v1[0]: </span><span class="si">{</span><span class="n">v1</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="se">\n</span><span class="s2">v2[0]: </span><span class="si">{</span><span class="n">v2</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">show_one</span><span class="p">()</span>
            <span class="k">assert</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">learn</span><span class="o">.</span><span class="n">xb</span><span class="p">)</span>

        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">z1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">target_model</span><span class="o">.</span><span class="n">projector</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">target_model</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">v1</span><span class="p">))</span>
            <span class="n">z2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">target_model</span><span class="o">.</span><span class="n">projector</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">target_model</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">v2</span><span class="p">))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">learn</span><span class="o">.</span><span class="n">yb</span> <span class="o">=</span> <span class="p">(</span><span class="n">z1</span><span class="p">,</span><span class="n">z2</span><span class="p">)</span>


    <span class="k">def</span> <span class="nf">after_step</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="s2">&quot;Update target model and T&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">T</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">T_sched</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pct_train</span><span class="p">)</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="k">for</span> <span class="n">param_k</span><span class="p">,</span> <span class="n">param_q</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">target_model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">()):</span>
                <span class="n">param_k</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">param_k</span><span class="o">.</span><span class="n">data</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">T</span> <span class="o">+</span> <span class="n">param_q</span><span class="o">.</span><span class="n">data</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
          

    <span class="k">def</span> <span class="nf">show_one</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">b1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">aug1</span><span class="o">.</span><span class="n">normalize</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">to_detach</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">learn</span><span class="o">.</span><span class="n">xb</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
        <span class="n">b2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">aug1</span><span class="o">.</span><span class="n">normalize</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">to_detach</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">learn</span><span class="o">.</span><span class="n">xb</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
        <span class="n">i</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">b1</span><span class="p">))</span>
        <span class="n">show_images</span><span class="p">([</span><span class="n">b1</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="n">b2</span><span class="p">[</span><span class="n">i</span><span class="p">]],</span> <span class="n">nrows</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">after_train</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>      
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">debug</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">show_one</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">after_validate</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>   
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">debug</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">show_one</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="2.-Pretext-Training">2. Pretext Training<a class="anchor-link" href="#2.-Pretext-Training"> </a></h2>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">sqrmom</span><span class="o">=</span><span class="mf">0.99</span>
<span class="n">mom</span><span class="o">=</span><span class="mf">0.95</span>
<span class="n">beta</span><span class="o">=</span><span class="mf">0.</span>
<span class="n">eps</span><span class="o">=</span><span class="mf">1e-4</span>
<span class="n">opt_func</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">ranger</span><span class="p">,</span> <span class="n">mom</span><span class="o">=</span><span class="n">mom</span><span class="p">,</span> <span class="n">sqr_mom</span><span class="o">=</span><span class="n">sqrmom</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="n">beta</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">bs</span><span class="o">=</span><span class="mi">256</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">get_dls</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">bs</span><span class="p">,</span> <span class="n">workers</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">path</span> <span class="o">=</span> <span class="n">URLs</span><span class="o">.</span><span class="n">IMAGEWANG_160</span> <span class="k">if</span> <span class="n">size</span> <span class="o">&lt;=</span> <span class="mi">160</span> <span class="k">else</span> <span class="n">URLs</span><span class="o">.</span><span class="n">IMAGEWANG</span>
    <span class="n">source</span> <span class="o">=</span> <span class="n">untar_data</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>
    
    <span class="n">files</span> <span class="o">=</span> <span class="n">get_image_files</span><span class="p">(</span><span class="n">source</span><span class="p">)</span>
    <span class="n">tfms</span> <span class="o">=</span> <span class="p">[[</span><span class="n">PILImage</span><span class="o">.</span><span class="n">create</span><span class="p">,</span> <span class="n">ToTensor</span><span class="p">,</span> <span class="n">RandomResizedCrop</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">min_scale</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)],</span> 
            <span class="p">[</span><span class="n">parent_label</span><span class="p">,</span> <span class="n">Categorize</span><span class="p">()]]</span>
    
    <span class="n">dsets</span> <span class="o">=</span> <span class="n">Datasets</span><span class="p">(</span><span class="n">files</span><span class="p">,</span> <span class="n">tfms</span><span class="o">=</span><span class="n">tfms</span><span class="p">,</span> <span class="n">splits</span><span class="o">=</span><span class="n">RandomSplitter</span><span class="p">(</span><span class="n">valid_pct</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)(</span><span class="n">files</span><span class="p">))</span>
    
    <span class="n">batch_tfms</span> <span class="o">=</span> <span class="p">[</span><span class="n">IntToFloatTensor</span><span class="p">]</span>
    <span class="n">dls</span> <span class="o">=</span> <span class="n">dsets</span><span class="o">.</span><span class="n">dataloaders</span><span class="p">(</span><span class="n">bs</span><span class="o">=</span><span class="n">bs</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="n">workers</span><span class="p">,</span> <span class="n">after_batch</span><span class="o">=</span><span class="n">batch_tfms</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">dls</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">dls</span> <span class="o">=</span> <span class="n">get_dls</span><span class="p">(</span><span class="n">resize</span><span class="p">,</span> <span class="n">bs</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">create_byol_model</span><span class="p">(</span><span class="n">arch</span><span class="o">=</span><span class="n">xresnet34</span><span class="p">,</span> <span class="n">pretrained</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">learn</span> <span class="o">=</span> <span class="n">Learner</span><span class="p">(</span><span class="n">dls</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">symmetric_mse_loss</span><span class="p">,</span> <span class="n">opt_func</span><span class="o">=</span><span class="n">opt_func</span><span class="p">,</span>
                <span class="n">cbs</span><span class="o">=</span><span class="p">[</span><span class="n">BYOLCallback</span><span class="p">(</span><span class="n">T</span><span class="o">=</span><span class="mf">0.99</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">size</span><span class="p">,</span> <span class="n">debug</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span> <span class="n">TerminateOnNaNCallback</span><span class="p">()])</span>
<span class="n">learn</span><span class="o">.</span><span class="n">to_fp16</span><span class="p">();</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learn</span><span class="o">.</span><span class="n">lr_find</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">

</div>

</div>

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>SuggestedLRs(lr_min=0.017378008365631102, lr_steep=0.0008317637839354575)</pre>
</div>

</div>

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,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
"
>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">lr</span><span class="o">=</span><span class="mf">1e-2</span>
<span class="n">wd</span><span class="o">=</span><span class="mf">1e-2</span>
<span class="n">epochs</span><span class="o">=</span><span class="mi">100</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learn</span><span class="o">.</span><span class="n">unfreeze</span><span class="p">()</span>
<span class="n">learn</span><span class="o">.</span><span class="n">fit_flat_cos</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">wd</span><span class="o">=</span><span class="n">wd</span><span class="p">,</span> <span class="n">pct_start</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>2.096235</td>
      <td>1.795209</td>
      <td>00:51</td>
    </tr>
    <tr>
      <td>1</td>
      <td>1.239398</td>
      <td>0.665621</td>
      <td>00:52</td>
    </tr>
    <tr>
      <td>2</td>
      <td>0.792607</td>
      <td>0.683888</td>
      <td>00:52</td>
    </tr>
    <tr>
      <td>3</td>
      <td>0.649155</td>
      <td>0.732521</td>
      <td>00:53</td>
    </tr>
    <tr>
      <td>4</td>
      <td>0.660881</td>
      <td>0.897377</td>
      <td>00:53</td>
    </tr>
    <tr>
      <td>5</td>
      <td>0.663077</td>
      <td>0.991256</td>
      <td>00:52</td>
    </tr>
    <tr>
      <td>6</td>
      <td>0.702402</td>
      <td>1.090657</td>
      <td>00:52</td>
    </tr>
    <tr>
      <td>7</td>
      <td>0.701458</td>
      <td>0.992844</td>
      <td>00:52</td>
    </tr>
    <tr>
      <td>8</td>
      <td>0.706645</td>
      <td>1.060086</td>
      <td>00:51</td>
    </tr>
    <tr>
      <td>9</td>
      <td>0.715314</td>
      <td>1.181449</td>
      <td>00:51</td>
    </tr>
    <tr>
      <td>10</td>
      <td>0.740935</td>
      <td>1.133617</td>
      <td>00:51</td>
    </tr>
    <tr>
      <td>11</td>
      <td>0.739741</td>
      <td>1.190462</td>
      <td>00:51</td>
    </tr>
    <tr>
      <td>12</td>
      <td>0.728084</td>
      <td>1.180310</td>
      <td>00:51</td>
    </tr>
    <tr>
      <td>13</td>
      <td>0.716606</td>
      <td>1.094525</td>
      <td>00:51</td>
    </tr>
    <tr>
      <td>14</td>
      <td>0.710006</td>
      <td>1.087112</td>
      <td>00:51</td>
    </tr>
    <tr>
      <td>15</td>
      <td>0.728010</td>
      <td>1.173662</td>
      <td>00:51</td>
    </tr>
    <tr>
      <td>16</td>
      <td>0.718316</td>
      <td>1.174830</td>
      <td>00:51</td>
    </tr>
    <tr>
      <td>17</td>
      <td>0.722422</td>
      <td>1.106405</td>
      <td>00:51</td>
    </tr>
    <tr>
      <td>18</td>
      <td>0.709678</td>
      <td>1.105262</td>
      <td>00:51</td>
    </tr>
    <tr>
      <td>19</td>
      <td>0.698151</td>
      <td>1.145373</td>
      <td>00:51</td>
    </tr>
  </tbody>
</table>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">save_name</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;byol_iwang_sz</span><span class="si">{</span><span class="n">size</span><span class="si">}</span><span class="s1">_epc</span><span class="si">{</span><span class="n">epochs</span><span class="si">}</span><span class="s1">&#39;</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learn</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">save_name</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">learn</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="n">learn</span><span class="o">.</span><span class="n">path</span><span class="o">/</span><span class="n">learn</span><span class="o">.</span><span class="n">model_dir</span><span class="o">/</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">save_name</span><span class="si">}</span><span class="s1">_encoder.pth&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learn</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">save_name</span><span class="p">);</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">lr</span><span class="o">=</span><span class="mf">5e-3</span>
<span class="n">wd</span><span class="o">=</span><span class="mf">1e-2</span>
<span class="n">epochs</span><span class="o">=</span><span class="mi">100</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learn</span><span class="o">.</span><span class="n">unfreeze</span><span class="p">()</span>
<span class="n">learn</span><span class="o">.</span><span class="n">fit_flat_cos</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">wd</span><span class="o">=</span><span class="n">wd</span><span class="p">,</span> <span class="n">pct_start</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0.455322</td>
      <td>0.830326</td>
      <td>01:11</td>
    </tr>
    <tr>
      <td>1</td>
      <td>0.418708</td>
      <td>0.832226</td>
      <td>01:10</td>
    </tr>
    <tr>
      <td>2</td>
      <td>0.441065</td>
      <td>0.805281</td>
      <td>01:10</td>
    </tr>
    <tr>
      <td>3</td>
      <td>0.441163</td>
      <td>0.727251</td>
      <td>01:09</td>
    </tr>
    <tr>
      <td>4</td>
      <td>0.451951</td>
      <td>0.857548</td>
      <td>01:10</td>
    </tr>
    <tr>
      <td>5</td>
      <td>0.455712</td>
      <td>0.831888</td>
      <td>01:10</td>
    </tr>
    <tr>
      <td>6</td>
      <td>0.449232</td>
      <td>0.722951</td>
      <td>01:09</td>
    </tr>
    <tr>
      <td>7</td>
      <td>0.466381</td>
      <td>0.961531</td>
      <td>01:10</td>
    </tr>
    <tr>
      <td>8</td>
      <td>0.456386</td>
      <td>0.836119</td>
      <td>01:10</td>
    </tr>
    <tr>
      <td>9</td>
      <td>0.451449</td>
      <td>0.762669</td>
      <td>01:10</td>
    </tr>
    <tr>
      <td>10</td>
      <td>0.435044</td>
      <td>1.031822</td>
      <td>01:10</td>
    </tr>
    <tr>
      <td>11</td>
      <td>0.458287</td>
      <td>0.674367</td>
      <td>01:10</td>
    </tr>
    <tr>
      <td>12</td>
      <td>0.472091</td>
      <td>0.884220</td>
      <td>01:10</td>
    </tr>
    <tr>
      <td>13</td>
      <td>0.454591</td>
      <td>0.956118</td>
      <td>01:10</td>
    </tr>
    <tr>
      <td>14</td>
      <td>0.481176</td>
      <td>0.976489</td>
      <td>01:10</td>
    </tr>
    <tr>
      <td>15</td>
      <td>0.475727</td>
      <td>1.118201</td>
      <td>01:10</td>
    </tr>
    <tr>
      <td>16</td>
      <td>0.469472</td>
      <td>0.783874</td>
      <td>01:09</td>
    </tr>
    <tr>
      <td>17</td>
      <td>0.470294</td>
      <td>0.952466</td>
      <td>01:10</td>
    </tr>
    <tr>
      <td>18</td>
      <td>0.483409</td>
      <td>1.060557</td>
      <td>01:10</td>
    </tr>
    <tr>
      <td>19</td>
      <td>0.498161</td>
      <td>0.882904</td>
      <td>01:10</td>
    </tr>
    <tr>
      <td>20</td>
      <td>0.470419</td>
      <td>1.042331</td>
      <td>01:09</td>
    </tr>
    <tr>
      <td>21</td>
      <td>0.439108</td>
      <td>0.915545</td>
      <td>01:10</td>
    </tr>
    <tr>
      <td>22</td>
      <td>0.451300</td>
      <td>1.095176</td>
      <td>01:10</td>
    </tr>
    <tr>
      <td>23</td>
      <td>0.457824</td>
      <td>0.916717</td>
      <td>01:10</td>
    </tr>
    <tr>
      <td>24</td>
      <td>0.451308</td>
      <td>0.766573</td>
      <td>01:09</td>
    </tr>
    <tr>
      <td>25</td>
      <td>0.486710</td>
      <td>1.108290</td>
      <td>01:10</td>
    </tr>
    <tr>
      <td>26</td>
      <td>0.470417</td>
      <td>0.825275</td>
      <td>01:09</td>
    </tr>
    <tr>
      <td>27</td>
      <td>0.451175</td>
      <td>1.103124</td>
      <td>01:10</td>
    </tr>
    <tr>
      <td>28</td>
      <td>0.460681</td>
      <td>0.976302</td>
      <td>01:10</td>
    </tr>
    <tr>
      <td>29</td>
      <td>0.459778</td>
      <td>0.919442</td>
      <td>01:10</td>
    </tr>
    <tr>
      <td>30</td>
      <td>0.464380</td>
      <td>1.026175</td>
      <td>01:10</td>
    </tr>
    <tr>
      <td>31</td>
      <td>0.466524</td>
      <td>1.159525</td>
      <td>01:10</td>
    </tr>
    <tr>
      <td>32</td>
      <td>0.466945</td>
      <td>0.972683</td>
      <td>01:10</td>
    </tr>
    <tr>
      <td>33</td>
      <td>0.458955</td>
      <td>1.129160</td>
      <td>01:10</td>
    </tr>
    <tr>
      <td>34</td>
      <td>0.426805</td>
      <td>1.074839</td>
      <td>01:10</td>
    </tr>
    <tr>
      <td>35</td>
      <td>0.472233</td>
      <td>1.062310</td>
      <td>01:10</td>
    </tr>
    <tr>
      <td>36</td>
      <td>0.489943</td>
      <td>0.902662</td>
      <td>01:10</td>
    </tr>
    <tr>
      <td>37</td>
      <td>0.483166</td>
      <td>1.022394</td>
      <td>01:10</td>
    </tr>
    <tr>
      <td>38</td>
      <td>0.473635</td>
      <td>0.821667</td>
      <td>01:10</td>
    </tr>
    <tr>
      <td>39</td>
      <td>0.461150</td>
      <td>0.772805</td>
      <td>01:10</td>
    </tr>
    <tr>
      <td>40</td>
      <td>0.453444</td>
      <td>0.943367</td>
      <td>01:10</td>
    </tr>
    <tr>
      <td>41</td>
      <td>0.449936</td>
      <td>0.979725</td>
      <td>01:10</td>
    </tr>
    <tr>
      <td>42</td>
      <td>0.481001</td>
      <td>0.806017</td>
      <td>01:10</td>
    </tr>
    <tr>
      <td>43</td>
      <td>0.450873</td>
      <td>1.017313</td>
      <td>01:10</td>
    </tr>
    <tr>
      <td>44</td>
      <td>0.472663</td>
      <td>0.879766</td>
      <td>01:10</td>
    </tr>
    <tr>
      <td>45</td>
      <td>0.461158</td>
      <td>1.111854</td>
      <td>01:10</td>
    </tr>
    <tr>
      <td>46</td>
      <td>0.500590</td>
      <td>0.847977</td>
      <td>01:10</td>
    </tr>
    <tr>
      <td>47</td>
      <td>0.445661</td>
      <td>0.944846</td>
      <td>01:10</td>
    </tr>
    <tr>
      <td>48</td>
      <td>0.438157</td>
      <td>1.182034</td>
      <td>01:10</td>
    </tr>
    <tr>
      <td>49</td>
      <td>0.458161</td>
      <td>0.808696</td>
      <td>01:10</td>
    </tr>
    <tr>
      <td>50</td>
      <td>0.449481</td>
      <td>0.961311</td>
      <td>01:10</td>
    </tr>
    <tr>
      <td>51</td>
      <td>0.474829</td>
      <td>1.112345</td>
      <td>01:10</td>
    </tr>
    <tr>
      <td>52</td>
      <td>0.459445</td>
      <td>1.128703</td>
      <td>01:10</td>
    </tr>
    <tr>
      <td>53</td>
      <td>0.466031</td>
      <td>1.079201</td>
      <td>01:10</td>
    </tr>
    <tr>
      <td>54</td>
      <td>0.465187</td>
      <td>0.928735</td>
      <td>01:10</td>
    </tr>
    <tr>
      <td>55</td>
      <td>0.448427</td>
      <td>1.057908</td>
      <td>01:10</td>
    </tr>
    <tr>
      <td>56</td>
      <td>0.462586</td>
      <td>1.151867</td>
      <td>01:10</td>
    </tr>
    <tr>
      <td>57</td>
      <td>0.486594</td>
      <td>0.992354</td>
      <td>01:10</td>
    </tr>
    <tr>
      <td>58</td>
      <td>0.454556</td>
      <td>1.144159</td>
      <td>01:10</td>
    </tr>
    <tr>
      <td>59</td>
      <td>0.429522</td>
      <td>1.273321</td>
      <td>01:10</td>
    </tr>
    <tr>
      <td>60</td>
      <td>0.461596</td>
      <td>1.156239</td>
      <td>01:10</td>
    </tr>
    <tr>
      <td>61</td>
      <td>0.481741</td>
      <td>0.853044</td>
      <td>01:10</td>
    </tr>
    <tr>
      <td>62</td>
      <td>0.487350</td>
      <td>1.002314</td>
      <td>01:10</td>
    </tr>
    <tr>
      <td>63</td>
      <td>0.462871</td>
      <td>0.920023</td>
      <td>01:10</td>
    </tr>
    <tr>
      <td>64</td>
      <td>0.427491</td>
      <td>1.120978</td>
      <td>01:11</td>
    </tr>
    <tr>
      <td>65</td>
      <td>0.452336</td>
      <td>0.958580</td>
      <td>01:11</td>
    </tr>
    <tr>
      <td>66</td>
      <td>0.445774</td>
      <td>0.965166</td>
      <td>01:11</td>
    </tr>
    <tr>
      <td>67</td>
      <td>0.459143</td>
      <td>1.159150</td>
      <td>01:11</td>
    </tr>
    <tr>
      <td>68</td>
      <td>0.413370</td>
      <td>0.986202</td>
      <td>01:11</td>
    </tr>
    <tr>
      <td>69</td>
      <td>0.427073</td>
      <td>1.044361</td>
      <td>01:11</td>
    </tr>
    <tr>
      <td>70</td>
      <td>0.444376</td>
      <td>0.997694</td>
      <td>01:11</td>
    </tr>
    <tr>
      <td>71</td>
      <td>0.430188</td>
      <td>0.833335</td>
      <td>01:11</td>
    </tr>
    <tr>
      <td>72</td>
      <td>0.429365</td>
      <td>0.874525</td>
      <td>01:10</td>
    </tr>
    <tr>
      <td>73</td>
      <td>0.402249</td>
      <td>1.220277</td>
      <td>01:10</td>
    </tr>
    <tr>
      <td>74</td>
      <td>0.427581</td>
      <td>0.872484</td>
      <td>01:10</td>
    </tr>
    <tr>
      <td>75</td>
      <td>0.437532</td>
      <td>1.096869</td>
      <td>01:10</td>
    </tr>
    <tr>
      <td>76</td>
      <td>0.416637</td>
      <td>1.067388</td>
      <td>01:10</td>
    </tr>
    <tr>
      <td>77</td>
      <td>0.442739</td>
      <td>1.133278</td>
      <td>01:10</td>
    </tr>
    <tr>
      <td>78</td>
      <td>0.432741</td>
      <td>0.942487</td>
      <td>01:10</td>
    </tr>
    <tr>
      <td>79</td>
      <td>0.408399</td>
      <td>0.934233</td>
      <td>01:10</td>
    </tr>
    <tr>
      <td>80</td>
      <td>0.444865</td>
      <td>1.118692</td>
      <td>01:10</td>
    </tr>
    <tr>
      <td>81</td>
      <td>0.428840</td>
      <td>1.014150</td>
      <td>01:10</td>
    </tr>
    <tr>
      <td>82</td>
      <td>0.460565</td>
      <td>1.034080</td>
      <td>01:11</td>
    </tr>
    <tr>
      <td>83</td>
      <td>0.457846</td>
      <td>0.937802</td>
      <td>01:11</td>
    </tr>
    <tr>
      <td>84</td>
      <td>0.408253</td>
      <td>1.139460</td>
      <td>01:10</td>
    </tr>
    <tr>
      <td>85</td>
      <td>0.415102</td>
      <td>0.955354</td>
      <td>01:11</td>
    </tr>
    <tr>
      <td>86</td>
      <td>0.397947</td>
      <td>1.177948</td>
      <td>01:10</td>
    </tr>
    <tr>
      <td>87</td>
      <td>0.397674</td>
      <td>0.877790</td>
      <td>01:10</td>
    </tr>
    <tr>
      <td>88</td>
      <td>0.394516</td>
      <td>0.893461</td>
      <td>01:10</td>
    </tr>
    <tr>
      <td>89</td>
      <td>0.421651</td>
      <td>1.176352</td>
      <td>01:10</td>
    </tr>
    <tr>
      <td>90</td>
      <td>0.393867</td>
      <td>0.993269</td>
      <td>01:10</td>
    </tr>
    <tr>
      <td>91</td>
      <td>0.395941</td>
      <td>1.033166</td>
      <td>01:10</td>
    </tr>
    <tr>
      <td>92</td>
      <td>0.420890</td>
      <td>0.983403</td>
      <td>01:10</td>
    </tr>
    <tr>
      <td>93</td>
      <td>0.425763</td>
      <td>1.046506</td>
      <td>01:10</td>
    </tr>
    <tr>
      <td>94</td>
      <td>0.393211</td>
      <td>0.881651</td>
      <td>01:10</td>
    </tr>
    <tr>
      <td>95</td>
      <td>0.433522</td>
      <td>0.624557</td>
      <td>01:10</td>
    </tr>
    <tr>
      <td>96</td>
      <td>0.416764</td>
      <td>0.899551</td>
      <td>01:10</td>
    </tr>
    <tr>
      <td>97</td>
      <td>0.414456</td>
      <td>0.808962</td>
      <td>01:10</td>
    </tr>
    <tr>
      <td>98</td>
      <td>0.443335</td>
      <td>1.000195</td>
      <td>01:10</td>
    </tr>
    <tr>
      <td>99</td>
      <td>0.431998</td>
      <td>1.095366</td>
      <td>01:10</td>
    </tr>
  </tbody>
</table>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">save_name</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;byol_iwang_sz</span><span class="si">{</span><span class="n">size</span><span class="si">}</span><span class="s1">_epc200&#39;</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learn</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">save_name</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">learn</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="n">learn</span><span class="o">.</span><span class="n">path</span><span class="o">/</span><span class="n">learn</span><span class="o">.</span><span class="n">model_dir</span><span class="o">/</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">save_name</span><span class="si">}</span><span class="s1">_encoder.pth&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="3.-Downstream-Task---Image-Classification">3. Downstream Task - Image Classification<a class="anchor-link" href="#3.-Downstream-Task---Image-Classification"> </a></h2>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">get_dls</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">bs</span><span class="p">,</span> <span class="n">workers</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">path</span> <span class="o">=</span> <span class="n">URLs</span><span class="o">.</span><span class="n">IMAGEWANG_160</span> <span class="k">if</span> <span class="n">size</span> <span class="o">&lt;=</span> <span class="mi">160</span> <span class="k">else</span> <span class="n">URLs</span><span class="o">.</span><span class="n">IMAGEWANG</span>
    <span class="n">source</span> <span class="o">=</span> <span class="n">untar_data</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>
    <span class="n">files</span> <span class="o">=</span> <span class="n">get_image_files</span><span class="p">(</span><span class="n">source</span><span class="p">,</span> <span class="n">folders</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;train&#39;</span><span class="p">,</span> <span class="s1">&#39;val&#39;</span><span class="p">])</span>
    <span class="n">splits</span> <span class="o">=</span> <span class="n">GrandparentSplitter</span><span class="p">(</span><span class="n">valid_name</span><span class="o">=</span><span class="s1">&#39;val&#39;</span><span class="p">)(</span><span class="n">files</span><span class="p">)</span>
    
    <span class="n">item_aug</span> <span class="o">=</span> <span class="p">[</span><span class="n">RandomResizedCrop</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">min_scale</span><span class="o">=</span><span class="mf">0.35</span><span class="p">),</span> <span class="n">FlipItem</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)]</span>
    <span class="n">tfms</span> <span class="o">=</span> <span class="p">[[</span><span class="n">PILImage</span><span class="o">.</span><span class="n">create</span><span class="p">,</span> <span class="n">ToTensor</span><span class="p">,</span> <span class="o">*</span><span class="n">item_aug</span><span class="p">],</span> 
            <span class="p">[</span><span class="n">parent_label</span><span class="p">,</span> <span class="n">Categorize</span><span class="p">()]]</span>
    
    <span class="n">dsets</span> <span class="o">=</span> <span class="n">Datasets</span><span class="p">(</span><span class="n">files</span><span class="p">,</span> <span class="n">tfms</span><span class="o">=</span><span class="n">tfms</span><span class="p">,</span> <span class="n">splits</span><span class="o">=</span><span class="n">splits</span><span class="p">)</span>
    
    <span class="n">batch_tfms</span> <span class="o">=</span> <span class="p">[</span><span class="n">IntToFloatTensor</span><span class="p">,</span> <span class="n">Normalize</span><span class="o">.</span><span class="n">from_stats</span><span class="p">(</span><span class="o">*</span><span class="n">imagenet_stats</span><span class="p">)]</span>
    <span class="n">dls</span> <span class="o">=</span> <span class="n">dsets</span><span class="o">.</span><span class="n">dataloaders</span><span class="p">(</span><span class="n">bs</span><span class="o">=</span><span class="n">bs</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="n">workers</span><span class="p">,</span> <span class="n">after_batch</span><span class="o">=</span><span class="n">batch_tfms</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">dls</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">do_train</span><span class="p">(</span><span class="n">epochs</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">runs</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">size</span><span class="p">,</span> <span class="n">bs</span><span class="o">=</span><span class="n">bs</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span> <span class="n">save_name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">dls</span> <span class="o">=</span> <span class="n">get_dls</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">bs</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">run</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">runs</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Run: </span><span class="si">{</span><span class="n">run</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
        <span class="n">learn</span> <span class="o">=</span> <span class="n">cnn_learner</span><span class="p">(</span><span class="n">dls</span><span class="p">,</span> <span class="n">xresnet34</span><span class="p">,</span> <span class="n">opt_func</span><span class="o">=</span><span class="n">opt_func</span><span class="p">,</span> <span class="n">normalize</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="n">accuracy</span><span class="p">,</span><span class="n">top_k_accuracy</span><span class="p">],</span> <span class="n">loss_func</span><span class="o">=</span><span class="n">LabelSmoothingCrossEntropy</span><span class="p">(),</span>
                <span class="n">pretrained</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">save_name</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">state_dict</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">learn</span><span class="o">.</span><span class="n">path</span><span class="o">/</span><span class="n">learn</span><span class="o">.</span><span class="n">model_dir</span><span class="o">/</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">save_name</span><span class="si">}</span><span class="s1">_encoder.pth&#39;</span><span class="p">)</span>
            <span class="n">learn</span><span class="o">.</span><span class="n">model</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">state_dict</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Model loaded...&quot;</span><span class="p">)</span>
            
        <span class="n">learn</span><span class="o">.</span><span class="n">unfreeze</span><span class="p">()</span>
        <span class="n">learn</span><span class="o">.</span><span class="n">fit_flat_cos</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">wd</span><span class="o">=</span><span class="n">wd</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">save_name</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>&#39;byol_iwang_sz128_epc200&#39;</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="ImageWang-Leaderboard">ImageWang Leaderboard<a class="anchor-link" href="#ImageWang-Leaderboard"> </a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>sz-128</strong></p>
<p><strong>Note:</strong> Below scores of Inpainting and Contrastive Learning is taken from PRs waiting as of running this notebook.</p>
<p><strong>Inpainting</strong></p>
<ul>
<li>5 epochs: 34.72%</li>
<li>20 epochs: 59.02%</li>
<li>80 epochs: 61.85%</li>
<li>200 epochs: 60.22%</li>
</ul>
<p><strong>Contrastive Learning</strong></p>
<ul>
<li>5 epochs: 50.78%</li>
</ul>
<p><strong>BYOL</strong></p>
<ul>
<li>5 epochs: <strong>58.74%</strong></li>
<li>20 epochs: <strong>65.29%</strong></li>
<li>80 epochs: <strong>63.98%</strong></li>
<li>200 epochs: <strong>64.44%</strong></li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="5-epochs">5 epochs<a class="anchor-link" href="#5-epochs"> </a></h3>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">lr</span> <span class="o">=</span> <span class="mf">1e-2</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">runs</span> <span class="o">=</span> <span class="mi">5</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">do_train</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">runs</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span> <span class="n">save_name</span><span class="o">=</span><span class="n">save_name</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Run: 0
Model loaded...
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>accuracy</th>
      <th>top_k_accuracy</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>1.739300</td>
      <td>2.432013</td>
      <td>0.346144</td>
      <td>0.764317</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>1</td>
      <td>1.311364</td>
      <td>1.984878</td>
      <td>0.463731</td>
      <td>0.851871</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>2</td>
      <td>1.138864</td>
      <td>2.390061</td>
      <td>0.350725</td>
      <td>0.738610</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>3</td>
      <td>1.055521</td>
      <td>1.903824</td>
      <td>0.515907</td>
      <td>0.879868</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>4</td>
      <td>0.968297</td>
      <td>1.699759</td>
      <td>0.587427</td>
      <td>0.907101</td>
      <td>00:19</td>
    </tr>
  </tbody>
</table>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Run: 1
Model loaded...
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>accuracy</th>
      <th>top_k_accuracy</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>1.735486</td>
      <td>2.291717</td>
      <td>0.363706</td>
      <td>0.787732</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>1</td>
      <td>1.314700</td>
      <td>2.034063</td>
      <td>0.447188</td>
      <td>0.852634</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>2</td>
      <td>1.136116</td>
      <td>2.391880</td>
      <td>0.350216</td>
      <td>0.735302</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>3</td>
      <td>1.047174</td>
      <td>1.878020</td>
      <td>0.511072</td>
      <td>0.869941</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>4</td>
      <td>0.952817</td>
      <td>1.689037</td>
      <td>0.589972</td>
      <td>0.912191</td>
      <td>00:20</td>
    </tr>
  </tbody>
</table>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Run: 2
Model loaded...
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>accuracy</th>
      <th>top_k_accuracy</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>1.769073</td>
      <td>2.719925</td>
      <td>0.272334</td>
      <td>0.666836</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>1</td>
      <td>1.316096</td>
      <td>2.099287</td>
      <td>0.440570</td>
      <td>0.843981</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>2</td>
      <td>1.133772</td>
      <td>1.988103</td>
      <td>0.487656</td>
      <td>0.874777</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>3</td>
      <td>1.047194</td>
      <td>1.979267</td>
      <td>0.495546</td>
      <td>0.847798</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>4</td>
      <td>0.956003</td>
      <td>1.717500</td>
      <td>0.585136</td>
      <td>0.905574</td>
      <td>00:20</td>
    </tr>
  </tbody>
</table>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Run: 3
Model loaded...
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>accuracy</th>
      <th>top_k_accuracy</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>1.702767</td>
      <td>2.337030</td>
      <td>0.332909</td>
      <td>0.811911</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>1</td>
      <td>1.303057</td>
      <td>2.017970</td>
      <td>0.447442</td>
      <td>0.855179</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>2</td>
      <td>1.135067</td>
      <td>2.272993</td>
      <td>0.378722</td>
      <td>0.845253</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>3</td>
      <td>1.068153</td>
      <td>1.857333</td>
      <td>0.532196</td>
      <td>0.883685</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>4</td>
      <td>0.976920</td>
      <td>1.712661</td>
      <td>0.581064</td>
      <td>0.899720</td>
      <td>00:20</td>
    </tr>
  </tbody>
</table>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Run: 4
Model loaded...
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>accuracy</th>
      <th>top_k_accuracy</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>1.754177</td>
      <td>2.464094</td>
      <td>0.351743</td>
      <td>0.762789</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>1</td>
      <td>1.317701</td>
      <td>2.013224</td>
      <td>0.470349</td>
      <td>0.857216</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>2</td>
      <td>1.139273</td>
      <td>2.083957</td>
      <td>0.441588</td>
      <td>0.847544</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>3</td>
      <td>1.047227</td>
      <td>1.955609</td>
      <td>0.490456</td>
      <td>0.861288</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>4</td>
      <td>0.961456</td>
      <td>1.722694</td>
      <td>0.576483</td>
      <td>0.907101</td>
      <td>00:20</td>
    </tr>
  </tbody>
</table>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">([</span><span class="mf">0.587427</span><span class="p">,</span><span class="mf">0.589972</span><span class="p">,</span><span class="mf">0.585136</span><span class="p">,</span><span class="mf">0.581064</span><span class="p">,</span><span class="mf">0.576483</span><span class="p">])</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>0.5840164000000001</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="20-epochs">20 epochs<a class="anchor-link" href="#20-epochs"> </a></h3>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">epochs</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">runs</span> <span class="o">=</span> <span class="mi">3</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">do_train</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">runs</span><span class="p">,</span> <span class="n">save_name</span><span class="o">=</span><span class="n">save_name</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Run: 0
Model loaded...
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>accuracy</th>
      <th>top_k_accuracy</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>1.806156</td>
      <td>2.471681</td>
      <td>0.408755</td>
      <td>0.848053</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>1</td>
      <td>1.348460</td>
      <td>1.887141</td>
      <td>0.511581</td>
      <td>0.871214</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>2</td>
      <td>1.147970</td>
      <td>1.889463</td>
      <td>0.513108</td>
      <td>0.885722</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>3</td>
      <td>1.037374</td>
      <td>1.675355</td>
      <td>0.596589</td>
      <td>0.920590</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>4</td>
      <td>0.974181</td>
      <td>1.825452</td>
      <td>0.543904</td>
      <td>0.889285</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>5</td>
      <td>0.930011</td>
      <td>1.788609</td>
      <td>0.565538</td>
      <td>0.902520</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>6</td>
      <td>0.894337</td>
      <td>2.260109</td>
      <td>0.421990</td>
      <td>0.826164</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>7</td>
      <td>0.871413</td>
      <td>1.747183</td>
      <td>0.586409</td>
      <td>0.897684</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>8</td>
      <td>0.850541</td>
      <td>1.873370</td>
      <td>0.516162</td>
      <td>0.879359</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>9</td>
      <td>0.834717</td>
      <td>1.717904</td>
      <td>0.606516</td>
      <td>0.892848</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>10</td>
      <td>0.820474</td>
      <td>1.798465</td>
      <td>0.571392</td>
      <td>0.895393</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>11</td>
      <td>0.809743</td>
      <td>1.808281</td>
      <td>0.575210</td>
      <td>0.888776</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>12</td>
      <td>0.795057</td>
      <td>1.737137</td>
      <td>0.599135</td>
      <td>0.900229</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>13</td>
      <td>0.781921</td>
      <td>1.638976</td>
      <td>0.627895</td>
      <td>0.912955</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>14</td>
      <td>0.772834</td>
      <td>1.902040</td>
      <td>0.553576</td>
      <td>0.857979</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>15</td>
      <td>0.762199</td>
      <td>1.756756</td>
      <td>0.603716</td>
      <td>0.893866</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>16</td>
      <td>0.737692</td>
      <td>1.751376</td>
      <td>0.608043</td>
      <td>0.903029</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>17</td>
      <td>0.718452</td>
      <td>1.598723</td>
      <td>0.648511</td>
      <td>0.918554</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>18</td>
      <td>0.694353</td>
      <td>1.581865</td>
      <td>0.652329</td>
      <td>0.922881</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>19</td>
      <td>0.674612</td>
      <td>1.583227</td>
      <td>0.655129</td>
      <td>0.921609</td>
      <td>00:20</td>
    </tr>
  </tbody>
</table>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Run: 1
Model loaded...
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>accuracy</th>
      <th>top_k_accuracy</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>1.803586</td>
      <td>2.445106</td>
      <td>0.369814</td>
      <td>0.781369</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>1</td>
      <td>1.342556</td>
      <td>1.868932</td>
      <td>0.513871</td>
      <td>0.873759</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>2</td>
      <td>1.145308</td>
      <td>1.926031</td>
      <td>0.483584</td>
      <td>0.871214</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>3</td>
      <td>1.034798</td>
      <td>1.697215</td>
      <td>0.586154</td>
      <td>0.902011</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>4</td>
      <td>0.976012</td>
      <td>2.079163</td>
      <td>0.477984</td>
      <td>0.880377</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>5</td>
      <td>0.932562</td>
      <td>1.639592</td>
      <td>0.613133</td>
      <td>0.907610</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>6</td>
      <td>0.904484</td>
      <td>1.904785</td>
      <td>0.509035</td>
      <td>0.881395</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>7</td>
      <td>0.881797</td>
      <td>1.772912</td>
      <td>0.572919</td>
      <td>0.900484</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>8</td>
      <td>0.857128</td>
      <td>1.638690</td>
      <td>0.607534</td>
      <td>0.910919</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>9</td>
      <td>0.844552</td>
      <td>1.699906</td>
      <td>0.597353</td>
      <td>0.912955</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>10</td>
      <td>0.826218</td>
      <td>1.835046</td>
      <td>0.536778</td>
      <td>0.902520</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>11</td>
      <td>0.808420</td>
      <td>1.735711</td>
      <td>0.599644</td>
      <td>0.895902</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>12</td>
      <td>0.798396</td>
      <td>1.785647</td>
      <td>0.575464</td>
      <td>0.903283</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>13</td>
      <td>0.787404</td>
      <td>1.639556</td>
      <td>0.631713</td>
      <td>0.911937</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>14</td>
      <td>0.774844</td>
      <td>1.915343</td>
      <td>0.551285</td>
      <td>0.888521</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>15</td>
      <td>0.753809</td>
      <td>1.663328</td>
      <td>0.626114</td>
      <td>0.913973</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>16</td>
      <td>0.737287</td>
      <td>1.674808</td>
      <td>0.636803</td>
      <td>0.912191</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>17</td>
      <td>0.712166</td>
      <td>1.645749</td>
      <td>0.645202</td>
      <td>0.905828</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>18</td>
      <td>0.689676</td>
      <td>1.603068</td>
      <td>0.659964</td>
      <td>0.916009</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>19</td>
      <td>0.679122</td>
      <td>1.578649</td>
      <td>0.664291</td>
      <td>0.916264</td>
      <td>00:19</td>
    </tr>
  </tbody>
</table>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Run: 2
Model loaded...
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>accuracy</th>
      <th>top_k_accuracy</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>1.763868</td>
      <td>2.454399</td>
      <td>0.374650</td>
      <td>0.822856</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>1</td>
      <td>1.327703</td>
      <td>1.870471</td>
      <td>0.529142</td>
      <td>0.875286</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>2</td>
      <td>1.143383</td>
      <td>1.877321</td>
      <td>0.508781</td>
      <td>0.898447</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>3</td>
      <td>1.037422</td>
      <td>1.760017</td>
      <td>0.563757</td>
      <td>0.901502</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>4</td>
      <td>0.972350</td>
      <td>1.921528</td>
      <td>0.518707</td>
      <td>0.877577</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>5</td>
      <td>0.940332</td>
      <td>1.754828</td>
      <td>0.580809</td>
      <td>0.899211</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>6</td>
      <td>0.898525</td>
      <td>1.915463</td>
      <td>0.528124</td>
      <td>0.905828</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>7</td>
      <td>0.872683</td>
      <td>1.732969</td>
      <td>0.580300</td>
      <td>0.897429</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>8</td>
      <td>0.849111</td>
      <td>1.677999</td>
      <td>0.607279</td>
      <td>0.912191</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>9</td>
      <td>0.836579</td>
      <td>1.724234</td>
      <td>0.601680</td>
      <td>0.905828</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>10</td>
      <td>0.827159</td>
      <td>1.742313</td>
      <td>0.584882</td>
      <td>0.892848</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>11</td>
      <td>0.814334</td>
      <td>1.748212</td>
      <td>0.595062</td>
      <td>0.899211</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>12</td>
      <td>0.797577</td>
      <td>1.833041</td>
      <td>0.575210</td>
      <td>0.876813</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>13</td>
      <td>0.787613</td>
      <td>1.777615</td>
      <td>0.586663</td>
      <td>0.890812</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>14</td>
      <td>0.777568</td>
      <td>2.144128</td>
      <td>0.522016</td>
      <td>0.871978</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>15</td>
      <td>0.766376</td>
      <td>1.712796</td>
      <td>0.612115</td>
      <td>0.902011</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>16</td>
      <td>0.746362</td>
      <td>1.726560</td>
      <td>0.606261</td>
      <td>0.890812</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>17</td>
      <td>0.719569</td>
      <td>1.701906</td>
      <td>0.617205</td>
      <td>0.907101</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>18</td>
      <td>0.689579</td>
      <td>1.646178</td>
      <td>0.635785</td>
      <td>0.904810</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>19</td>
      <td>0.673921</td>
      <td>1.635748</td>
      <td>0.639348</td>
      <td>0.909646</td>
      <td>00:20</td>
    </tr>
  </tbody>
</table>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">([</span><span class="mf">0.655129</span><span class="p">,</span> <span class="mf">0.664291</span><span class="p">,</span> <span class="mf">0.639348</span><span class="p">])</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>0.6529226666666667</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="80-epochs">80 epochs<a class="anchor-link" href="#80-epochs"> </a></h3>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">epochs</span> <span class="o">=</span> <span class="mi">80</span>
<span class="n">runs</span> <span class="o">=</span> <span class="mi">1</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">do_train</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">runs</span><span class="p">,</span> <span class="n">save_name</span><span class="o">=</span><span class="n">save_name</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Run: 0
Model loaded...
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>accuracy</th>
      <th>top_k_accuracy</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>1.805516</td>
      <td>2.539602</td>
      <td>0.385340</td>
      <td>0.830491</td>
      <td>00:21</td>
    </tr>
    <tr>
      <td>1</td>
      <td>1.350613</td>
      <td>1.925428</td>
      <td>0.501909</td>
      <td>0.865869</td>
      <td>00:21</td>
    </tr>
    <tr>
      <td>2</td>
      <td>1.153064</td>
      <td>2.091336</td>
      <td>0.465513</td>
      <td>0.828710</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>3</td>
      <td>1.038459</td>
      <td>1.807745</td>
      <td>0.540596</td>
      <td>0.910155</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>4</td>
      <td>0.976143</td>
      <td>1.877089</td>
      <td>0.526088</td>
      <td>0.876813</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>5</td>
      <td>0.941280</td>
      <td>1.715550</td>
      <td>0.584627</td>
      <td>0.899466</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>6</td>
      <td>0.906941</td>
      <td>1.929877</td>
      <td>0.522525</td>
      <td>0.872487</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>7</td>
      <td>0.872006</td>
      <td>1.667068</td>
      <td>0.610842</td>
      <td>0.909137</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>8</td>
      <td>0.859712</td>
      <td>1.854542</td>
      <td>0.552049</td>
      <td>0.871978</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>9</td>
      <td>0.848755</td>
      <td>1.698098</td>
      <td>0.592008</td>
      <td>0.911173</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>10</td>
      <td>0.829552</td>
      <td>1.860939</td>
      <td>0.555612</td>
      <td>0.889285</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>11</td>
      <td>0.816118</td>
      <td>1.719802</td>
      <td>0.595571</td>
      <td>0.911428</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>12</td>
      <td>0.800338</td>
      <td>1.758544</td>
      <td>0.596844</td>
      <td>0.904556</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>13</td>
      <td>0.784567</td>
      <td>1.668902</td>
      <td>0.619496</td>
      <td>0.906083</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>14</td>
      <td>0.775867</td>
      <td>2.008516</td>
      <td>0.548740</td>
      <td>0.884449</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>15</td>
      <td>0.768302</td>
      <td>1.751890</td>
      <td>0.596589</td>
      <td>0.897429</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>16</td>
      <td>0.755189</td>
      <td>1.941873</td>
      <td>0.558921</td>
      <td>0.867142</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>17</td>
      <td>0.749353</td>
      <td>1.795056</td>
      <td>0.578519</td>
      <td>0.901247</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>18</td>
      <td>0.742762</td>
      <td>1.864902</td>
      <td>0.568592</td>
      <td>0.882158</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>19</td>
      <td>0.736773</td>
      <td>1.659408</td>
      <td>0.634004</td>
      <td>0.907101</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>20</td>
      <td>0.729762</td>
      <td>1.881632</td>
      <td>0.565793</td>
      <td>0.880631</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>21</td>
      <td>0.724415</td>
      <td>1.852578</td>
      <td>0.589718</td>
      <td>0.876304</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>22</td>
      <td>0.718456</td>
      <td>1.901825</td>
      <td>0.561975</td>
      <td>0.868669</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>23</td>
      <td>0.714878</td>
      <td>1.805974</td>
      <td>0.598626</td>
      <td>0.889794</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>24</td>
      <td>0.707485</td>
      <td>1.914376</td>
      <td>0.572410</td>
      <td>0.866887</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>25</td>
      <td>0.710242</td>
      <td>1.617307</td>
      <td>0.647748</td>
      <td>0.905319</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>26</td>
      <td>0.705949</td>
      <td>1.981145</td>
      <td>0.559939</td>
      <td>0.863069</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>27</td>
      <td>0.699580</td>
      <td>1.763338</td>
      <td>0.618223</td>
      <td>0.889285</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>28</td>
      <td>0.694289</td>
      <td>1.949060</td>
      <td>0.565284</td>
      <td>0.875032</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>29</td>
      <td>0.693415</td>
      <td>1.690055</td>
      <td>0.629931</td>
      <td>0.903029</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>30</td>
      <td>0.690179</td>
      <td>1.887315</td>
      <td>0.580555</td>
      <td>0.867651</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>31</td>
      <td>0.689409</td>
      <td>1.823779</td>
      <td>0.603207</td>
      <td>0.874777</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>32</td>
      <td>0.684529</td>
      <td>1.737365</td>
      <td>0.614915</td>
      <td>0.888267</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>33</td>
      <td>0.679852</td>
      <td>1.806477</td>
      <td>0.609061</td>
      <td>0.881140</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>34</td>
      <td>0.677441</td>
      <td>1.721731</td>
      <td>0.625350</td>
      <td>0.900993</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>35</td>
      <td>0.682127</td>
      <td>1.790280</td>
      <td>0.618733</td>
      <td>0.891066</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>36</td>
      <td>0.677045</td>
      <td>1.846383</td>
      <td>0.594299</td>
      <td>0.882667</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>37</td>
      <td>0.676579</td>
      <td>1.848590</td>
      <td>0.593281</td>
      <td>0.881140</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>38</td>
      <td>0.673064</td>
      <td>2.079417</td>
      <td>0.534996</td>
      <td>0.839399</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>39</td>
      <td>0.673679</td>
      <td>1.835063</td>
      <td>0.612370</td>
      <td>0.878850</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>40</td>
      <td>0.674616</td>
      <td>1.706696</td>
      <td>0.631967</td>
      <td>0.893866</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>41</td>
      <td>0.668277</td>
      <td>1.806029</td>
      <td>0.610842</td>
      <td>0.883940</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>42</td>
      <td>0.666215</td>
      <td>1.850944</td>
      <td>0.598117</td>
      <td>0.876050</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>43</td>
      <td>0.664985</td>
      <td>1.807673</td>
      <td>0.601934</td>
      <td>0.886485</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>44</td>
      <td>0.666493</td>
      <td>1.829504</td>
      <td>0.607279</td>
      <td>0.873759</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>45</td>
      <td>0.662585</td>
      <td>1.892237</td>
      <td>0.589972</td>
      <td>0.860270</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>46</td>
      <td>0.662220</td>
      <td>1.807784</td>
      <td>0.604479</td>
      <td>0.880377</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>47</td>
      <td>0.661277</td>
      <td>1.898638</td>
      <td>0.584118</td>
      <td>0.876813</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>48</td>
      <td>0.663680</td>
      <td>2.076900</td>
      <td>0.549249</td>
      <td>0.839654</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>49</td>
      <td>0.658001</td>
      <td>1.813951</td>
      <td>0.612115</td>
      <td>0.879104</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>50</td>
      <td>0.655565</td>
      <td>1.921036</td>
      <td>0.587681</td>
      <td>0.863069</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>51</td>
      <td>0.657130</td>
      <td>1.953669</td>
      <td>0.587172</td>
      <td>0.859761</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>52</td>
      <td>0.653731</td>
      <td>1.843616</td>
      <td>0.601934</td>
      <td>0.879359</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>53</td>
      <td>0.654839</td>
      <td>1.932282</td>
      <td>0.584627</td>
      <td>0.865615</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>54</td>
      <td>0.655050</td>
      <td>1.860253</td>
      <td>0.593790</td>
      <td>0.874523</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>55</td>
      <td>0.652670</td>
      <td>1.858571</td>
      <td>0.604225</td>
      <td>0.872741</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>56</td>
      <td>0.656193</td>
      <td>1.913682</td>
      <td>0.573937</td>
      <td>0.861033</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>57</td>
      <td>0.653525</td>
      <td>1.798937</td>
      <td>0.612879</td>
      <td>0.885213</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>58</td>
      <td>0.650326</td>
      <td>1.981321</td>
      <td>0.560702</td>
      <td>0.858488</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>59</td>
      <td>0.650744</td>
      <td>1.849183</td>
      <td>0.596844</td>
      <td>0.871469</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>60</td>
      <td>0.647394</td>
      <td>2.014749</td>
      <td>0.566047</td>
      <td>0.838890</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>61</td>
      <td>0.646208</td>
      <td>1.832178</td>
      <td>0.614660</td>
      <td>0.884194</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>62</td>
      <td>0.642630</td>
      <td>1.933835</td>
      <td>0.581064</td>
      <td>0.870705</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>63</td>
      <td>0.644698</td>
      <td>1.902367</td>
      <td>0.597353</td>
      <td>0.875795</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>64</td>
      <td>0.643769</td>
      <td>1.886313</td>
      <td>0.599389</td>
      <td>0.862815</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>65</td>
      <td>0.639762</td>
      <td>1.928158</td>
      <td>0.581064</td>
      <td>0.866124</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>66</td>
      <td>0.638610</td>
      <td>1.998271</td>
      <td>0.574701</td>
      <td>0.865615</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>67</td>
      <td>0.634140</td>
      <td>1.831187</td>
      <td>0.608806</td>
      <td>0.886485</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>68</td>
      <td>0.630203</td>
      <td>1.841122</td>
      <td>0.616187</td>
      <td>0.878595</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>69</td>
      <td>0.627519</td>
      <td>1.782396</td>
      <td>0.629931</td>
      <td>0.889285</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>70</td>
      <td>0.624604</td>
      <td>1.823376</td>
      <td>0.622805</td>
      <td>0.877577</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>71</td>
      <td>0.622309</td>
      <td>1.773276</td>
      <td>0.632476</td>
      <td>0.895902</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>72</td>
      <td>0.620571</td>
      <td>1.733409</td>
      <td>0.641385</td>
      <td>0.895648</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>73</td>
      <td>0.619527</td>
      <td>1.721003</td>
      <td>0.640876</td>
      <td>0.896157</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>74</td>
      <td>0.616857</td>
      <td>1.732499</td>
      <td>0.638839</td>
      <td>0.894884</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>75</td>
      <td>0.615296</td>
      <td>1.719967</td>
      <td>0.643675</td>
      <td>0.900229</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>76</td>
      <td>0.613047</td>
      <td>1.714291</td>
      <td>0.640366</td>
      <td>0.900484</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>77</td>
      <td>0.612636</td>
      <td>1.723857</td>
      <td>0.642657</td>
      <td>0.896666</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>78</td>
      <td>0.612132</td>
      <td>1.712115</td>
      <td>0.643421</td>
      <td>0.899211</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>79</td>
      <td>0.612107</td>
      <td>1.733283</td>
      <td>0.639857</td>
      <td>0.895648</td>
      <td>00:19</td>
    </tr>
  </tbody>
</table>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="200-epochs">200 epochs<a class="anchor-link" href="#200-epochs"> </a></h3>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">epochs</span> <span class="o">=</span> <span class="mi">200</span>
<span class="n">runs</span> <span class="o">=</span> <span class="mi">1</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">do_train</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">runs</span><span class="p">,</span> <span class="n">save_name</span><span class="o">=</span><span class="n">save_name</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Run: 0
Model loaded...
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>accuracy</th>
      <th>top_k_accuracy</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>1.788447</td>
      <td>2.222363</td>
      <td>0.428353</td>
      <td>0.816747</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>1</td>
      <td>1.333959</td>
      <td>1.907482</td>
      <td>0.503690</td>
      <td>0.876559</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>2</td>
      <td>1.140998</td>
      <td>1.813898</td>
      <td>0.543904</td>
      <td>0.880377</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>3</td>
      <td>1.040098</td>
      <td>1.710489</td>
      <td>0.572156</td>
      <td>0.908119</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>4</td>
      <td>0.970557</td>
      <td>1.827150</td>
      <td>0.540087</td>
      <td>0.880631</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>5</td>
      <td>0.936841</td>
      <td>1.740689</td>
      <td>0.571647</td>
      <td>0.910919</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>6</td>
      <td>0.902657</td>
      <td>2.122650</td>
      <td>0.463731</td>
      <td>0.863324</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>7</td>
      <td>0.880133</td>
      <td>1.622014</td>
      <td>0.623059</td>
      <td>0.918045</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>8</td>
      <td>0.858296</td>
      <td>1.827521</td>
      <td>0.548995</td>
      <td>0.888776</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>9</td>
      <td>0.838976</td>
      <td>1.706643</td>
      <td>0.608297</td>
      <td>0.892848</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>10</td>
      <td>0.820468</td>
      <td>1.882023</td>
      <td>0.554849</td>
      <td>0.871469</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>11</td>
      <td>0.813076</td>
      <td>1.683047</td>
      <td>0.611861</td>
      <td>0.898956</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>12</td>
      <td>0.802138</td>
      <td>1.810165</td>
      <td>0.562739</td>
      <td>0.870705</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>13</td>
      <td>0.791036</td>
      <td>1.577172</td>
      <td>0.645202</td>
      <td>0.921863</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>14</td>
      <td>0.773915</td>
      <td>1.960244</td>
      <td>0.535760</td>
      <td>0.844490</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>15</td>
      <td>0.773038</td>
      <td>1.785423</td>
      <td>0.594553</td>
      <td>0.895648</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>16</td>
      <td>0.769214</td>
      <td>1.699421</td>
      <td>0.612370</td>
      <td>0.912955</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>17</td>
      <td>0.760999</td>
      <td>1.715210</td>
      <td>0.612624</td>
      <td>0.904301</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>18</td>
      <td>0.747662</td>
      <td>1.804498</td>
      <td>0.582591</td>
      <td>0.884703</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>19</td>
      <td>0.744253</td>
      <td>1.739144</td>
      <td>0.603970</td>
      <td>0.903029</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>20</td>
      <td>0.734699</td>
      <td>1.837028</td>
      <td>0.577246</td>
      <td>0.882158</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>21</td>
      <td>0.732611</td>
      <td>1.767073</td>
      <td>0.608552</td>
      <td>0.890812</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>22</td>
      <td>0.724325</td>
      <td>1.893778</td>
      <td>0.562230</td>
      <td>0.871214</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>23</td>
      <td>0.716781</td>
      <td>1.715179</td>
      <td>0.616442</td>
      <td>0.913464</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>24</td>
      <td>0.713026</td>
      <td>2.109119</td>
      <td>0.511072</td>
      <td>0.844744</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>25</td>
      <td>0.716128</td>
      <td>1.830011</td>
      <td>0.591245</td>
      <td>0.879868</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>26</td>
      <td>0.713188</td>
      <td>1.753691</td>
      <td>0.602443</td>
      <td>0.909901</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>27</td>
      <td>0.702400</td>
      <td>1.838896</td>
      <td>0.598626</td>
      <td>0.871469</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>28</td>
      <td>0.693524</td>
      <td>1.849681</td>
      <td>0.584373</td>
      <td>0.870196</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>29</td>
      <td>0.695242</td>
      <td>1.810225</td>
      <td>0.613897</td>
      <td>0.872487</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>30</td>
      <td>0.692123</td>
      <td>2.167989</td>
      <td>0.511072</td>
      <td>0.810639</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>31</td>
      <td>0.696890</td>
      <td>1.756600</td>
      <td>0.620005</td>
      <td>0.890812</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>32</td>
      <td>0.692692</td>
      <td>1.887212</td>
      <td>0.572410</td>
      <td>0.866633</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>33</td>
      <td>0.688161</td>
      <td>1.834672</td>
      <td>0.601171</td>
      <td>0.872487</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>34</td>
      <td>0.680691</td>
      <td>1.925852</td>
      <td>0.571138</td>
      <td>0.876559</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>35</td>
      <td>0.676987</td>
      <td>1.733503</td>
      <td>0.625859</td>
      <td>0.886740</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>36</td>
      <td>0.679311</td>
      <td>1.975125</td>
      <td>0.560193</td>
      <td>0.857470</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>37</td>
      <td>0.682201</td>
      <td>1.806882</td>
      <td>0.604225</td>
      <td>0.886994</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>38</td>
      <td>0.676366</td>
      <td>1.882509</td>
      <td>0.586918</td>
      <td>0.852889</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>39</td>
      <td>0.672527</td>
      <td>1.817231</td>
      <td>0.609570</td>
      <td>0.886231</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>40</td>
      <td>0.671382</td>
      <td>1.828146</td>
      <td>0.602952</td>
      <td>0.874014</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>41</td>
      <td>0.672672</td>
      <td>1.882340</td>
      <td>0.592263</td>
      <td>0.883431</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>42</td>
      <td>0.664628</td>
      <td>1.882548</td>
      <td>0.587936</td>
      <td>0.871978</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>43</td>
      <td>0.666581</td>
      <td>1.792534</td>
      <td>0.616696</td>
      <td>0.892084</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>44</td>
      <td>0.667866</td>
      <td>1.881686</td>
      <td>0.599644</td>
      <td>0.877322</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>45</td>
      <td>0.665559</td>
      <td>1.794605</td>
      <td>0.619751</td>
      <td>0.875795</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>46</td>
      <td>0.664640</td>
      <td>1.800795</td>
      <td>0.601934</td>
      <td>0.891321</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>47</td>
      <td>0.662976</td>
      <td>1.797163</td>
      <td>0.612115</td>
      <td>0.882413</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>48</td>
      <td>0.663646</td>
      <td>1.923095</td>
      <td>0.576992</td>
      <td>0.870450</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>49</td>
      <td>0.659030</td>
      <td>1.851395</td>
      <td>0.604989</td>
      <td>0.879613</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>50</td>
      <td>0.656526</td>
      <td>2.037882</td>
      <td>0.559684</td>
      <td>0.848817</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>51</td>
      <td>0.658094</td>
      <td>1.827323</td>
      <td>0.612370</td>
      <td>0.887758</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>52</td>
      <td>0.655273</td>
      <td>2.099701</td>
      <td>0.536269</td>
      <td>0.834563</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>53</td>
      <td>0.657230</td>
      <td>1.754815</td>
      <td>0.623568</td>
      <td>0.895393</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>54</td>
      <td>0.653337</td>
      <td>1.913960</td>
      <td>0.573174</td>
      <td>0.855179</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>55</td>
      <td>0.655727</td>
      <td>1.830217</td>
      <td>0.611097</td>
      <td>0.886231</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>56</td>
      <td>0.658001</td>
      <td>1.940593</td>
      <td>0.576483</td>
      <td>0.856707</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>57</td>
      <td>0.655019</td>
      <td>1.774529</td>
      <td>0.618733</td>
      <td>0.889030</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>58</td>
      <td>0.654243</td>
      <td>1.862697</td>
      <td>0.603970</td>
      <td>0.877577</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>59</td>
      <td>0.651666</td>
      <td>1.838747</td>
      <td>0.607534</td>
      <td>0.880886</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>60</td>
      <td>0.647895</td>
      <td>1.865936</td>
      <td>0.595317</td>
      <td>0.869432</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>61</td>
      <td>0.645316</td>
      <td>1.949759</td>
      <td>0.590481</td>
      <td>0.864597</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>62</td>
      <td>0.645148</td>
      <td>1.844949</td>
      <td>0.597608</td>
      <td>0.871723</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>63</td>
      <td>0.647794</td>
      <td>1.855244</td>
      <td>0.602952</td>
      <td>0.875032</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>64</td>
      <td>0.648138</td>
      <td>2.033894</td>
      <td>0.564775</td>
      <td>0.840163</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>65</td>
      <td>0.650163</td>
      <td>1.750934</td>
      <td>0.632731</td>
      <td>0.888521</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>66</td>
      <td>0.646461</td>
      <td>1.763798</td>
      <td>0.631204</td>
      <td>0.887249</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>67</td>
      <td>0.646995</td>
      <td>1.727342</td>
      <td>0.640366</td>
      <td>0.882158</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>68</td>
      <td>0.647809</td>
      <td>1.893868</td>
      <td>0.591499</td>
      <td>0.861542</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>69</td>
      <td>0.645132</td>
      <td>1.810125</td>
      <td>0.607534</td>
      <td>0.881395</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>70</td>
      <td>0.641698</td>
      <td>1.894384</td>
      <td>0.597098</td>
      <td>0.865106</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>71</td>
      <td>0.646158</td>
      <td>1.845926</td>
      <td>0.600662</td>
      <td>0.883431</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>72</td>
      <td>0.642203</td>
      <td>1.769616</td>
      <td>0.620005</td>
      <td>0.882667</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>73</td>
      <td>0.642102</td>
      <td>1.789540</td>
      <td>0.621532</td>
      <td>0.894630</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>74</td>
      <td>0.643646</td>
      <td>1.809317</td>
      <td>0.613133</td>
      <td>0.878595</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>75</td>
      <td>0.642156</td>
      <td>1.809922</td>
      <td>0.614660</td>
      <td>0.874523</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>76</td>
      <td>0.639307</td>
      <td>1.869377</td>
      <td>0.604479</td>
      <td>0.877322</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>77</td>
      <td>0.642624</td>
      <td>1.806548</td>
      <td>0.613897</td>
      <td>0.881649</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>78</td>
      <td>0.642521</td>
      <td>1.898778</td>
      <td>0.591245</td>
      <td>0.874523</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>79</td>
      <td>0.639071</td>
      <td>1.848847</td>
      <td>0.615678</td>
      <td>0.865360</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>80</td>
      <td>0.636979</td>
      <td>1.886000</td>
      <td>0.599898</td>
      <td>0.862560</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>81</td>
      <td>0.636114</td>
      <td>1.891905</td>
      <td>0.604479</td>
      <td>0.867651</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>82</td>
      <td>0.637871</td>
      <td>1.947315</td>
      <td>0.582336</td>
      <td>0.860524</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>83</td>
      <td>0.640102</td>
      <td>1.815076</td>
      <td>0.613642</td>
      <td>0.883940</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>84</td>
      <td>0.638178</td>
      <td>1.935675</td>
      <td>0.583864</td>
      <td>0.862051</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>85</td>
      <td>0.639939</td>
      <td>1.861207</td>
      <td>0.603461</td>
      <td>0.870960</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>86</td>
      <td>0.639177</td>
      <td>1.980729</td>
      <td>0.568847</td>
      <td>0.858234</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>87</td>
      <td>0.638325</td>
      <td>1.874490</td>
      <td>0.598880</td>
      <td>0.866124</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>88</td>
      <td>0.636290</td>
      <td>1.909350</td>
      <td>0.589208</td>
      <td>0.875286</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>89</td>
      <td>0.635993</td>
      <td>1.781221</td>
      <td>0.623568</td>
      <td>0.889539</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>90</td>
      <td>0.637998</td>
      <td>1.706915</td>
      <td>0.635276</td>
      <td>0.902774</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>91</td>
      <td>0.636162</td>
      <td>2.077932</td>
      <td>0.558157</td>
      <td>0.841181</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>92</td>
      <td>0.636574</td>
      <td>1.954805</td>
      <td>0.580300</td>
      <td>0.858743</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>93</td>
      <td>0.631568</td>
      <td>1.891559</td>
      <td>0.592517</td>
      <td>0.865615</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>94</td>
      <td>0.632811</td>
      <td>1.795309</td>
      <td>0.621787</td>
      <td>0.886994</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>95</td>
      <td>0.633151</td>
      <td>1.930137</td>
      <td>0.590990</td>
      <td>0.859252</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>96</td>
      <td>0.632690</td>
      <td>1.821770</td>
      <td>0.610079</td>
      <td>0.881140</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>97</td>
      <td>0.634061</td>
      <td>1.885013</td>
      <td>0.603207</td>
      <td>0.882158</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>98</td>
      <td>0.632966</td>
      <td>2.004281</td>
      <td>0.566047</td>
      <td>0.853907</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>99</td>
      <td>0.635094</td>
      <td>1.895080</td>
      <td>0.596589</td>
      <td>0.881395</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>100</td>
      <td>0.633725</td>
      <td>2.020555</td>
      <td>0.569865</td>
      <td>0.841181</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>101</td>
      <td>0.632875</td>
      <td>1.867979</td>
      <td>0.604225</td>
      <td>0.885467</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>102</td>
      <td>0.636084</td>
      <td>1.919583</td>
      <td>0.582846</td>
      <td>0.869432</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>103</td>
      <td>0.631767</td>
      <td>1.884655</td>
      <td>0.596335</td>
      <td>0.869432</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>104</td>
      <td>0.629370</td>
      <td>1.932784</td>
      <td>0.590481</td>
      <td>0.870196</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>105</td>
      <td>0.628115</td>
      <td>1.928075</td>
      <td>0.584882</td>
      <td>0.875032</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>106</td>
      <td>0.629225</td>
      <td>1.909722</td>
      <td>0.594044</td>
      <td>0.874014</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>107</td>
      <td>0.629669</td>
      <td>1.879631</td>
      <td>0.602698</td>
      <td>0.874014</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>108</td>
      <td>0.632277</td>
      <td>2.089931</td>
      <td>0.540341</td>
      <td>0.850598</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>109</td>
      <td>0.631273</td>
      <td>1.961001</td>
      <td>0.581827</td>
      <td>0.862815</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>110</td>
      <td>0.631539</td>
      <td>1.904205</td>
      <td>0.588445</td>
      <td>0.868923</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>111</td>
      <td>0.630912</td>
      <td>1.876460</td>
      <td>0.608043</td>
      <td>0.884703</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>112</td>
      <td>0.629563</td>
      <td>1.945174</td>
      <td>0.579791</td>
      <td>0.874268</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>113</td>
      <td>0.628146</td>
      <td>1.864862</td>
      <td>0.610333</td>
      <td>0.880631</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>114</td>
      <td>0.630053</td>
      <td>1.919952</td>
      <td>0.598371</td>
      <td>0.874523</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>115</td>
      <td>0.628935</td>
      <td>2.060869</td>
      <td>0.567320</td>
      <td>0.836854</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>116</td>
      <td>0.628508</td>
      <td>1.850890</td>
      <td>0.616442</td>
      <td>0.877577</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>117</td>
      <td>0.628996</td>
      <td>1.979206</td>
      <td>0.584373</td>
      <td>0.854416</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>118</td>
      <td>0.630173</td>
      <td>1.953967</td>
      <td>0.589972</td>
      <td>0.864851</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>119</td>
      <td>0.629441</td>
      <td>1.890770</td>
      <td>0.601680</td>
      <td>0.869941</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>120</td>
      <td>0.631669</td>
      <td>2.038199</td>
      <td>0.564775</td>
      <td>0.851616</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>121</td>
      <td>0.631118</td>
      <td>1.949847</td>
      <td>0.592517</td>
      <td>0.864597</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>122</td>
      <td>0.629141</td>
      <td>1.954854</td>
      <td>0.589718</td>
      <td>0.860270</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>123</td>
      <td>0.626017</td>
      <td>1.877972</td>
      <td>0.606516</td>
      <td>0.868923</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>124</td>
      <td>0.625076</td>
      <td>1.867004</td>
      <td>0.607279</td>
      <td>0.877577</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>125</td>
      <td>0.627533</td>
      <td>1.976396</td>
      <td>0.582336</td>
      <td>0.852889</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>126</td>
      <td>0.627839</td>
      <td>1.904018</td>
      <td>0.602443</td>
      <td>0.865106</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>127</td>
      <td>0.627806</td>
      <td>1.998600</td>
      <td>0.586409</td>
      <td>0.845508</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>128</td>
      <td>0.626394</td>
      <td>2.032707</td>
      <td>0.578773</td>
      <td>0.851362</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>129</td>
      <td>0.627592</td>
      <td>2.031481</td>
      <td>0.577246</td>
      <td>0.851871</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>130</td>
      <td>0.626435</td>
      <td>2.013503</td>
      <td>0.576737</td>
      <td>0.851616</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>131</td>
      <td>0.626660</td>
      <td>1.891907</td>
      <td>0.607279</td>
      <td>0.874523</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>132</td>
      <td>0.625915</td>
      <td>1.983392</td>
      <td>0.588699</td>
      <td>0.864088</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>133</td>
      <td>0.624334</td>
      <td>1.836789</td>
      <td>0.621787</td>
      <td>0.878850</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>134</td>
      <td>0.621802</td>
      <td>1.922182</td>
      <td>0.601934</td>
      <td>0.866887</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>135</td>
      <td>0.621892</td>
      <td>1.876658</td>
      <td>0.612879</td>
      <td>0.866124</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>136</td>
      <td>0.626680</td>
      <td>1.920279</td>
      <td>0.603716</td>
      <td>0.871723</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>137</td>
      <td>0.627502</td>
      <td>2.009685</td>
      <td>0.574955</td>
      <td>0.847544</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>138</td>
      <td>0.627374</td>
      <td>2.050853</td>
      <td>0.572156</td>
      <td>0.835836</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>139</td>
      <td>0.626375</td>
      <td>1.888702</td>
      <td>0.609061</td>
      <td>0.877322</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>140</td>
      <td>0.628275</td>
      <td>2.039680</td>
      <td>0.570374</td>
      <td>0.864088</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>141</td>
      <td>0.628948</td>
      <td>1.884383</td>
      <td>0.607788</td>
      <td>0.869178</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>142</td>
      <td>0.623883</td>
      <td>1.880599</td>
      <td>0.610588</td>
      <td>0.870705</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>143</td>
      <td>0.622230</td>
      <td>1.878903</td>
      <td>0.605498</td>
      <td>0.872487</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>144</td>
      <td>0.622553</td>
      <td>1.837793</td>
      <td>0.616951</td>
      <td>0.868414</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>145</td>
      <td>0.625010</td>
      <td>1.958198</td>
      <td>0.581827</td>
      <td>0.858743</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>146</td>
      <td>0.623686</td>
      <td>1.934684</td>
      <td>0.584118</td>
      <td>0.855688</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>147</td>
      <td>0.624866</td>
      <td>1.766191</td>
      <td>0.641130</td>
      <td>0.876050</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>148</td>
      <td>0.623982</td>
      <td>1.959521</td>
      <td>0.585645</td>
      <td>0.857470</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>149</td>
      <td>0.624417</td>
      <td>1.921258</td>
      <td>0.591754</td>
      <td>0.860524</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>150</td>
      <td>0.624255</td>
      <td>2.036730</td>
      <td>0.568847</td>
      <td>0.854925</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>151</td>
      <td>0.623506</td>
      <td>1.813617</td>
      <td>0.621278</td>
      <td>0.877322</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>152</td>
      <td>0.624313</td>
      <td>1.772198</td>
      <td>0.638076</td>
      <td>0.877068</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>153</td>
      <td>0.623637</td>
      <td>1.976919</td>
      <td>0.588445</td>
      <td>0.851871</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>154</td>
      <td>0.625042</td>
      <td>1.900232</td>
      <td>0.600662</td>
      <td>0.868669</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>155</td>
      <td>0.622866</td>
      <td>1.920645</td>
      <td>0.593790</td>
      <td>0.875286</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>156</td>
      <td>0.619771</td>
      <td>1.792299</td>
      <td>0.625604</td>
      <td>0.894884</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>157</td>
      <td>0.620046</td>
      <td>1.858712</td>
      <td>0.607534</td>
      <td>0.878850</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>158</td>
      <td>0.620307</td>
      <td>2.000669</td>
      <td>0.585645</td>
      <td>0.869687</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>159</td>
      <td>0.619296</td>
      <td>1.941861</td>
      <td>0.596844</td>
      <td>0.857725</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>160</td>
      <td>0.619387</td>
      <td>1.776270</td>
      <td>0.634767</td>
      <td>0.880631</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>161</td>
      <td>0.619328</td>
      <td>1.972739</td>
      <td>0.584373</td>
      <td>0.860779</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>162</td>
      <td>0.619143</td>
      <td>1.912949</td>
      <td>0.600916</td>
      <td>0.869687</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>163</td>
      <td>0.618967</td>
      <td>1.902179</td>
      <td>0.601934</td>
      <td>0.860270</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>164</td>
      <td>0.619085</td>
      <td>1.964359</td>
      <td>0.589208</td>
      <td>0.862306</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>165</td>
      <td>0.617266</td>
      <td>1.917641</td>
      <td>0.602952</td>
      <td>0.861033</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>166</td>
      <td>0.615513</td>
      <td>1.866802</td>
      <td>0.621787</td>
      <td>0.873505</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>167</td>
      <td>0.614507</td>
      <td>1.841087</td>
      <td>0.620769</td>
      <td>0.881649</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>168</td>
      <td>0.614005</td>
      <td>1.783974</td>
      <td>0.625350</td>
      <td>0.889539</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>169</td>
      <td>0.614376</td>
      <td>1.839119</td>
      <td>0.622805</td>
      <td>0.885722</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>170</td>
      <td>0.612189</td>
      <td>1.851580</td>
      <td>0.619496</td>
      <td>0.872741</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>171</td>
      <td>0.612450</td>
      <td>1.813574</td>
      <td>0.630695</td>
      <td>0.884703</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>172</td>
      <td>0.611825</td>
      <td>1.954180</td>
      <td>0.596589</td>
      <td>0.861797</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>173</td>
      <td>0.611794</td>
      <td>1.759165</td>
      <td>0.641894</td>
      <td>0.893103</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>174</td>
      <td>0.611827</td>
      <td>1.926094</td>
      <td>0.602952</td>
      <td>0.861542</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>175</td>
      <td>0.609457</td>
      <td>1.842836</td>
      <td>0.619242</td>
      <td>0.878086</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>176</td>
      <td>0.609831</td>
      <td>1.893147</td>
      <td>0.611097</td>
      <td>0.874268</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>177</td>
      <td>0.608191</td>
      <td>1.823268</td>
      <td>0.625095</td>
      <td>0.885213</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>178</td>
      <td>0.607421</td>
      <td>1.880575</td>
      <td>0.618223</td>
      <td>0.873759</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>179</td>
      <td>0.607186</td>
      <td>1.822483</td>
      <td>0.629931</td>
      <td>0.886231</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>180</td>
      <td>0.606747</td>
      <td>1.854411</td>
      <td>0.625859</td>
      <td>0.876050</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>181</td>
      <td>0.606742</td>
      <td>1.817598</td>
      <td>0.628659</td>
      <td>0.881395</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>182</td>
      <td>0.605950</td>
      <td>1.850865</td>
      <td>0.618223</td>
      <td>0.880377</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>183</td>
      <td>0.605646</td>
      <td>1.820217</td>
      <td>0.621278</td>
      <td>0.880631</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>184</td>
      <td>0.605882</td>
      <td>1.779565</td>
      <td>0.637821</td>
      <td>0.886231</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>185</td>
      <td>0.605022</td>
      <td>1.784718</td>
      <td>0.637567</td>
      <td>0.884194</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>186</td>
      <td>0.604871</td>
      <td>1.767439</td>
      <td>0.643421</td>
      <td>0.881904</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>187</td>
      <td>0.604333</td>
      <td>1.765563</td>
      <td>0.642657</td>
      <td>0.884703</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>188</td>
      <td>0.604595</td>
      <td>1.747075</td>
      <td>0.646475</td>
      <td>0.889539</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>189</td>
      <td>0.604287</td>
      <td>1.753215</td>
      <td>0.644693</td>
      <td>0.887503</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>190</td>
      <td>0.603534</td>
      <td>1.769150</td>
      <td>0.642148</td>
      <td>0.882922</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>191</td>
      <td>0.604266</td>
      <td>1.763857</td>
      <td>0.641894</td>
      <td>0.885722</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>192</td>
      <td>0.604683</td>
      <td>1.752640</td>
      <td>0.644948</td>
      <td>0.885467</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>193</td>
      <td>0.603986</td>
      <td>1.768355</td>
      <td>0.640621</td>
      <td>0.884194</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>194</td>
      <td>0.604229</td>
      <td>1.766256</td>
      <td>0.641639</td>
      <td>0.883176</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>195</td>
      <td>0.604418</td>
      <td>1.736175</td>
      <td>0.646220</td>
      <td>0.890812</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>196</td>
      <td>0.604674</td>
      <td>1.763229</td>
      <td>0.643421</td>
      <td>0.883685</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>197</td>
      <td>0.603900</td>
      <td>1.748144</td>
      <td>0.644184</td>
      <td>0.887758</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>198</td>
      <td>0.604083</td>
      <td>1.766765</td>
      <td>0.644439</td>
      <td>0.883685</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>199</td>
      <td>0.604054</td>
      <td>1.745990</td>
      <td>0.644439</td>
      <td>0.888521</td>
      <td>00:20</td>
    </tr>
  </tbody>
</table>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

</div>
 

