---

title: ViT


keywords: fastai
sidebar: home_sidebar



nb_path: "nbs/90 - models.vision_transformer.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/90 - models.vision_transformer.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><a href="https://stats.stackexchange.com/questions/228670/what-is-the-benefit-of-the-truncated-normal-distribution-in-initializing-weights&#39;">Why truncated normal initialization?</a></p>
<p>Neurons would be dead below &lt; -2 and above &gt; 2 since <a href="https://arxiv.org/pdf/1606.08415.pdf">GeLU</a> can be approximated with input times the sigmoid function:</p>
$$
x\sigma(1.702x)
$$<p>so truncated normal helps with that and makes sures all neurons are updated.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="trunc_normal_" class="doc_header"><code>trunc_normal_</code><a href="https://github.com/keremturgutlu/self_supervised/tree/main/self_supervised/models/vision_transformer.py#L51" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>trunc_normal_</code>(<strong><code>tensor</code></strong>, <strong><code>mean</code></strong>=<em><code>0.0</code></em>, <strong><code>std</code></strong>=<em><code>1.0</code></em>, <strong><code>a</code></strong>=<em><code>-2.0</code></em>, <strong><code>b</code></strong>=<em><code>2.0</code></em>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">trunc_dist</span> <span class="o">=</span> <span class="p">[</span><span class="n">trunc_normal_</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.</span><span class="p">]),</span><span class="n">std</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span><span class="n">a</span><span class="o">=-</span><span class="mi">2</span><span class="p">,</span><span class="n">b</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5000</span><span class="p">)]</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">trunc_dist</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">30</span><span class="p">);</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,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
"
>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="drop_path" class="doc_header"><code>drop_path</code><a href="https://github.com/keremturgutlu/self_supervised/tree/main/self_supervised/models/vision_transformer.py#L56" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>drop_path</code>(<strong><code>x</code></strong>, <strong><code>drop_prob</code></strong>:<code>float</code>=<em><code>0.0</code></em>, <strong><code>training</code></strong>:<code>bool</code>=<em><code>False</code></em>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="DropPath" class="doc_header"><code>class</code> <code>DropPath</code><a href="https://github.com/keremturgutlu/self_supervised/tree/main/self_supervised/models/vision_transformer.py#L67" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>DropPath</code>(<strong><code>drop_prob</code></strong>=<em><code>None</code></em>) :: <code>Module</code></p>
</blockquote>
<p>Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="Mlp" class="doc_header"><code>class</code> <code>Mlp</code><a href="https://github.com/keremturgutlu/self_supervised/tree/main/self_supervised/models/vision_transformer.py#L78" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>Mlp</code>(<strong><code>in_features</code></strong>, <strong><code>hidden_features</code></strong>=<em><code>None</code></em>, <strong><code>out_features</code></strong>=<em><code>None</code></em>, <strong><code>act_layer</code></strong>=<em><code>GELU</code></em>, <strong><code>drop</code></strong>=<em><code>0.0</code></em>) :: <code>Module</code></p>
</blockquote>
<p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>

<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))

</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<p>:ivar training: Boolean represents whether this module is in training or
                evaluation mode.
:vartype training: bool</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="Attention" class="doc_header"><code>class</code> <code>Attention</code><a href="https://github.com/keremturgutlu/self_supervised/tree/main/self_supervised/models/vision_transformer.py#L97" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>Attention</code>(<strong><code>dim</code></strong>, <strong><code>num_heads</code></strong>=<em><code>8</code></em>, <strong><code>qkv_bias</code></strong>=<em><code>False</code></em>, <strong><code>qk_scale</code></strong>=<em><code>None</code></em>, <strong><code>attn_drop</code></strong>=<em><code>0.0</code></em>, <strong><code>proj_drop</code></strong>=<em><code>0.0</code></em>) :: <code>Module</code></p>
</blockquote>
<p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>

<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))

</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<p>:ivar training: Boolean represents whether this module is in training or
                evaluation mode.
:vartype training: bool</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="Block" class="doc_header"><code>class</code> <code>Block</code><a href="https://github.com/keremturgutlu/self_supervised/tree/main/self_supervised/models/vision_transformer.py#L124" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>Block</code>(<strong><code>dim</code></strong>, <strong><code>num_heads</code></strong>, <strong><code>mlp_ratio</code></strong>=<em><code>4.0</code></em>, <strong><code>qkv_bias</code></strong>=<em><code>False</code></em>, <strong><code>qk_scale</code></strong>=<em><code>None</code></em>, <strong><code>drop</code></strong>=<em><code>0.0</code></em>, <strong><code>attn_drop</code></strong>=<em><code>0.0</code></em>, <strong><code>drop_path</code></strong>=<em><code>0.0</code></em>, <strong><code>act_layer</code></strong>=<em><code>GELU</code></em>, <strong><code>norm_layer</code></strong>=<em><code>LayerNorm</code></em>) :: <code>Module</code></p>
</blockquote>
<p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>

<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))

</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<p>:ivar training: Boolean represents whether this module is in training or
                evaluation mode.
:vartype training: bool</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="PatchEmbed" class="doc_header"><code>class</code> <code>PatchEmbed</code><a href="https://github.com/keremturgutlu/self_supervised/tree/main/self_supervised/models/vision_transformer.py#L145" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>PatchEmbed</code>(<strong><code>img_size</code></strong>=<em><code>224</code></em>, <strong><code>patch_size</code></strong>=<em><code>16</code></em>, <strong><code>in_chans</code></strong>=<em><code>3</code></em>, <strong><code>embed_dim</code></strong>=<em><code>768</code></em>) :: <code>Module</code></p>
</blockquote>
<p>Image to Patch Embedding</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="VisionTransformer" class="doc_header"><code>class</code> <code>VisionTransformer</code><a href="https://github.com/keremturgutlu/self_supervised/tree/main/self_supervised/models/vision_transformer.py#L163" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>VisionTransformer</code>(<strong><code>img_size</code></strong>=<em><code>[224]</code></em>, <strong><code>patch_size</code></strong>=<em><code>16</code></em>, <strong><code>in_chans</code></strong>=<em><code>3</code></em>, <strong><code>num_classes</code></strong>=<em><code>0</code></em>, <strong><code>embed_dim</code></strong>=<em><code>768</code></em>, <strong><code>depth</code></strong>=<em><code>12</code></em>, <strong><code>num_heads</code></strong>=<em><code>12</code></em>, <strong><code>mlp_ratio</code></strong>=<em><code>4.0</code></em>, <strong><code>qkv_bias</code></strong>=<em><code>False</code></em>, <strong><code>qk_scale</code></strong>=<em><code>None</code></em>, <strong><code>drop_rate</code></strong>=<em><code>0.0</code></em>, <strong><code>attn_drop_rate</code></strong>=<em><code>0.0</code></em>, <strong><code>drop_path_rate</code></strong>=<em><code>0.0</code></em>, <strong><code>norm_layer</code></strong>=<em><code>LayerNorm</code></em>, <strong>**<code>kwargs</code></strong>) :: <code>Module</code></p>
</blockquote>
<p>Vision Transformer</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="MultiCropWrapper" class="doc_header"><code>class</code> <code>MultiCropWrapper</code><a href="https://github.com/keremturgutlu/self_supervised/tree/main/self_supervised/models/vision_transformer.py#L265" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>MultiCropWrapper</code>(<strong><code>encoder</code></strong>) :: <code>Module</code></p>
</blockquote>
<p>Perform forward pass separately on each resolution input.
The inputs corresponding to a single resolution are clubbed and single
forward is run on the same resolution inputs. Hence we do several
forward passes = number of different resolutions used. We then
concatenate all the output features and run the head forward on these
concatenated features.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">bs</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">x_large</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">224</span><span class="p">,</span><span class="mi">224</span><span class="p">)]</span><span class="o">*</span><span class="mi">2</span>
<span class="n">x_small</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">96</span><span class="p">,</span><span class="mi">96</span><span class="p">)]</span><span class="o">*</span><span class="mi">4</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">x_large</span> <span class="o">+</span> <span class="n">x_small</span><span class="p">;</span> <span class="p">[</span><span class="n">xi</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="k">for</span> <span class="n">xi</span> <span class="ow">in</span> <span class="n">x</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>[torch.Size([4, 3, 224, 224]),
 torch.Size([4, 3, 224, 224]),
 torch.Size([16, 3, 96, 96]),
 torch.Size([16, 3, 96, 96]),
 torch.Size([16, 3, 96, 96]),
 torch.Size([16, 3, 96, 96])]</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">vit_encoder</span> <span class="o">=</span> <span class="n">VisionTransformer</span><span class="p">(</span><span class="n">patch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">embed_dim</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">depth</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">mlp_ratio</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
                                <span class="n">qkv_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">norm_layer</span><span class="o">=</span><span class="n">partial</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">vit</span> <span class="o">=</span> <span class="n">MultiCropWrapper</span><span class="p">(</span><span class="n">vit_encoder</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">out</span> <span class="o">=</span> <span class="n">vit</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3063: UserWarning: Default upsampling behavior when mode=bicubic is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.
  &#34;See the documentation of nn.Upsample for details.&#34;.format(mode))
/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3103: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  warnings.warn(&#34;The default behavior for interpolate/upsample with float scale_factor changed &#34;
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">len</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>72</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="deit_tiny" class="doc_header"><code>deit_tiny</code><a href="https://github.com/keremturgutlu/self_supervised/tree/main/self_supervised/models/vision_transformer.py#L300" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>deit_tiny</code>(<strong><code>patch_size</code></strong>=<em><code>16</code></em>, <strong><code>img_size</code></strong>=<em><code>[224]</code></em>, <strong><code>in_chans</code></strong>=<em><code>3</code></em>, <strong><code>num_classes</code></strong>=<em><code>0</code></em>, <strong><code>qk_scale</code></strong>=<em><code>None</code></em>, <strong><code>drop_rate</code></strong>=<em><code>0.0</code></em>, <strong><code>attn_drop_rate</code></strong>=<em><code>0.0</code></em>, <strong><code>drop_path_rate</code></strong>=<em><code>0.0</code></em>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="deit_small" class="doc_header"><code>deit_small</code><a href="https://github.com/keremturgutlu/self_supervised/tree/main/self_supervised/models/vision_transformer.py#L307" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>deit_small</code>(<strong><code>patch_size</code></strong>=<em><code>16</code></em>, <strong><code>img_size</code></strong>=<em><code>[224]</code></em>, <strong><code>in_chans</code></strong>=<em><code>3</code></em>, <strong><code>num_classes</code></strong>=<em><code>0</code></em>, <strong><code>qk_scale</code></strong>=<em><code>None</code></em>, <strong><code>drop_rate</code></strong>=<em><code>0.0</code></em>, <strong><code>attn_drop_rate</code></strong>=<em><code>0.0</code></em>, <strong><code>drop_path_rate</code></strong>=<em><code>0.0</code></em>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="vit_base" class="doc_header"><code>vit_base</code><a href="https://github.com/keremturgutlu/self_supervised/tree/main/self_supervised/models/vision_transformer.py#L314" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>vit_base</code>(<strong><code>patch_size</code></strong>=<em><code>16</code></em>, <strong><code>img_size</code></strong>=<em><code>[224]</code></em>, <strong><code>in_chans</code></strong>=<em><code>3</code></em>, <strong><code>num_classes</code></strong>=<em><code>0</code></em>, <strong><code>qk_scale</code></strong>=<em><code>None</code></em>, <strong><code>drop_rate</code></strong>=<em><code>0.0</code></em>, <strong><code>attn_drop_rate</code></strong>=<em><code>0.0</code></em>, <strong><code>drop_path_rate</code></strong>=<em><code>0.0</code></em>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

</div>
 

