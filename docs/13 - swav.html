---

title: SwAV


keywords: fastai
sidebar: home_sidebar

summary: "**SwAV**: <a href='https://arxiv.org/pdf/2006.09882.pdf'>Unsupervised Learning of Visual Features by Contrasting Cluster Assignments</a>"
description: "**SwAV**: <a href='https://arxiv.org/pdf/2006.09882.pdf'>Unsupervised Learning of Visual Features by Contrasting Cluster Assignments</a>"
nb_path: "nbs/13 - swav.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/13 - swav.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Algorithm">Algorithm<a class="anchor-link" href="#Algorithm"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="SwAV">SwAV<a class="anchor-link" href="#SwAV"> </a></h4>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/self_supervised/images/swav.png" alt="SwAV"></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Absract</strong>: Unsupervised image representations have significantly reduced the gap with supervised
pretraining, notably with the recent achievements of contrastive learning
methods. These contrastive methods typically work online and rely on a large number
of explicit pairwise feature comparisons, which is computationally challenging.
In this paper, we propose an online algorithm, SwAV, that takes advantage of contrastive
methods without requiring to compute pairwise comparisons. Specifically,
our method simultaneously clusters the data while enforcing consistency between
cluster assignments produced for different augmentations (or “views”) of the same
image, instead of comparing features directly as in contrastive learning. Simply put,
we use a “swapped” prediction mechanism where we predict the code of a view
from the representation of another view. Our method can be trained with large and
small batches and can scale to unlimited amounts of data. Compared to previous
contrastive methods, our method is more memory efficient since it does not require
a large memory bank or a special momentum network. In addition, we also propose
a new data augmentation strategy, multi-crop, that uses a mix of views with
different resolutions in place of two full-resolution views, without increasing the
memory or compute requirements. We validate our findings by achieving 75:3%
top-1 accuracy on ImageNet with ResNet-50, as well as surpassing supervised
pretraining on all the considered transfer tasks.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="SwAVModel" class="doc_header"><code>class</code> <code>SwAVModel</code><a href="https://github.com/keremturgutlu/self_supervised/tree/master/self_supervised/vision/swav.py#L11" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>SwAVModel</code>(<strong><code>encoder</code></strong>, <strong><code>projector</code></strong>, <strong><code>prototypes</code></strong>) :: <code>Module</code></p>
</blockquote>
<p>Same as <code>nn.Module</code>, but no need for subclasses to call <code>super().__init__</code></p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="create_swav_model" class="doc_header"><code>create_swav_model</code><a href="https://github.com/keremturgutlu/self_supervised/tree/master/self_supervised/vision/swav.py#L34" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>create_swav_model</code>(<strong><code>encoder</code></strong>, <strong><code>hidden_size</code></strong>=<em><code>256</code></em>, <strong><code>projection_size</code></strong>=<em><code>128</code></em>, <strong><code>n_protos</code></strong>=<em><code>3000</code></em>)</p>
</blockquote>
<p>Create SwAV model</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">encoder</span> <span class="o">=</span> <span class="n">create_encoder</span><span class="p">(</span><span class="s2">&quot;tf_efficientnet_b0_ns&quot;</span><span class="p">,</span> <span class="n">n_in</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">pretrained</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">pool_type</span><span class="o">=</span><span class="n">PoolingType</span><span class="o">.</span><span class="n">CatAvgMax</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">create_swav_model</span><span class="p">(</span><span class="n">encoder</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span><span class="mi">2048</span><span class="p">,</span> <span class="n">projection_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">n_protos</span><span class="o">=</span><span class="mi">3000</span><span class="p">)</span>
<span class="n">multi_view_inputs</span> <span class="o">=</span> <span class="p">([</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">224</span><span class="p">,</span><span class="mi">224</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">)]</span> <span class="o">+</span>
                     <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">96</span><span class="p">,</span><span class="mi">96</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">)])</span>
<span class="n">embedding</span><span class="p">,</span> <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">multi_view_inputs</span><span class="p">)</span>
<span class="n">norms</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">prototypes</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">norms</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="mi">3000</span>
<span class="k">assert</span> <span class="p">[</span><span class="n">n</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">norms</span> <span class="k">if</span> <span class="n">test_close</span><span class="p">(</span><span class="n">n</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="mf">1.</span><span class="p">)]</span> <span class="o">==</span> <span class="p">[]</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="SwAV-Callback">SwAV Callback<a class="anchor-link" href="#SwAV-Callback"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The following parameters can be passed;</p>
<ul>
<li><p><strong>aug_pipelines</strong> list of augmentation pipelines List[Pipeline, Pipeline,...,Pipeline] created using functions from <code>self_supervised.augmentations</code> module. Each <code>Pipeline</code> should be set to <code>split_idx=0</code>.  You can simply use <a href="/self_supervised/13 - swav.html#get_swav_aug_pipelines"><code>get_swav_aug_pipelines</code></a> utility to get aug_pipelines. SWAV algorithm uses a mix of large and small scale crops.</p>
</li>
<li><p><strong>crop_assgn_ids</strong> indexes for large crops from <strong>aug_pipelines</strong>, e.g. if you have total of 8 Pipelines in the <code>aug_pipelines</code> list and if you define large crops as first 2 Pipelines then indexes would be [0,1], if as first 3 then [0,1,2] and if as last 2 then [6,7], so on.</p>
</li>
<li><p><strong>K</strong> is queue size. For simplicity K needs to be a multiple of batch size and it needs to be less than total training data. You can try out different values e.g. <code>bs*2^k</code> by varying k where bs i batch size. You can pass None to disable queue. Idea is similar to MoCo.</p>
</li>
<li><p><strong>queue_start_pct</strong> defines when to start using queue in terms of total training percentage, e.g if you train for 100 epochs and if <code>queue_start_pct</code> is set to 0.25 then queue will be used starting from epoch 25. You should tune queue size and queue start percentage for your own data and problem. For more information you can refer to <a href="https://github.com/facebookresearch/swav#training-gets-unstable-when-using-the-queue">README from official implementation</a>.</p>
</li>
<li><p><strong>temp</strong> temperature scaling for cross entropy loss similar to <a href="/self_supervised/10 - simclr.html#SimCLR"><code>SimCLR</code></a>.</p>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>SWAV algorithm uses multi-sized-multi-crop views of image. In original paper 2 large crop views and 6 small crop views are used during training. The reason of using smaller crops is to save memory and perhaps it also helps model to learn local features better.</p>
<p>You can manually pass a mix of large and small scale Pipeline instances within a list to <strong>aug_pipelines</strong> or you can simply use <strong>get_swav_aug_pipelines()</strong> helper function below:</p>
<ul>
<li><strong>num_crops</strong> Number of large and small scale views to be used. </li>
<li><strong>crop_sizes</strong> Image crop sizes for large and small views. </li>
<li><strong>min_scales</strong> Min scale to use in RandomResizedCrop for large and small views. </li>
<li><strong>max_scales</strong> Max scale to use in RandomResizedCrop for large and small views. </li>
</ul>
<p>I highly recommend this <a href="https://albumentations-demo.herokuapp.com/">UI from albumentations</a> to get a feel about RandomResizedCrop parameters.</p>
<p>Let's take the following example <code>get_swav_aug_pipelines(num_crops=(2,6), crop_sizes=(224,96), min_scales=(0.25,0.05), max_scales=(1.,0.14))</code>. This will create 2 large scale view augmentations with size 224 and with RandomResizedCrop scales between 0.25-1.0. Additionally, it will create 2 small scale view augmentations with size 96 and with RandomResizedCrop scales between 0.05-0.14.</p>
<p><strong>Note</strong>: Of course, the notion of small and large scale views depend on the values you pass to <code>crop_sizes</code>, <code>min_scales</code>, and <code>max_scales</code>. For example, if I we flip crop sizes from previous example as <code>crop_sizes=(96,224)</code>, then in this case first 2 views will have image resolution of 96 and last 6 views will have 224. For reducing confusion it's better to make relative changes, e.g. if you want to try different parameters always try to keep first values for larger resolution views and second values for smaller resolution views.</p>
<ul>
<li><strong>**kwargs</strong> This function uses <a href="/self_supervised/01 - augmentations.html#get_multi_aug_pipelines"><code>get_multi_aug_pipelines</code></a> which then uses <a href="/self_supervised/01 - augmentations.html#get_batch_augs"><code>get_batch_augs</code></a>. For more information you may refer to <code>self_supervised.augmentations</code> module. kwargs takes any passable argument to <a href="/self_supervised/01 - augmentations.html#get_batch_augs"><code>get_batch_augs</code></a></li>
</ul>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="get_swav_aug_pipelines" class="doc_header"><code>get_swav_aug_pipelines</code><a href="https://github.com/keremturgutlu/self_supervised/tree/master/self_supervised/vision/swav.py#L47" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>get_swav_aug_pipelines</code>(<strong><code>num_crops</code></strong>=<em><code>(2, 6)</code></em>, <strong><code>crop_sizes</code></strong>=<em><code>(224, 96)</code></em>, <strong><code>min_scales</code></strong>=<em><code>(0.25, 0.05)</code></em>, <strong><code>max_scales</code></strong>=<em><code>(1.0, 0.14)</code></em>, <strong><code>rotate</code></strong>=<em><code>True</code></em>, <strong><code>jitter</code></strong>=<em><code>True</code></em>, <strong><code>bw</code></strong>=<em><code>True</code></em>, <strong><code>blur</code></strong>=<em><code>True</code></em>, <strong><code>resize_ratio</code></strong>=<em><code>(0.75, 1.3333333333333333)</code></em>, <strong><code>rotate_deg</code></strong>=<em><code>30</code></em>, <strong><code>s</code></strong>=<em><code>0.6</code></em>, <strong><code>blur_s</code></strong>=<em><code>(4, 32)</code></em>, <strong><code>same_on_batch</code></strong>=<em><code>False</code></em>, <strong><code>flip_p</code></strong>=<em><code>0.5</code></em>, <strong><code>rotate_p</code></strong>=<em><code>0.3</code></em>, <strong><code>jitter_p</code></strong>=<em><code>0.3</code></em>, <strong><code>bw_p</code></strong>=<em><code>0.3</code></em>, <strong><code>blur_p</code></strong>=<em><code>0.3</code></em>, <strong><code>stats</code></strong>=<em><code>([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])</code></em>, <strong><code>cuda</code></strong>=<em><code>True</code></em>, <strong><code>xtra_tfms</code></strong>=<em><code>[]</code></em>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="SWAV" class="doc_header"><code>class</code> <code>SWAV</code><a href="https://github.com/keremturgutlu/self_supervised/tree/master/self_supervised/vision/swav.py#L55" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>SWAV</code>(<strong><code>aug_pipelines</code></strong>, <strong><code>crop_assgn_ids</code></strong>, <strong><code>K</code></strong>=<em><code>3000</code></em>, <strong><code>queue_start_pct</code></strong>=<em><code>0.25</code></em>, <strong><code>temp</code></strong>=<em><code>0.07</code></em>, <strong><code>eps</code></strong>=<em><code>0.05</code></em>, <strong><code>n_sinkh_iter</code></strong>=<em><code>3</code></em>, <strong><code>print_augs</code></strong>=<em><code>False</code></em>) :: <code>Callback</code></p>
</blockquote>
<p>Basic class handling tweaks of the training loop by changing a <code>Learner</code> in various events</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><code>crop_sizes</code> defines the size to be used for original crops and low resolution crops respectively. <code>num_crops</code> define <code>N</code>: number of original views and <code>V</code>: number of low resolution views respectively. <code>min_scales</code> and <code>max_scales</code> are used for original and low resolution views during random resized crop. <code>eps</code> is used during Sinkhorn-Knopp algorithm for calculating the codes and <code>n_sinkh_iter</code> is the number of iterations during it's calculation. <code>temp</code> is the temperature parameter in cross entropy loss</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Example-Usage">Example Usage<a class="anchor-link" href="#Example-Usage"> </a></h3>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">path</span> <span class="o">=</span> <span class="n">untar_data</span><span class="p">(</span><span class="n">URLs</span><span class="o">.</span><span class="n">MNIST_TINY</span><span class="p">)</span>
<span class="n">items</span> <span class="o">=</span> <span class="n">get_image_files</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>
<span class="n">tds</span> <span class="o">=</span> <span class="n">Datasets</span><span class="p">(</span><span class="n">items</span><span class="p">,</span> <span class="p">[</span><span class="n">PILImageBW</span><span class="o">.</span><span class="n">create</span><span class="p">,</span> <span class="p">[</span><span class="n">parent_label</span><span class="p">,</span> <span class="n">Categorize</span><span class="p">()]],</span> <span class="n">splits</span><span class="o">=</span><span class="n">GrandparentSplitter</span><span class="p">()(</span><span class="n">items</span><span class="p">))</span>
<span class="n">dls</span> <span class="o">=</span> <span class="n">tds</span><span class="o">.</span><span class="n">dataloaders</span><span class="p">(</span><span class="n">bs</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">after_item</span><span class="o">=</span><span class="p">[</span><span class="n">ToTensor</span><span class="p">(),</span> <span class="n">IntToFloatTensor</span><span class="p">()],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cpu&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">fastai_encoder</span> <span class="o">=</span> <span class="n">create_fastai_encoder</span><span class="p">(</span><span class="n">xresnet18</span><span class="p">,</span> <span class="n">n_in</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">pretrained</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">create_swav_model</span><span class="p">(</span><span class="n">fastai_encoder</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span><span class="mi">2048</span><span class="p">,</span> <span class="n">projection_size</span><span class="o">=</span><span class="mi">128</span><span class="p">)</span>
<span class="n">aug_pipelines</span> <span class="o">=</span> <span class="n">get_swav_aug_pipelines</span><span class="p">(</span><span class="n">num_crops</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span><span class="mi">6</span><span class="p">],</span>
                                       <span class="n">crop_sizes</span><span class="o">=</span><span class="p">[</span><span class="mi">28</span><span class="p">,</span><span class="mi">16</span><span class="p">],</span> 
                                       <span class="n">min_scales</span><span class="o">=</span><span class="p">[</span><span class="mf">0.25</span><span class="p">,</span><span class="mf">0.05</span><span class="p">],</span>
                                       <span class="n">max_scales</span><span class="o">=</span><span class="p">[</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">0.3</span><span class="p">],</span>
                                       <span class="n">rotate</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">jitter</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">bw</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">blur</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">stats</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">cuda</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> 
<span class="n">learn</span> <span class="o">=</span> <span class="n">Learner</span><span class="p">(</span><span class="n">dls</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span>
                <span class="n">cbs</span><span class="o">=</span><span class="p">[</span><span class="n">SWAV</span><span class="p">(</span><span class="n">aug_pipelines</span><span class="o">=</span><span class="n">aug_pipelines</span><span class="p">,</span> <span class="n">crop_assgn_ids</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="n">K</span><span class="o">=</span><span class="kc">None</span><span class="p">),</span> <span class="n">ShortEpochCallback</span><span class="p">(</span><span class="mf">0.001</span><span class="p">)])</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">b</span> <span class="o">=</span> <span class="n">dls</span><span class="o">.</span><span class="n">one_batch</span><span class="p">()</span>
<span class="n">learn</span><span class="o">.</span><span class="n">_split</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
<span class="n">learn</span><span class="p">(</span><span class="s1">&#39;before_batch&#39;</span><span class="p">)</span>
<span class="n">learn</span><span class="o">.</span><span class="n">pred</span> <span class="o">=</span> <span class="n">learn</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="o">*</span><span class="n">learn</span><span class="o">.</span><span class="n">xb</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Display 2 standard resolution crops and 6 additional low resolution crops</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">axes</span> <span class="o">=</span> <span class="n">learn</span><span class="o">.</span><span class="n">swav</span><span class="o">.</span><span class="n">show_one</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,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
"
>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>00:01</td>
    </tr>
  </tbody>
</table>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learn</span><span class="o">.</span><span class="n">recorder</span><span class="o">.</span><span class="n">losses</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>[tensor(8.7490)]</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

</div>
 

