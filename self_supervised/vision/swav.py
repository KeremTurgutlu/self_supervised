# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/13 - swav.ipynb (unless otherwise specified).

__all__ = ['SwAVModel', 'create_swav_model', 'get_swav_aug_pipelines', 'SWAV']

# Cell
from fastai.vision.all import *
from ..augmentations import *
from ..layers import *

# Cell
class SwAVModel(Module):
    def __init__(self,encoder,projector,prototypes):
        self.encoder,self.projector,self.prototypes = encoder,projector,prototypes

    def forward(self, inputs):

        if not isinstance(inputs, list): inputs = [inputs]

        crop_idxs = torch.cumsum(torch.unique_consecutive(
                                torch.tensor([inp.shape[-1] for inp in inputs]),
                                return_counts=True)[1], 0)

        start_idx = 0
        for idx in crop_idxs:
            _z = self.encoder(torch.cat(inputs[start_idx: idx]))
            if not start_idx: z = _z
            else:             z = torch.cat((z, _z))
            start_idx = idx

        z = F.normalize(self.projector(z))
        return z, self.prototypes(z)

# Cell
def create_swav_model(encoder, hidden_size=256, projection_size=128, n_protos=3000, bn=True, nlayers=2):
    "Create SwAV model"
    n_in  = in_channels(encoder)
    with torch.no_grad(): representation = encoder(torch.randn((2,n_in,128,128)))
    projector = create_mlp_module(representation.size(1), hidden_size, projection_size, bn=bn, nlayers=nlayers)
    prototypes = nn.Linear(projection_size, n_protos, bias=False)
    apply_init(projector)
    with torch.no_grad():
        w = prototypes.weight.data.clone()
        prototypes.weight.copy_(F.normalize(w))
    return SwAVModel(encoder, projector, prototypes)

# Cell
@delegates(get_multi_aug_pipelines, but=['n', 'size', 'resize_scale'])
def get_swav_aug_pipelines(num_crops=(2,6), crop_sizes=(224,96), min_scales=(0.25,0.05), max_scales=(1.,0.14), **kwargs):
    aug_pipelines = []
    for nc, size, mins, maxs in zip(num_crops, crop_sizes, min_scales, max_scales):
        aug_pipelines += get_multi_aug_pipelines(n=nc, size=size, resize_scale=(mins,maxs), **kwargs)
    return aug_pipelines

# Cell
class SWAV(Callback):
    order,run_valid = 9,True
    def __init__(self, aug_pipelines, crop_assgn_ids,
                       K=3000, queue_start_pct=0.25, temp=0.1,
                       eps=0.05,  n_sinkh_iter=3, print_augs=False):

        store_attr('K,queue_start_pct,crop_assgn_ids,temp,eps,n_sinkh_iter')
        self.augs = aug_pipelines
        if print_augs:
            for aug in self.augs: print(aug)


    def before_fit(self):
        self.learn.loss_func = self.lf

        # init queue
        if self.K is not None:
            nf = self.learn.model.projector[-1].out_features
            self.queue = torch.randn(self.K, nf).to(self.dls.device)
            self.queue = nn.functional.normalize(self.queue, dim=1)
            self.queue_ptr = 0


    def before_batch(self):
        "Compute multi crop inputs"
        self.bs = self.x.size(0)
        self.learn.xb = ([aug(self.x) for aug in self.augs],)


    def after_batch(self):
        with torch.no_grad():
            w = self.learn.model.prototypes.weight.data.clone()
            self.learn.model.prototypes.weight.data.copy_(F.normalize(w))


    @torch.no_grad()
    def sinkhorn_knopp(self, Q, nmb_iters, device=default_device):
        "https://en.wikipedia.org/wiki/Sinkhorn%27s_theorem#Sinkhorn-Knopp_algorithm"
        sum_Q = torch.sum(Q)
        Q /= sum_Q

        r = (torch.ones(Q.shape[0]) / Q.shape[0]).to(device)
        c = (torch.ones(Q.shape[1]) / Q.shape[1]).to(device)

        curr_sum = torch.sum(Q, dim=1)

        for it in range(nmb_iters):
            u = curr_sum
            Q *= (r / u).unsqueeze(1)
            Q *= (c / torch.sum(Q, dim=0)).unsqueeze(0)
            curr_sum = torch.sum(Q, dim=1)
        return (Q / torch.sum(Q, dim=0, keepdim=True)).t().float()


    @torch.no_grad()
    def _dequeue_and_enqueue(self, embedding):
        assert self.K % self.bs == 0  # for simplicity
        self.queue[self.queue_ptr:self.queue_ptr+self.bs, :] = embedding
        self.queue_ptr = (self.queue_ptr + self.bs) % self.K  # move pointer


    @torch.no_grad()
    def _compute_codes(self, output):
        qs = []
        for i in self.crop_assgn_ids:
            # use queue
            if (self.K is not None) and (self.learn.pct_train > self.queue_start_pct):
                target_b = output[self.bs*i:self.bs*(i+1)]
                queue_b = self.learn.model.prototypes(self.queue)
                merged_b = torch.cat([target_b, queue_b])
                q = torch.exp(merged_b/self.eps).t()
                q = self.sinkhorn_knopp(q, self.n_sinkh_iter, q.device)
                qs.append(q[:self.bs])

            # don't use queue
            else:
                target_b = output[self.bs*i:self.bs*(i+1)]
                q = torch.exp(target_b/self.eps).t()
                q = self.sinkhorn_knopp(q, self.n_sinkh_iter, q.device)
                qs.append(q)
        return qs


    def after_pred(self):
        "Compute ps and qs"

        embedding, output = self.pred

        # Update - no need to store all assignment crops, e.g. just 0 from [0,1]
        # Update queue only during training
        if (self.K is not None) and (self.learn.training):  self._dequeue_and_enqueue(embedding[:self.bs])

        # Compute codes
        qs = self._compute_codes(output)

        # Compute predictions
        log_ps = []
        for v in np.arange(len(self.augs)):
            log_p = F.log_softmax(output[self.bs*v:self.bs*(v+1)] / self.temp, dim=1)
            log_ps.append(log_p)

        log_ps, qs = torch.stack(log_ps), torch.stack(qs)
        self.learn.pred, self.learn.yb = log_ps, (qs,)


    def lf(self, pred, *yb):
        log_ps, qs, loss = pred, yb[0], 0
        t = (qs.unsqueeze(1)*log_ps.unsqueeze(0)).sum(-1).mean(-1)
        for i, ti in enumerate(t): loss -= (ti.sum() - ti[i])/(len(ti)-1)/len(t)
        return loss

    @torch.no_grad()
    def show(self, n=1):
        xbs = self.learn.xb[0]
        idxs = np.random.choice(range(self.bs), n, False)
        images = [aug.decode(xb.to('cpu').clone()).clamp(0, 1)[i]
                  for i in idxs
                  for xb, aug in zip(xbs, self.augs)]
        return show_batch(images[0], None, images, max_n=len(images), nrows=n)