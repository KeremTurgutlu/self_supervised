# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/15 - dino.ipynb (unless otherwise specified).

__all__ = ['DINOHead', 'get_dino_aug_pipelines', 'DINO']

# Cell
from fastai.vision.all import *
from ..augmentations import *
from ..layers import *
from ..models.vision_transformer import *

# Cell
class DINOHead(nn.Module):
    '''
    copy.deepcopy:
    RuntimeError: Only Tensors created explicitly by the user (graph leaves)
    support the deepcopy protocol at the moment

    https://pytorch.org/docs/stable/generated/torch.nn.utils.weight_norm.html
    https://pytorch.org/docs/stable/generated/torch.nn.GELU.html
    '''
    def __init__(self, in_dim, out_dim, use_bn=False, norm_last_layer=True, nlayers=3, hidden_dim=2048, bottleneck_dim=256):
        super().__init__()
        nlayers = max(nlayers, 1)
        if nlayers == 1:
            self.mlp = nn.Linear(in_dim, bottleneck_dim)
        else:
            layers = [nn.Linear(in_dim, hidden_dim)]
            if use_bn:
                layers.append(nn.BatchNorm1d(hidden_dim))
            layers.append(nn.GELU())
            for _ in range(nlayers - 2):
                layers.append(nn.Linear(hidden_dim, hidden_dim))
                if use_bn:
                    layers.append(nn.BatchNorm1d(hidden_dim))
                layers.append(nn.GELU())
            layers.append(nn.Linear(hidden_dim, bottleneck_dim))
            self.mlp = nn.Sequential(*layers)
        self.apply(self._init_weights)
        self.last_layer = nn.utils.weight_norm(nn.Linear(bottleneck_dim, out_dim, bias=False))
        self.last_layer.weight_g.data.fill_(1)
        if norm_last_layer:
            self.last_layer.weight_g.requires_grad = False

    def _init_weights(self, m):
        if isinstance(m, nn.Linear):
            trunc_normal_(m.weight, std=.02)
            if isinstance(m, nn.Linear) and m.bias is not None:
                nn.init.constant_(m.bias, 0)

    def forward(self, x):
        x = self.mlp(x)
        x = nn.functional.normalize(x, dim=-1, p=2)
        x = self.last_layer(x)
        return x

# Cell
@delegates(get_multi_aug_pipelines, but=['n', 'size', 'resize_scale'])
def get_dino_aug_pipelines(num_crops=(2,4), crop_sizes=(224,96), min_scales=(0.4,0.05), max_scales=(1.,0.4), **kwargs):
    aug_pipelines = []
    for nc, size, mins, maxs in zip(num_crops, crop_sizes, min_scales, max_scales):
        aug_pipelines += get_multi_aug_pipelines(n=nc, size=size, resize_scale=(mins,maxs), **kwargs)
    return aug_pipelines

# Cell
class DINO(Callback):
    order,run_valid = 9,True
    def __init__(self, aug_pipelines, teacher_model, large_crop_ids=[0,1],
                         cmom=0.9,
                         tmom_start=0.9995, tmom_end=1., tmom_sched=SchedCos,
                         tpt_start=0.04, tpt_end=0.07, tpt_warmup_pct=0.3, tpt_sched=SchedLin,
                         tps=0.1,
                         freeze_last_layer=5,
                         print_augs=False):
        """
        DINO teacher student training with distillation.
        Refer to original repo:
        https://github.com/facebookresearch/dino/blob/0be6e112dd579203caaa1d0f066e29ca536f76dd/main_dino.py#L41
            cmom:           Center update momentum.
            tmom:           Teacher update momentum. Set larger, e.g. 0.9995, for small batches or 0.996 for large batches (256+).
            tpt_warmup:     Warm up starting temperature
            tpt_warmup_pct: Percentage of training for warmup
            tpt_sched:      Warm up scheduler, e.g. SchedLin, SchedCos, SchedExp
            tpt:            Teacher temperature after warm up. Decrease if training loss does not decrease.
                            Smaller temperature means more sharpening.
            tps:            Student temperature.
            freeze_last_layer: How many epochs to freeze the last layer
        """
        store_attr('teacher_model,large_crop_ids,cmom,tps,teacher_model,freeze_last_layer')
        self.augs = aug_pipelines
        self.tpt_scheduler  = combine_scheds([tpt_warmup_pct,1-tpt_warmup_pct],
                                             [tpt_sched(tpt_start,tpt_end),SchedNo(tpt_end,tpt_end)])
        self.tmom_scheduler = tmom_sched(tmom_start, tmom_end)
        if print_augs:
            for aug in self.augs: print(aug)

    def before_fit(self):
        "Create teacher model as a copy of student"
#         https://github.com/pytorch/pytorch/issues/28594
#         self.teacher_model = deepcopy(self.learn.model).to(self.dls.device)

        self.teacher_model.to(self.dls.device)
        self.teacher_model.load_state_dict(self.learn.model.state_dict())
        for param_t in self.teacher_model.parameters(): param_t.requires_grad = False

        self.learn.loss_func = self.lf
        self.C = torch.zeros(1,num_features_model(self.learn.model)).to(self.dls.device)
        self.tpt  = self.tpt_scheduler(0.)
        self.tmom = self.tmom_scheduler(0.)

        for n,p in self.learn.model[1].last_layer.named_parameters():
            if n == 'weight_v' : p.requires_grad = False

    def before_train(self):    self.teacher_model.train() # learn.summary()
    def before_validate(self): self.teacher_model.eval()  # learn.summary()
    def before_batch(self):
        "Augment multi crop views"
        self.bs = self.x.size(0)
        self.learn.xb = ([aug(self.x) for aug in self.augs],)
        x_large = [self.learn.xb[0][i] for i in self.large_crop_ids]
        # TODO: Do we need to put the teacher in eval(), not it original repo?
        with torch.no_grad():
            targs = self.teacher_model(x_large)
            self.learn.yb = (targs,)
            self.cb = targs.mean(0)


    def _momentum_update_teacher(self):
        for param_q, param_t in zip(self.learn.model.parameters(), self.teacher_model.parameters()):
            param_t.data = param_t.data * self.tmom + param_t.data * (1. - self.tmom)


    def _momentum_update_center(self):
        self.C = self.C*self.cmom + self.cb*(1-self.cmom)


    def after_step(self):
        "Center and teacher updates"
        self._momentum_update_center()
        self._momentum_update_teacher()


    def after_epoch(self):
        "Update tpt at the end of each epoch"
        self.tpt  = self.tpt_scheduler(self.pct_train)
        self.tmom = self.tmom_scheduler(self.pct_train)

        if self.epoch == self.freeze_last_layer:
            print("Setting last layer to trainable")
            for n,p in self.learn.model[1].last_layer.named_parameters():
                if n == 'weight_v' : p.requires_grad = True



    def lf(self, pred, *yb):
        "Multi crop cross entropy loss: -qlog(p)"
        yb = yb[0]
        pred = F.log_softmax(pred / self.tps, dim=-1)
        yb   = F.softmax(yb - self.C / self.tpt, dim=-1)

        n_targs, n_preds = yb.size(0)//self.bs, pred.size(0)//self.bs
        yb, pred = yb.chunk(n_targs), pred.chunk(n_preds)

        loss, npairs = 0, n_targs*(n_preds-1)
        for ti in range(n_targs):
            for pi in range(n_preds):
                if ti != pi:
                    loss += (-yb[ti]*pred[pi]).sum(-1).mean() / npairs
        return loss


    @torch.no_grad()
    def show(self, n=1):
        xbs = self.learn.xb[0]
        idxs = np.random.choice(range(self.bs), n, False)
        images = [aug.decode(xb.to('cpu').clone()).clamp(0, 1)[i]
                  for i in idxs
                  for xb, aug in zip(xbs, self.augs)]
        return show_batch(images[0], None, images, max_n=len(images), nrows=n)