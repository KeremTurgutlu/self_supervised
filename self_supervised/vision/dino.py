# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/15 - dino.ipynb (unless otherwise specified).

__all__ = ['DINOHead', 'get_dino_aug_pipelines', 'DINO']

# Cell
from fastai.vision.all import *
from ..augmentations import *
from ..layers import *
from ..models.vision_transformer import *

# Cell
class DINOHead(nn.Module):
    '''
    https://pytorch.org/docs/stable/generated/torch.nn.utils.weight_norm.html
    https://pytorch.org/docs/stable/generated/torch.nn.GELU.html
    '''
    def __init__(self, in_dim, out_dim, use_bn=False, norm_last_layer=True, nlayers=3, hidden_dim=2048, bottleneck_dim=256):
        super().__init__()
        nlayers = max(nlayers, 1)
        if nlayers == 1:
            self.mlp = nn.Linear(in_dim, bottleneck_dim)
        else:
            layers = [nn.Linear(in_dim, hidden_dim)]
            if use_bn:
                layers.append(nn.BatchNorm1d(hidden_dim))
            layers.append(nn.GELU())
            for _ in range(nlayers - 2):
                layers.append(nn.Linear(hidden_dim, hidden_dim))
                if use_bn:
                    layers.append(nn.BatchNorm1d(hidden_dim))
                layers.append(nn.GELU())
            layers.append(nn.Linear(hidden_dim, bottleneck_dim))
            self.mlp = nn.Sequential(*layers)
        self.apply(self._init_weights)
        self.last_layer = nn.utils.weight_norm(nn.Linear(bottleneck_dim, out_dim, bias=False))
        self.last_layer.weight_g.data.fill_(1)
        if norm_last_layer:
            self.last_layer.weight_g.requires_grad = False

    def _init_weights(self, m):
        if isinstance(m, nn.Linear):
            trunc_normal_(m.weight, std=.02)
            if isinstance(m, nn.Linear) and m.bias is not None:
                nn.init.constant_(m.bias, 0)

    def forward(self, x):
        x = self.mlp(x)
        x = nn.functional.normalize(x, dim=-1, p=2)
        x = self.last_layer(x)
        return x

# Cell
@delegates(get_multi_aug_pipelines, but=['n', 'size', 'resize_scale'])
def get_dino_aug_pipelines(num_crops=(2,4), crop_sizes=(224,96), min_scales=(0.4,0.05), max_scales=(1.,0.4), **kwargs):
    aug_pipelines = []
    for nc, size, mins, maxs in zip(num_crops, crop_sizes, min_scales, max_scales):
        aug_pipelines += get_multi_aug_pipelines(n=nc, size=size, resize_scale=(mins,maxs), **kwargs)
    return aug_pipelines

# Cell
class DINO(Callback):
    order,run_valid = 9,True
    def __init__(self, aug_pipelines, large_crop_ids=[0,1],
                         cmom=0.9, tmom=0.996,
                         tpt_warmup=0.04, tpt_warmup_pct=0.3, tpt_sched=sched_lin, tpt=0.07,
                         tps=0.1,
                         print_augs=False):
        """
        Refer to original repo:
        https://github.com/facebookresearch/dino/blob/0be6e112dd579203caaa1d0f066e29ca536f76dd/main_dino.py#L41
            cmom:           Center update momentum.
            tmom:           Teacher update momentum. Set larger, e.g. 0.9995, for small batches.
            tpt_warmup:     Warm up starting temperature
            tpt_warmup_pct: Percentage of training for warmup
            tpt_sched:      Warm up scheduler, e.g. sched_lin, sched_cos, sched_exp
            tpt:            Teacher temperature after warm up. Decrease if training loss does not decrease.
                            Smaller temperature means more sharpening.
            tps:            Student temperature.
        """
        store_attr('K,large_crop_ids,cmom,tmom,tpt,tps')
        self.augs = aug_pipelines
        if print_augs:
            for aug in self.augs: print(aug)
        self.C = torch.zeros(1,K)


    def before_fit(self):
        "Create teacher model as a copy of student"
        self.teacher_model = deepcopy(self.learn.model).to(self.dls.device)
        for param_t in self.teacher_model.parameters(): param_t.requires_grad = False
        self.learn.loss_func = self.lf
        self.C = torch.zeros(1,num_features_model(self.learn.model))


    def before_train(self):    self.teacher_model.train()
    def before_validate(self): self.teacher_model.eval()
    def before_batch(self):
        "Augment multi crop views"
        self.bs = self.x.size(0)
        self.learn.xb = ([aug(self.x) for aug in self.augs],)
        x_large = [self.learn.xb[0][i] for i in self.large_crop_ids]
        self.cb = torch.cat(x_large).mean(0)
        with torch.no_grad(): self.yb = (self.teacher_model(x_large),)


    def _momentum_update_teacher(self):
        for param_q, param_t in zip(self.learn.model.parameters(), self.teacher_model.parameters()):
            param_t.data = param_t.data * self.tmom + param_t.data * (1. - self.tmom)


    def _momentum_update_center(self):
        self.C = self.C*self.cmom + self.cb*(1-self.cmom)


    def after_step(self):
        "Center and teacher updates"
        self._momentum_update_center()
        self._momentum_update_teacher()


    def lf(self, pred, *yb):
        "Multi crop cross entropy loss: -qlog(p)"
        pred = F.log_softmax(pred / tps, dim=-1)
        yb = F.softmax(yb - C / tpt, dim=-1)
        n_targs, n_preds = yb.size(0)//bs, pred.size(0)//bs
        yb,pred = yb.chunk(n_targs), pred.chunk(n_preds)
        npairs = len(targs)*(len(preds)-1)
        loss = 0
        for ti in range(len(targs)):
            for pi in range(len(preds)):
                if ti != pi:
                    loss += (-targs[ti]*preds[pi]).sum(-1).mean() / npairs
        return loss



    @torch.no_grad()
    def show(self, n=1):
        xbs = self.learn.xb[0]
        idxs = np.random.choice(range(self.bs), n, False)
        images = [aug.decode(xb.to('cpu').clone()).clamp(0, 1)[i]
                  for i in idxs
                  for xb, aug in zip(xbs, self.augs)]
        return show_batch(images[0], None, images, max_n=len(images), nrows=n)